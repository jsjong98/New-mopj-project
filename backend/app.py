from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, classification_report
import os
import json
import warnings
import random
import traceback
from datetime import datetime, timedelta
import matplotlib
matplotlib.use('Agg')  # ì„œë²„ì—ì„œ GUI ë°±ì—”ë“œ ì‚¬ìš© ì•ˆ í•¨
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.gridspec import GridSpec
import seaborn as sns
from werkzeug.utils import secure_filename
import io
import base64
import tempfile
import time
from threading import Thread
import logging
import calendar
import shutil
import optuna
import csv
from pathlib import Path

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

# ëœë¤ ì‹œë“œ ì„¤ì •
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)  # GPU ì‚¬ìš© ì‹œ
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# ë””ë ‰í† ë¦¬ ì„¤ì • - íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œ
UPLOAD_FOLDER = 'uploads'
HOLIDAY_DIR = 'holidays'
CACHE_ROOT_DIR = 'cache'  # ğŸ”‘ ìƒˆë¡œìš´ íŒŒì¼ë³„ ìºì‹œ ë£¨íŠ¸

# ê¸°ë³¸ ë””ë ‰í† ë¦¬ ìƒì„± (ìµœì†Œí•œë§Œ ìœ ì§€)
for d in [UPLOAD_FOLDER, CACHE_ROOT_DIR]:
    os.makedirs(d, exist_ok=True)

def get_file_cache_dirs(file_path=None):
    """
    íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    ğŸ¯ ê° íŒŒì¼ë§ˆë‹¤ ë…ë¦½ì ì¸ ëª¨ë¸, ì˜ˆì¸¡, ì‹œê°í™” ìºì‹œ ì œê³µ
    """
    try:
        if not file_path:
            file_path = prediction_state.get('current_file', None)
        
        # Debug: file cache directory setup
        
        if not file_path:
            logger.warning(f"âš ï¸ No file path provided and no current_file in prediction_state")
            # ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬ ë°˜í™˜ (íŒŒì¼ë³„ ìºì‹œ ì—†ì´)
            default_cache_root = Path(CACHE_ROOT_DIR) / 'default'
            dirs = {
                'root': default_cache_root,
                'models': default_cache_root / 'models',
                'predictions': default_cache_root / 'predictions',
                'plots': default_cache_root / 'static' / 'plots',
                'ma_plots': default_cache_root / 'static' / 'ma_plots',
                'accumulated': default_cache_root / 'accumulated'
            }
            
            # Create default directories
            for name, dir_path in dirs.items():
                try:
                    dir_path.mkdir(parents=True, exist_ok=True)
                except Exception as e:
                    logger.error(f"âŒ Failed to create default {name} directory {dir_path}: {str(e)}")
            
            logger.warning(f"âš ï¸ Using default cache directory")
            return dirs
        
        if not os.path.exists(file_path):
            logger.error(f"âŒ File does not exist: {file_path}")
            raise ValueError(f"File does not exist: {file_path}")
        
        # Generate file cache directory
        file_content_hash = get_data_content_hash(file_path)
        
        if not file_content_hash:
            logger.error(f"âŒ Failed to get content hash for file: {file_path}")
            raise ValueError(f"Unable to generate content hash for file: {file_path}")
        
        file_name = Path(file_path).stem
        file_dir_name = f"{file_content_hash[:12]}_{file_name}"
        file_cache_root = Path(CACHE_ROOT_DIR) / file_dir_name
        
        dirs = {
            'root': file_cache_root,
            'models': file_cache_root / 'models',
            'predictions': file_cache_root / 'predictions',
            'plots': file_cache_root / 'static' / 'plots',
            'ma_plots': file_cache_root / 'static' / 'ma_plots',
            'accumulated': file_cache_root / 'accumulated'
        }
        
        # Create cache directories
        for name, dir_path in dirs.items():
            try:
                dir_path.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                logger.error(f"âŒ Failed to create {name} directory {dir_path}: {str(e)}")
        
        return dirs
        
    except Exception as e:
        logger.error(f"âŒ Error in get_file_cache_dirs: {str(e)}")
        logger.error(traceback.format_exc())
        raise e  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì „íŒŒ

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',  # ë” ê°„ê²°í•œ ë¡œê·¸ í¬ë§·
    handlers=[
        logging.FileHandler("app.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Flask ì„¤ì •
app = Flask(__name__)
CORS(app, resources={r"/api/*": {"origins": "*"}}, supports_credentials=False)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 32 * 1024 * 1024  # ìµœëŒ€ íŒŒì¼ í¬ê¸° 32MBë¡œ ì¦ê°€

# ì „ì—­ ìƒíƒœ ë³€ìˆ˜ì— ìƒˆ í•„ë“œ ì¶”ê°€
prediction_state = {
    'current_data': None,
    'latest_predictions': None,
    'latest_interval_scores': None,
    'latest_attention_data': None,
    'latest_ma_results': None,
    'latest_plots': None,  # ì¶”ê°€
    'latest_metrics': None,  # ì¶”ê°€
    'current_date': None,
    'current_file': None,  # ì¶”ê°€: í˜„ì¬ íŒŒì¼ ê²½ë¡œ
    'is_predicting': False,
    'prediction_progress': 0,
    'error': None,
    'selected_features': None,
    'feature_importance': None,
    'semimonthly_period': None,
    'next_semimonthly_period': None,
    'accumulated_predictions': [],
    'accumulated_metrics': {},
    'prediction_dates': [],
    'accumulated_consistency_scores': {},
}

# ë°ì´í„° ë¡œë”ì˜ ì›Œì»¤ ì‹œë“œ ê³ ì •ì„ ìœ„í•œ í•¨ìˆ˜
def seed_worker(worker_id):
    worker_seed = SEED
    np.random.seed(worker_seed)
    random.seed(worker_seed)

# ë°ì´í„° ë¡œë”ì˜ ìƒì„±ì ì‹œë“œ ê³ ì •
g = torch.Generator()
g.manual_seed(SEED)

#######################################################################
# ëª¨ë¸ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
#######################################################################

# ë‚ ì§œ í¬ë§·íŒ… ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def format_date(date_obj, format_str='%Y-%m-%d'):
    """ë‚ ì§œ ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
    try:
        # pandas Timestamp ë˜ëŠ” datetime.datetime
        if hasattr(date_obj, 'strftime'):
            return date_obj.strftime(format_str)
        
        # numpy.datetime64
        elif isinstance(date_obj, np.datetime64):
            # ë‚ ì§œ í¬ë§·ì´ 'YYYY-MM-DD'ì¸ ê²½ìš°
            return str(date_obj)[:10]
        
        # ë¬¸ìì—´ì¸ ê²½ìš° ì´ë¯¸ ë‚ ì§œ í˜•ì‹ì´ë¼ë©´ ì¶”ê°€ ì²˜ë¦¬
        elif isinstance(date_obj, str):
            # GMT í˜•ì‹ì´ë©´ íŒŒì‹±í•˜ì—¬ ë³€í™˜
            if 'GMT' in date_obj:
                parsed_date = datetime.strptime(date_obj, '%a, %d %b %Y %H:%M:%S GMT')
                return parsed_date.strftime(format_str)
            return date_obj[:10] if len(date_obj) > 10 else date_obj
        
        # ê·¸ ì™¸ ê²½ìš°
        else:
            return str(date_obj)
    
    except Exception as e:
        logger.warning(f"ë‚ ì§œ í¬ë§·íŒ… ì˜¤ë¥˜: {str(e)}")
        return str(date_obj)

# ğŸ”§ ìŠ¤ë§ˆíŠ¸ íŒŒì¼ ìºì‹œ ì‹œìŠ¤í…œ í•¨ìˆ˜ë“¤
def calculate_file_hash(file_path, chunk_size=8192):
    """íŒŒì¼ ë‚´ìš©ì˜ SHA256 í•´ì‹œë¥¼ ê³„ì‚°"""
    import hashlib
    
    hash_sha256 = hashlib.sha256()
    try:
        with open(file_path, 'rb') as f:
            while chunk := f.read(chunk_size):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        logger.error(f"File hash calculation failed: {str(e)}")
        return None

def get_data_content_hash(file_path):
    """CSV íŒŒì¼ì˜ ë°ì´í„° ë‚´ìš©ë§Œìœ¼ë¡œ í•´ì‹œ ìƒì„± (ë‚ ì§œ ìˆœì„œ ê¸°ì¤€)"""
    import hashlib
    
    try:
        df = pd.read_csv(file_path)
        if 'Date' in df.columns:
            # ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¼ê´€ëœ í•´ì‹œ ìƒì„±
            df = df.sort_values('Date')
        
        # ë°ì´í„°í”„ë ˆì„ì˜ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í•´ì‹œ ê³„ì‚°
        content_str = df.to_string()
        return hashlib.sha256(content_str.encode()).hexdigest()[:16]  # ì§§ì€ í•´ì‹œ ì‚¬ìš©
    except Exception as e:
        logger.error(f"Data content hash calculation failed: {str(e)}")
        return None

def check_data_extension(old_file_path, new_file_path):
    """
    ìƒˆ íŒŒì¼ì´ ê¸°ì¡´ íŒŒì¼ì˜ ìˆœì°¨ì  í™•ì¥(ê¸°ì¡´ ë°ì´í„° ì´í›„ì—ë§Œ ìƒˆ í–‰ ì¶”ê°€)ì¸ì§€ ì—„ê²©í•˜ê²Œ í™•ì¸
    
    âš ï¸ ì¤‘ìš”: ë‹¤ìŒ ê²½ìš°ë§Œ í™•ì¥ìœ¼ë¡œ ì¸ì •:
    1. ê¸°ì¡´ ë°ì´í„°ì™€ ì •í™•íˆ ë™ì¼í•œ ë¶€ë¶„ì´ ìˆìŒ
    2. ìƒˆ ë°ì´í„°ê°€ ê¸°ì¡´ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ ì´í›„ì—ë§Œ ì¶”ê°€ë¨
    3. ê¸°ì¡´ ë°ì´í„°ì˜ ì‹œì‘/ì¤‘ê°„ ë‚ ì§œê°€ ë³€ê²½ë˜ì§€ ì•ŠìŒ
    
    Returns:
    --------
    dict: {
        'is_extension': bool,
        'new_rows_count': int,
        'base_hash': str,  # ê¸°ì¡´ ë°ì´í„° ë¶€ë¶„ì˜ í•´ì‹œ
        'old_start_date': str,
        'old_end_date': str,
        'new_start_date': str,
        'new_end_date': str,
        'validation_details': dict
    }
    """
    try:
        old_df = pd.read_csv(old_file_path)
        new_df = pd.read_csv(new_file_path)
        
        # ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'Date' not in old_df.columns or 'Date' not in new_df.columns:
            return {
                'is_extension': False, 
                'new_rows_count': 0,
                'validation_details': {'error': 'No Date column found'}
            }
        
        # ë‚ ì§œë¡œ ì •ë ¬
        old_df = old_df.sort_values('Date').reset_index(drop=True)
        new_df = new_df.sort_values('Date').reset_index(drop=True)
        
        # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
        old_df['Date'] = pd.to_datetime(old_df['Date'])
        new_df['Date'] = pd.to_datetime(new_df['Date'])
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        old_start_date = old_df['Date'].iloc[0]
        old_end_date = old_df['Date'].iloc[-1]
        new_start_date = new_df['Date'].iloc[0]
        new_end_date = new_df['Date'].iloc[-1]
        
        logger.info(f"ğŸ” [EXTENSION_CHECK] Old data: {old_start_date.strftime('%Y-%m-%d')} ~ {old_end_date.strftime('%Y-%m-%d')} ({len(old_df)} rows)")
        logger.info(f"ğŸ” [EXTENSION_CHECK] New data: {new_start_date.strftime('%Y-%m-%d')} ~ {new_end_date.strftime('%Y-%m-%d')} ({len(new_df)} rows)")
        
        # âœ… ê²€ì¦ 1: ìƒˆ íŒŒì¼ì´ ë” ê¸¸ì–´ì•¼ í•¨
        if len(new_df) <= len(old_df):
            logger.info(f"âŒ [EXTENSION_CHECK] New file is not longer ({len(new_df)} <= {len(old_df)})")
            return {
                'is_extension': False, 
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'New file is not longer than old file'}
            }
        
        # âœ… ê²€ì¦ 2: ìƒˆ ë°ì´í„°ì˜ ì‹œì‘ ë‚ ì§œê°€ ê¸°ì¡´ ë°ì´í„°ì˜ ì‹œì‘ ë‚ ì§œì™€ ê°™ê±°ë‚˜ ê·¸ ì´í›„ì—¬ì•¼ í•¨
        if new_start_date < old_start_date:
            logger.info(f"âŒ [EXTENSION_CHECK] New data starts before old data ({new_start_date} < {old_start_date})")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'New data contains dates before existing data start date'}
            }
        
        # âœ… ê²€ì¦ 3: ìƒˆ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œê°€ ê¸°ì¡´ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ ì´í›„ì—¬ì•¼ í•¨
        if new_end_date <= old_end_date:
            logger.info(f"âŒ [EXTENSION_CHECK] New data doesn't extend beyond old data ({new_end_date} <= {old_end_date})")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'New data does not extend beyond existing end date'}
            }
        
        # âœ… ê²€ì¦ 4: ê¸°ì¡´ ë°ì´í„°ì˜ ëª¨ë“  ë‚ ì§œê°€ ìƒˆ ë°ì´í„°ì— í¬í•¨ë˜ì–´ì•¼ í•¨
        old_dates = set(old_df['Date'].dt.strftime('%Y-%m-%d'))
        new_dates = set(new_df['Date'].dt.strftime('%Y-%m-%d'))
        
        missing_dates = old_dates - new_dates
        if missing_dates:
            logger.info(f"âŒ [EXTENSION_CHECK] Some old dates are missing in new data: {missing_dates}")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': f'Missing dates from old data: {list(missing_dates)}'}
            }
        
        # âœ… ê²€ì¦ 5: ì»¬ëŸ¼ì´ ë™ì¼í•´ì•¼ í•¨
        if list(old_df.columns) != list(new_df.columns):
            logger.info(f"âŒ [EXTENSION_CHECK] Column structure differs")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'Column structure differs'}
            }
        
        # âœ… ê²€ì¦ 6: ê¸°ì¡´ ë°ì´í„° ë¶€ë¶„ì´ ì •í™•íˆ ë™ì¼í•œì§€ í™•ì¸ (ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­)
        logger.info(f"ğŸ” [EXTENSION_CHECK] Comparing overlapping data...")
        
        # ê¸°ì¡´ ë°ì´í„°ì˜ ê° ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ìƒˆ ë°ì´í„° í–‰ ì°¾ê¸°
        data_matches = True
        mismatch_details = []
        
        for idx, old_row in old_df.iterrows():
            old_date = old_row['Date']
            old_date_str = old_date.strftime('%Y-%m-%d')
            
            # ìƒˆ ë°ì´í„°ì—ì„œ í•´ë‹¹ ë‚ ì§œ ì°¾ê¸°
            new_matching_rows = new_df[new_df['Date'] == old_date]
            
            if len(new_matching_rows) == 0:
                data_matches = False
                mismatch_details.append(f"Date {old_date_str} missing in new data")
                break
            elif len(new_matching_rows) > 1:
                data_matches = False
                mismatch_details.append(f"Duplicate date {old_date_str} in new data")
                break
            
            new_row = new_matching_rows.iloc[0]
            
            # ìˆ˜ì¹˜ ì»¬ëŸ¼ ë¹„êµ (Date ì œì™¸)
            numeric_cols = old_df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if not np.allclose([old_row[col]], [new_row[col]], rtol=1e-10, atol=1e-12, equal_nan=True):
                    data_matches = False
                    mismatch_details.append(f"Value mismatch on {old_date_str}, column {col}: {old_row[col]} != {new_row[col]}")
                    break
            
            if not data_matches:
                break
            
            # ë¬¸ìì—´ ì»¬ëŸ¼ ë¹„êµ (Date ì œì™¸)
            str_cols = old_df.select_dtypes(include=['object']).columns
            str_cols = [col for col in str_cols if col != 'Date']
            for col in str_cols:
                if old_row[col] != new_row[col]:
                    data_matches = False
                    mismatch_details.append(f"String mismatch on {old_date_str}, column {col}: '{old_row[col]}' != '{new_row[col]}'")
                    break
            
            if not data_matches:
                break
        
        if not data_matches:
            logger.info(f"âŒ [EXTENSION_CHECK] Data content differs: {mismatch_details}")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'Data content differs', 'details': mismatch_details}
            }
        
        # âœ… ê²€ì¦ 7: ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„°ê°€ ê¸°ì¡´ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ ì´í›„ì—ë§Œ ìˆëŠ”ì§€ í™•ì¸
        new_only_dates = new_dates - old_dates
        if new_only_dates:
            new_only_dates_dt = [pd.to_datetime(date) for date in new_only_dates]
            earliest_new_date = min(new_only_dates_dt)
            
            if earliest_new_date <= old_end_date:
                logger.info(f"âŒ [EXTENSION_CHECK] New dates are not strictly after old end date: {earliest_new_date} <= {old_end_date}")
                return {
                    'is_extension': False,
                    'new_rows_count': 0,
                    'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                    'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                    'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                    'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                    'validation_details': {'reason': 'New dates are not strictly sequential after old end date'}
                }
        
        # âœ… ëª¨ë“  ê²€ì¦ í†µê³¼: ìˆœì°¨ì  í™•ì¥ìœ¼ë¡œ ì¸ì •
        new_rows_count = len(new_only_dates)
        base_hash = get_data_content_hash(old_file_path)
        
        logger.info(f"âœ… [EXTENSION_CHECK] Valid sequential extension: +{new_rows_count} new dates after {old_end_date.strftime('%Y-%m-%d')}")
        
        return {
            'is_extension': True,
            'new_rows_count': new_rows_count,
            'base_hash': base_hash,
            'old_start_date': old_start_date.strftime('%Y-%m-%d'),
            'old_end_date': old_end_date.strftime('%Y-%m-%d'),
            'new_start_date': new_start_date.strftime('%Y-%m-%d'),
            'new_end_date': new_end_date.strftime('%Y-%m-%d'),
            'validation_details': {
                'reason': 'Valid sequential extension',
                'new_dates_added': sorted(list(new_only_dates))
            }
        }
        
    except Exception as e:
        logger.error(f"Data extension check failed: {str(e)}")
        return {
            'is_extension': False, 
            'new_rows_count': 0,
            'old_start_date': None,
            'old_end_date': None,
            'new_start_date': None,
            'new_end_date': None,
            'validation_details': {'error': str(e)}
        }

def find_compatible_cache_file(new_file_path):
    """
    ìƒˆ íŒŒì¼ê³¼ í˜¸í™˜ë˜ëŠ” ê¸°ì¡´ ìºì‹œë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    
    Returns:
    --------
    dict: {
        'found': bool,
        'cache_type': str,  # 'exact', 'extension', None
        'cache_file': str,
        'extension_info': dict
    }
    """
    try:
        # ìƒˆ íŒŒì¼ì˜ ë°ì´í„° í•´ì‹œ
        new_hash = get_data_content_hash(new_file_path)
        if not new_hash:
            return {'found': False, 'cache_type': None}
        
        # uploads í´ë”ì˜ ëª¨ë“  CSV íŒŒì¼ í™•ì¸
        upload_dir = Path(UPLOAD_FOLDER)
        existing_files = list(upload_dir.glob('*.csv'))
        
        logger.info(f"ğŸ” [CACHE] Checking {len(existing_files)} existing files for cache compatibility")
        
        for existing_file in existing_files:
            if existing_file.name == os.path.basename(new_file_path):
                continue  # ìê¸° ìì‹ ì€ ì œì™¸
            
            try:
                # 1. ì •í™•í•œ ë§¤ì¹˜ í™•ì¸
                existing_hash = get_data_content_hash(str(existing_file))
                if existing_hash == new_hash:
                    logger.info(f"âœ… [CACHE] Found exact match: {existing_file.name}")
                    return {
                        'found': True,
                        'cache_type': 'exact',
                        'cache_file': str(existing_file),
                        'extension_info': None
                    }
                
                # 2. í™•ì¥ íŒŒì¼ì¸ì§€ í™•ì¸
                extension_info = check_data_extension(str(existing_file), new_file_path)
                if extension_info['is_extension']:
                    logger.info(f"ğŸ“ˆ [CACHE] Found extension base: {existing_file.name} (+{extension_info['new_rows_count']} rows)")
                    return {
                        'found': True,
                        'cache_type': 'extension',
                        'cache_file': str(existing_file),
                        'extension_info': extension_info
                    }
                    
            except Exception as e:
                logger.warning(f"Error checking file {existing_file}: {str(e)}")
                continue
        
        logger.info("âŒ [CACHE] No compatible cache found")
        return {'found': False, 'cache_type': None}
        
    except Exception as e:
        logger.error(f"Cache compatibility check failed: {str(e)}")
        return {'found': False, 'cache_type': None}

# ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ í•¨ìˆ˜
def load_data(file_path):
    """ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬"""
    logger.info("Loading data...")
    df = pd.read_csv(file_path)
    df['Date'] = pd.to_datetime(df['Date'])
    df.set_index('Date', inplace=True)
    
    logger.info(f"Original data shape: {df.shape}")
    
    # ëª¨ë“  inf ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
    df = df.replace([np.inf, -np.inf], np.nan)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - ëª¨ë“  ì»¬ëŸ¼ì— ë™ì¼í•˜ê²Œ ì ìš©
    df = df.fillna(method='ffill').fillna(method='bfill')
    
    # ì²˜ë¦¬ í›„ ë‚¨ì•„ìˆëŠ” infë‚˜ nan í™•ì¸
    if df.isnull().any().any() or np.isinf(df.values).any():
        logger.warning("Dataset still contains NaN or inf values after preprocessing")
        # ë¬¸ì œê°€ ìˆëŠ” ì—´ ì¶œë ¥
        problematic_cols = df.columns[
            df.isnull().any() | np.isinf(df).any()
        ]
        logger.warning(f"Problematic columns: {problematic_cols}")
        
        # ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬: ë‚¨ì€ inf/nan ê°’ì„ í•´ë‹¹ ì»¬ëŸ¼ì˜ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
        for col in problematic_cols:
            col_mean = df[col].replace([np.inf, -np.inf], np.nan).mean()
            df[col] = df[col].replace([np.inf, -np.inf], np.nan).fillna(col_mean)
    
    logger.info(f"Final shape after preprocessing: {df.shape}")
    return df

# ë³€ìˆ˜ ê·¸ë£¹ ì •ì˜
variable_groups = {
    'crude_oil': ['WTI', 'Brent', 'Dubai'],
    'gasoline': ['Gasoline_92RON', 'Gasoline_95RON', 'Europe_M.G_10ppm', 'RBOB (NYMEX)_M1'],
    'naphtha': ['MOPAG', 'MOPS', 'Europe_CIF NWE'],
    'lpg': ['C3_LPG', 'C4_LPG'],
    'product': ['EL_CRF NEA', 'EL_CRF SEA', 'PL_FOB Korea', 'BZ_FOB Korea', 'BZ_FOB SEA', 'BZ_FOB US M1', 'BZ_FOB US M2', 'TL_FOB Korea', 'TL_FOB US M1', 'TL_FOB US M2',
    'MX_FOB Korea', 'PX_FOB Korea', 'SM_FOB Korea', 'RPG Value_FOB PG', 'FO_HSFO 180 CST', 'MTBE_FOB Singapore'],
    'spread': ['biweekly Spread','BZ_H2-TIME SPREAD', 'Brent_WTI', 'MOPJ_MOPAG', 'MOPJ_MOPS', 'Naphtha_Spread', 'MG92_E Nap', 'C3_MOPJ', 'C4_MOPJ', 'Nap_Dubai',
    'MG92_Nap_MOPS', '95R_92R_Asia', 'M1_M2_RBOB', 'RBOB_Brent_m1', 'RBOB_Brent_m2', 'EL_MOPJ', 'PL_MOPJ', 'BZ_MOPJ', 'TL_MOPJ', 'PX_MOPJ', 'HD_EL', 'LD_EL', 'LLD_EL', 'PP_PL',
    'SM_EL+BZ', 'US_FOBK_BZ', 'NAP_HSFO_180', 'MTBE_MOPJ'],
    'economics': ['Dow_Jones', 'Euro', 'Gold'],
    'freight': ['Freight_55_PG', 'Freight_55_Maili', 'Freight_55_Yosu', 'Freight_55_Daes', 'Freight_55_Chiba',
    'Freight_75_PG', 'Freight_75_Maili', 'Freight_75_Yosu', 'Freight_75_Daes', 'Freight_75_Chiba', 'Flat Rate_PG', 'Flat Rate_Maili', 'Flat Rate_Yosu', 'Flat Rate_Daes',
    'Flat Rate_Chiba'],
    'ETF': ['DIG', 'DUG', 'IYE', 'VDE', 'XLE']
}

def load_holidays_from_file(filepath=None):
    """
    CSV ë˜ëŠ” Excel íŒŒì¼ì—ì„œ íœ´ì¼ ëª©ë¡ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        filepath (str): íœ´ì¼ ëª©ë¡ íŒŒì¼ ê²½ë¡œ, Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
    
    Returns:
        set: íœ´ì¼ ë‚ ì§œ ì§‘í•© (YYYY-MM-DD í˜•ì‹)
    """
    # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ - holidays í´ë”ë¡œ ë³€ê²½
    if filepath is None:
        holidays_dir = Path('holidays')
        holidays_dir.mkdir(exist_ok=True)
        filepath = str(holidays_dir / 'holidays.csv')
    
    # íŒŒì¼ í™•ì¥ì í™•ì¸
    _, ext = os.path.splitext(filepath)
    
    # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ íœ´ì¼ ëª©ë¡ ìƒì„±
    if not os.path.exists(filepath):
        logger.warning(f"Holiday file {filepath} not found. Creating default holiday file.")
        
        # ê¸°ë³¸ 2025ë…„ ì‹±ê°€í´ ê³µíœ´ì¼
        default_holidays = [
            "2025-01-01", "2025-01-29", "2025-01-30", "2025-03-31", "2025-04-18", 
            "2025-05-01", "2025-05-12", "2025-06-07", "2025-08-09", "2025-10-20", 
            "2025-12-25", "2026-01-01"
        ]
        
        # ê¸°ë³¸ íŒŒì¼ ìƒì„±
        df = pd.DataFrame({'date': default_holidays, 'description': ['Singapore Holiday']*len(default_holidays)})
        
        if ext.lower() == '.xlsx':
            df.to_excel(filepath, index=False)
        else:
            df.to_csv(filepath, index=False)
        
        logger.info(f"Created default holiday file at {filepath}")
        return set(default_holidays)
    
    try:
        # íŒŒì¼ ë¡œë“œ
        if ext.lower() == '.xlsx':
            df = pd.read_excel(filepath)
        else:
            df = pd.read_csv(filepath)
        
        # 'date' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'date' not in df.columns:
            logger.error(f"Holiday file {filepath} does not have 'date' column")
            return set()
        
        # ë‚ ì§œ í˜•ì‹ í‘œì¤€í™”
        holidays = set()
        for date_str in df['date']:
            try:
                date = pd.to_datetime(date_str)
                holidays.add(date.strftime('%Y-%m-%d'))
            except:
                logger.warning(f"Invalid date format: {date_str}")
        
        logger.info(f"Loaded {len(holidays)} holidays from {filepath}")
        return holidays
        
    except Exception as e:
        logger.error(f"Error loading holiday file: {str(e)}")
        logger.error(traceback.format_exc())
        return set()

# ì „ì—­ ë³€ìˆ˜ë¡œ íœ´ì¼ ì§‘í•© ê´€ë¦¬
holidays = load_holidays_from_file()

def is_holiday(date):
    """ì£¼ì–´ì§„ ë‚ ì§œê°€ íœ´ì¼ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    date_str = format_date(date, '%Y-%m-%d')
    return date_str in holidays

# ë°ì´í„°ì—ì„œ í‰ì¼ ë¹ˆ ë‚ ì§œë¥¼ íœ´ì¼ë¡œ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
def detect_missing_weekdays_as_holidays(df, date_column='Date'):
    """
    ë°ì´í„°í”„ë ˆì„ì—ì„œ í‰ì¼(ì›”~ê¸ˆ)ì¸ë° ë°ì´í„°ê°€ ì—†ëŠ” ë‚ ì§œë“¤ì„ íœ´ì¼ë¡œ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        df (pd.DataFrame): ë°ì´í„°í”„ë ˆì„
        date_column (str): ë‚ ì§œ ì»¬ëŸ¼ëª…
    
    Returns:
        set: ê°ì§€ëœ íœ´ì¼ ë‚ ì§œ ì§‘í•© (YYYY-MM-DD í˜•ì‹)
    """
    if df.empty or date_column not in df.columns:
        return set()
    
    try:
        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        df_dates = pd.to_datetime(df[date_column]).dt.date
        date_set = set(df_dates)
        
        # ë°ì´í„° ë²”ìœ„ì˜ ì²« ë‚ ê³¼ ë§ˆì§€ë§‰ ë‚ 
        start_date = min(df_dates)
        end_date = max(df_dates)
        
        # ì „ì²´ ê¸°ê°„ì˜ ëª¨ë“  í‰ì¼ ìƒì„±
        current_date = start_date
        missing_weekdays = set()
        
        while current_date <= end_date:
            # í‰ì¼ì¸ì§€ í™•ì¸ (ì›”ìš”ì¼=0, ì¼ìš”ì¼=6)
            if current_date.weekday() < 5:  # ì›”~ê¸ˆ
                if current_date not in date_set:
                    missing_weekdays.add(current_date.strftime('%Y-%m-%d'))
            current_date += pd.Timedelta(days=1)
        
        logger.info(f"Detected {len(missing_weekdays)} missing weekdays as potential holidays")
        if missing_weekdays:
            logger.info(f"Missing weekdays sample: {list(missing_weekdays)[:10]}")
        
        return missing_weekdays
        
    except Exception as e:
        logger.error(f"Error detecting missing weekdays: {str(e)}")
        return set()

# íœ´ì¼ ì •ë³´ì™€ ë°ì´í„° ë¹ˆ ë‚ ì§œë¥¼ ê²°í•©í•˜ëŠ” í•¨ìˆ˜
def get_combined_holidays(df=None, filepath=None):
    """
    íœ´ì¼ íŒŒì¼ì˜ íœ´ì¼ê³¼ ë°ì´í„°ì—ì„œ ê°ì§€ëœ íœ´ì¼ì„ ê²°í•©í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        df (pd.DataFrame): ë°ì´í„°í”„ë ˆì„ (ë¹ˆ ë‚ ì§œ ê°ì§€ìš©)
        filepath (str): íœ´ì¼ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        set: ê²°í•©ëœ íœ´ì¼ ë‚ ì§œ ì§‘í•©
    """
    # íœ´ì¼ íŒŒì¼ì—ì„œ íœ´ì¼ ë¡œë“œ
    file_holidays = load_holidays_from_file(filepath)
    
    # ë°ì´í„°ì—ì„œ ë¹ˆ í‰ì¼ ê°ì§€
    data_holidays = set()
    if df is not None:
        data_holidays = detect_missing_weekdays_as_holidays(df)
    
    # ë‘ ì„¸íŠ¸ ê²°í•©
    combined_holidays = file_holidays.union(data_holidays)
    
    logger.info(f"Combined holidays: {len(file_holidays)} from file + {len(data_holidays)} from data = {len(combined_holidays)} total")
    
    return combined_holidays

# íœ´ì¼ ì •ë³´ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_holidays(filepath=None, df=None):
    """íœ´ì¼ ì •ë³´ë¥¼ ì¬ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (ë°ì´í„° ë¹ˆ ë‚ ì§œ í¬í•¨)"""
    global holidays
    holidays = get_combined_holidays(df, filepath)
    return holidays

# TimeSeriesDataset ë° í‰ê°€ ë©”íŠ¸ë¦­ìŠ¤
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y, device, prev_values=None):
        if isinstance(X, torch.Tensor):
            self.X = X
            self.y = y
        else:
            self.X = torch.FloatTensor(X).to(device)
            self.y = torch.FloatTensor(y).to(device)
        
        if prev_values is not None:
            if isinstance(prev_values, torch.Tensor):
                self.prev_values = prev_values
            else:
                self.prev_values = torch.FloatTensor(prev_values).to(device)
        else:
            self.prev_values = None
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        if self.prev_values is not None:
            return self.X[idx], self.y[idx], self.prev_values[idx]
        return self.X[idx], self.y[idx]

# ë³µí•© ì†ì‹¤ í•¨ìˆ˜
def composite_loss(pred, target, prev_value, alpha=0.6, beta=0.2, gamma=0.15, delta=0.05, eps=1e-8):
    """
    Composite loss for:
      - MSE loss (for overall price accuracy)
      - Volatility loss (to match the magnitude of changes)
      - Directional loss (surrogate for F1-score improvement)
      - Continuity loss (to ensure smooth transition)
    """
    # MSE Loss
    loss_mse = F.mse_loss(pred, target)
    
    # Volatility Loss
    pred_diff = pred[:, 1:] - pred[:, :-1]
    target_diff = target[:, 1:] - target[:, :-1]
    loss_vol = torch.mean(torch.abs(torch.log1p(torch.abs(pred_diff) + eps) - 
                                  torch.log1p(torch.abs(target_diff) + eps)))
    
    # Directional Loss
    cos_sim = torch.cosine_similarity(pred_diff, target_diff, dim=-1)
    loss_dir = 1 - cos_sim.mean()
    
    # Continuity Loss
    loss_cont = torch.mean(torch.abs(pred[:, 0:1] - prev_value))
    
    total_loss = alpha * loss_mse + beta * loss_vol + gamma * loss_dir + delta * loss_cont
    loss_info = {
        'mse_loss': loss_mse.item(),
        'volatility_loss': loss_vol.item(),
        'directional_loss': loss_dir.item(),
        'continuity_loss': loss_cont.item(),
        'total_loss': total_loss.item()
    }
    return total_loss, loss_info

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ í´ë˜ìŠ¤
class CombinedScheduler:
    def __init__(self, optimizer, warmup_steps, max_lr, factor=0.5, patience=5, min_lr=1e-6):
        self.warmup_scheduler = WarmupScheduler(
            optimizer=optimizer,
            warmup_steps=warmup_steps,
            max_lr=max_lr
        )
        self.plateau_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer=optimizer,
            mode='min',
            factor=factor,
            patience=patience,
            verbose=True,
            min_lr=min_lr
        )
        self.current_step = 0
        self.warmup_steps = warmup_steps
    
    def step(self, val_loss=None):
        # Warmup ë‹¨ê³„ì—ì„œëŠ” ë§¤ ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œ
        if self.current_step < self.warmup_steps:
            self.warmup_scheduler.step()
            self.current_step += 1
        # Warmup ì´í›„ì—ëŠ” validation lossì— ë”°ë¼ ì¡°ì •
        elif val_loss is not None:
            self.plateau_scheduler.step(val_loss)

class WarmupScheduler:
    def __init__(self, optimizer, warmup_steps, max_lr):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.max_lr = max_lr
        self.current_step = 0

    def step(self):
        self.current_step += 1
        # warmup ë‹¨ê³„ ë™ì•ˆ ì„ í˜• ì¦ê°€
        lr = self.max_lr * self.current_step / self.warmup_steps
        # warmup ë‹¨ê³„ë¥¼ ì´ˆê³¼í•˜ë©´ max_lrë¡œ ê³ ì •
        if self.current_step > self.warmup_steps:
            lr = self.max_lr
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

# ê°œì„ ëœ LSTM ì˜ˆì¸¡ ëª¨ë¸
class ImprovedLSTMPredictor(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2, output_size=23):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # hidden_sizeë¥¼ 8ì˜ ë°°ìˆ˜ë¡œ ì¡°ì •
        self.adjusted_hidden = (hidden_size // 8) * 8
        if self.adjusted_hidden < 32:
            self.adjusted_hidden = 32
        
        # LSTM dropout ì„¤ì •
        self.lstm_dropout = 0.0 if num_layers == 1 else dropout
        
        # ê³„ì¸µì  LSTM êµ¬ì¡°
        self.lstm_layers = nn.ModuleList([
            nn.LSTM(
                input_size=input_size if i == 0 else self.adjusted_hidden,
                hidden_size=self.adjusted_hidden,
                num_layers=1,
                batch_first=True
            ) for i in range(num_layers)
        ])
        
        # ë“€ì–¼ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.temporal_attention = nn.MultiheadAttention(
            self.adjusted_hidden,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        self.feature_attention = nn.MultiheadAttention(
            self.adjusted_hidden,
            num_heads=4,
            dropout=dropout,
            batch_first=True
        )
        
        # Layer Normalization
        self.layer_norms = nn.ModuleList([
            nn.LayerNorm(self.adjusted_hidden) for _ in range(num_layers)
        ])
        
        self.final_layer_norm = nn.LayerNorm(self.adjusted_hidden)
        
        # Dropout ë ˆì´ì–´
        self.dropout_layer = nn.Dropout(dropout)
        
        # ì´ì „ ê°’ ì •ë³´ë¥¼ ê²°í•©í•˜ê¸° ìœ„í•œ ë ˆì´ì–´
        self.prev_value_encoder = nn.Sequential(
            nn.Linear(1, self.adjusted_hidden // 4),
            nn.ReLU(),
            nn.Linear(self.adjusted_hidden // 4, self.adjusted_hidden)
        )
        
        # ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œì„ ìœ„í•œ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
        self.conv_layers = nn.Sequential(
            nn.Conv1d(self.adjusted_hidden, self.adjusted_hidden, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(self.adjusted_hidden, self.adjusted_hidden, kernel_size=3, padding=1),
            nn.ReLU()
        )
        
        # ì¶œë ¥ ë ˆì´ì–´ - ê³„ì¸µì  êµ¬ì¡°
        self.output_layers = nn.ModuleList([
            nn.Linear(self.adjusted_hidden, self.adjusted_hidden // 2),
            nn.Linear(self.adjusted_hidden // 2, self.adjusted_hidden // 4),
            nn.Linear(self.adjusted_hidden // 4, output_size)
        ])
        
        # ì”ì°¨ ì—°ê²°ì„ ìœ„í•œ í”„ë¡œì ì…˜ ë ˆì´ì–´
        self.residual_proj = nn.Linear(self.adjusted_hidden, output_size)
        
    def forward(self, x, prev_value=None, return_attention=False):
        batch_size = x.size(0)
        
        # ê³„ì¸µì  LSTM ì²˜ë¦¬
        lstm_out = x
        skip_connections = []
        
        for i, (lstm, layer_norm) in enumerate(zip(self.lstm_layers, self.layer_norms)):
            lstm_out, _ = lstm(lstm_out)
            lstm_out = layer_norm(lstm_out)
            lstm_out = self.dropout_layer(lstm_out)
            skip_connections.append(lstm_out)
        
        # ì‹œê°„ì  ì–´í…ì…˜
        temporal_context, temporal_weights = self.temporal_attention(lstm_out, lstm_out, lstm_out)
        temporal_context = self.dropout_layer(temporal_context)
        
        # íŠ¹ì§• ì–´í…ì…˜
        # íŠ¹ì§• ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (B, L, H) -> (B, H, L)
        feature_input = lstm_out.transpose(1, 2)
        feature_input = self.conv_layers(feature_input)
        feature_input = feature_input.transpose(1, 2)
        
        feature_context, feature_weights = self.feature_attention(feature_input, feature_input, feature_input)
        feature_context = self.dropout_layer(feature_context)
        
        # ì»¨í…ìŠ¤íŠ¸ ê²°í•©
        combined_context = temporal_context + feature_context
        for skip in skip_connections:
            combined_context = combined_context + skip
        
        combined_context = self.final_layer_norm(combined_context)
        
        # ì´ì „ ê°’ ì •ë³´ ì²˜ë¦¬
        if prev_value is not None:
            prev_value = prev_value.unsqueeze(1) if len(prev_value.shape) == 1 else prev_value
            prev_encoded = self.prev_value_encoder(prev_value)
            combined_context = combined_context + prev_encoded.unsqueeze(1)
        
        # ìµœì¢… íŠ¹ì§• ì¶”ì¶œ (ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤)
        final_features = combined_context[:, -1, :]
        
        # ê³„ì¸µì  ì¶œë ¥ ì²˜ë¦¬
        out = final_features
        residual = self.residual_proj(final_features)
        
        for i, layer in enumerate(self.output_layers):
            out = layer(out)
            if i < len(self.output_layers) - 1:
                out = F.relu(out)
                out = self.dropout_layer(out)
        
        # ì”ì°¨ ì—°ê²° ì¶”ê°€
        out = out + residual
        
        if return_attention:
            attention_weights = {
                'temporal_weights': temporal_weights,
                'feature_weights': feature_weights
            }
            return out, attention_weights
        
        return out
        
    def get_attention_maps(self, x, prev_value=None):
        """ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ë§µì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        with torch.no_grad():
            # forward ë©”ì„œë“œì— return_attention=True ì „ë‹¬
            _, attention_weights = self.forward(x, prev_value, return_attention=True)
            
            # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í‰ê·  ê³„ì‚° (multi-head -> single map)
            temporal_weights = attention_weights['temporal_weights'].mean(dim=1)  # í—¤ë“œ í‰ê· 
            feature_weights = attention_weights['feature_weights'].mean(dim=1)    # í—¤ë“œ í‰ê· 
            
            return {
                'temporal_weights': temporal_weights.cpu().numpy(),
                'feature_weights': feature_weights.cpu().numpy()
            }

#######################################################################
# ë°˜ì›” ê¸°ê°„ ê´€ë ¨ í•¨ìˆ˜
#######################################################################

# 1. ë°˜ì›” ê¸°ê°„ ê³„ì‚° í•¨ìˆ˜
def get_semimonthly_period(date):
    """
    ë‚ ì§œë¥¼ ë°˜ì›” ê¸°ê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    - 1ì¼~15ì¼: "YYYY-MM-SM1"
    - 16ì¼~ë§ì¼: "YYYY-MM-SM2"
    """
    year = date.year
    month = date.month
    day = date.day
    
    if day <= 15:
        semimonthly = f"{year}-{month:02d}-SM1"
    else:
        semimonthly = f"{year}-{month:02d}-SM2"
    
    return semimonthly

# 2. íŠ¹ì • ë‚ ì§œ ì´í›„ì˜ ë‹¤ìŒ ë°˜ì›” ê¸°ê°„ ê³„ì‚° í•¨ìˆ˜
def get_next_semimonthly_period(date):
    """
    ì£¼ì–´ì§„ ë‚ ì§œ ì´í›„ì˜ ë‹¤ìŒ ë°˜ì›” ê¸°ê°„ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    """
    year = date.year
    month = date.month
    day = date.day
    
    if day <= 15:
        # í˜„ì¬ ìƒë°˜ì›”ì´ë©´ ê°™ì€ ë‹¬ì˜ í•˜ë°˜ì›”
        semimonthly = f"{year}-{month:02d}-SM2"
    else:
        # í˜„ì¬ í•˜ë°˜ì›”ì´ë©´ ë‹¤ìŒ ë‹¬ì˜ ìƒë°˜ì›”
        if month == 12:
            # 12ì›” í•˜ë°˜ì›”ì´ë©´ ë‹¤ìŒ í•´ 1ì›” ìƒë°˜ì›”
            semimonthly = f"{year+1}-01-SM1"
        else:
            semimonthly = f"{year}-{(month+1):02d}-SM1"
    
    return semimonthly

# 3. ë°˜ì›” ê¸°ê°„ì˜ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ê³„ì‚° í•¨ìˆ˜
def get_semimonthly_date_range(semimonthly_period):
    """
    ë°˜ì›” ê¸°ê°„ ë¬¸ìì—´ì„ ë°›ì•„ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    semimonthly_period : str
        "YYYY-MM-SM1" ë˜ëŠ” "YYYY-MM-SM2" í˜•ì‹ì˜ ë°˜ì›” ê¸°ê°„
    
    Returns:
    --------
    tuple
        (ì‹œì‘ì¼, ì¢…ë£Œì¼) - datetime ê°ì²´
    """
    parts = semimonthly_period.split('-')
    year = int(parts[0])
    month = int(parts[1])
    half = parts[2]
    
    if half == "SM1":
        # ìƒë°˜ì›” (1ì¼~15ì¼)
        start_date = pd.Timestamp(year=year, month=month, day=1)
        end_date = pd.Timestamp(year=year, month=month, day=15)
    else:
        # í•˜ë°˜ì›” (16ì¼~ë§ì¼)
        start_date = pd.Timestamp(year=year, month=month, day=16)
        _, last_day = calendar.monthrange(year, month)
        end_date = pd.Timestamp(year=year, month=month, day=last_day)
    
    return start_date, end_date

# 4. ë‹¤ìŒ ë°˜ì›”ì˜ ëª¨ë“  ë‚ ì§œ ëª©ë¡ ìƒì„± í•¨ìˆ˜
def get_next_semimonthly_dates(reference_date, original_df):
    """
    ì°¸ì¡° ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë°˜ì›” ê¸°ê°„ì— ì†í•˜ëŠ” ëª¨ë“  ì˜ì—…ì¼ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    # ë‹¤ìŒ ë°˜ì›” ê¸°ê°„ ê³„ì‚°
    next_period = get_next_semimonthly_period(reference_date)
    
    logger.info(f"Calculating next semimonthly dates from reference: {format_date(reference_date)} â†’ target period: {next_period}")
    
    # ë°˜ì›” ê¸°ê°„ì˜ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ê³„ì‚°
    start_date, end_date = get_semimonthly_date_range(next_period)
    
    logger.info(f"Target period date range: {format_date(start_date)} ~ {format_date(end_date)}")
    
    # ì´ ê¸°ê°„ì— ì†í•˜ëŠ” ì˜ì—…ì¼(ì›”~ê¸ˆ, íœ´ì¼ ì œì™¸) ì„ íƒ
    business_days = []
    
    # ì›ë³¸ ë°ì´í„°ì—ì„œ ì°¾ê¸°
    future_dates = original_df.index[original_df.index > reference_date]
    for date in future_dates:
        if start_date <= date <= end_date and date.weekday() < 5 and not is_holiday(date):
            business_days.append(date)
    
    # ì›ë³¸ ë°ì´í„°ì— ì—†ëŠ” ê²½ìš°, ë‚ ì§œ ë²”ìœ„ì—ì„œ ì§ì ‘ ìƒì„±
    if len(business_days) == 0:
        logger.info(f"No business days found in original data for period {next_period}. Generating from date range.")
        current_date = start_date
        while current_date <= end_date:
            if current_date.weekday() < 5 and not is_holiday(current_date):
                business_days.append(current_date)
            current_date += pd.Timedelta(days=1)
    
    # ë‚ ì§œê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ë¡œì§
    min_required_days = 5
    if len(business_days) < min_required_days:
        logger.warning(f"Only {len(business_days)} business days found in period {next_period}. Creating synthetic dates.")
        
        if business_days:
            synthetic_date = business_days[-1] + pd.Timedelta(days=1)
        else:
            synthetic_date = max(reference_date, start_date) + pd.Timedelta(days=1)
        
        while len(business_days) < 15 and synthetic_date <= end_date:
            if synthetic_date.weekday() < 5 and not is_holiday(synthetic_date):
                business_days.append(synthetic_date)
            synthetic_date += pd.Timedelta(days=1)
        
        logger.info(f"Created synthetic dates. Total business days: {len(business_days)} for period {next_period}")
    
    business_days.sort()
    logger.info(f"Final business days for purchase interval: {len(business_days)} days in {next_period}")
    
    return business_days, next_period

# 5. ë‹¤ìŒ N ì˜ì—…ì¼ ê³„ì‚° í•¨ìˆ˜
def get_next_n_business_days(current_date, original_df, n_days=23):
    """
    í˜„ì¬ ë‚ ì§œ ì´í›„ì˜ n_days ì˜ì—…ì¼ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ - ì›ë³¸ ë°ì´í„°ì— ì—†ëŠ” ë¯¸ë˜ ë‚ ì§œë„ ìƒì„±
    íœ´ì¼(ì£¼ë§ ë° ê³µíœ´ì¼)ì€ ì œì™¸
    """
    # í˜„ì¬ ë‚ ì§œ ì´í›„ì˜ ë°ì´í„°í”„ë ˆì„ì—ì„œ ì˜ì—…ì¼ ì°¾ê¸°
    future_df = original_df[original_df.index > current_date]
    
    # í•„ìš”í•œ ìˆ˜ì˜ ì˜ì—…ì¼ ì„ íƒ
    business_days = []
    
    # ë¨¼ì € ë°ì´í„°í”„ë ˆì„ì— ìˆëŠ” ì˜ì—…ì¼ ì¶”ê°€
    for date in future_df.index:
        if date.weekday() < 5 and not is_holiday(date):  # ì›”~ê¸ˆì´ê³  íœ´ì¼ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì„ íƒ
            business_days.append(date)
        
        if len(business_days) >= n_days:
            break
    
    # ë°ì´í„°í”„ë ˆì„ì—ì„œ ì¶©ë¶„í•œ ë‚ ì§œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° í•©ì„± ë‚ ì§œ ìƒì„±
    if len(business_days) < n_days:
        # ë§ˆì§€ë§‰ ë‚ ì§œ ë˜ëŠ” í˜„ì¬ ë‚ ì§œì—ì„œ ì‹œì‘
        last_date = business_days[-1] if business_days else current_date
        
        # í•„ìš”í•œ ë§Œí¼ ì¶”ê°€ ë‚ ì§œ ìƒì„±
        current = last_date + pd.Timedelta(days=1)
        while len(business_days) < n_days:
            if current.weekday() < 5 and not is_holiday(current):  # ì›”~ê¸ˆì´ê³  íœ´ì¼ì´ ì•„ë‹Œ ê²½ìš°ë§Œ í¬í•¨
                business_days.append(current)
            current += pd.Timedelta(days=1)
    
    logger.info(f"Generated {len(business_days)} business days, excluding holidays")
    return business_days

# 6. êµ¬ê°„ë³„ í‰ê·  ê°€ê²© ê³„ì‚° ë° ì ìˆ˜ ë¶€ì—¬ í•¨ìˆ˜
def calculate_interval_averages_and_scores(predictions, business_days, min_window_size=5):
    """
    ë‹¤ìŒ ë°˜ì›” ê¸°ê°„ì— ëŒ€í•´ ë‹¤ì–‘í•œ í¬ê¸°ì˜ êµ¬ê°„ë³„ í‰ê·  ê°€ê²©ì„ ê³„ì‚°í•˜ê³  ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ëŠ” í•¨ìˆ˜
    - ë°˜ì›” ì „ì²´ ì˜ì—…ì¼ ìˆ˜ì— ë§ì¶° ìœˆë„ìš° í¬ê¸° ë²”ìœ„ ì¡°ì •
    - global_rank ë°©ì‹: ëª¨ë“  êµ¬ê°„ì„ ë¹„êµí•´ ì „ì—­ì ìœ¼ë¡œ ê°€ì¥ ì €ë ´í•œ êµ¬ê°„ì— ì ìˆ˜ ë¶€ì—¬
    
    Parameters:
    -----------
    predictions : list
        ë‚ ì§œë³„ ì˜ˆì¸¡ ê°€ê²© ì •ë³´ (ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸)
    business_days : list
        ë‹¤ìŒ ë°˜ì›”ì˜ ì˜ì—…ì¼ ëª©ë¡
    min_window_size : int
        ìµœì†Œ ê³ ë ¤í•  ìœˆë„ìš° í¬ê¸° (ê¸°ë³¸ê°’: 3)
    
    Returns:
    -----------
    tuple
        (êµ¬ê°„ë³„ í‰ê·  ê°€ê²© ì •ë³´, êµ¬ê°„ë³„ ì ìˆ˜ ì •ë³´, ë¶„ì„ ì¶”ê°€ ì •ë³´)
    """
    import numpy as np
    
    # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ë‚ ì§œë³„ë¡œ ì •ë¦¬
    predictions_dict = {pred['Date']: pred['Prediction'] for pred in predictions if pred['Date'] in business_days}
    
    # ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ì˜ì—…ì¼ ëª©ë¡
    sorted_days = sorted(business_days)
    
    # ë‹¤ìŒ ë°˜ì›” ì´ ì˜ì—…ì¼ ìˆ˜ ê³„ì‚°
    total_days = len(sorted_days)
    
    # ìµœì†Œ ìœˆë„ìš° í¬ê¸°ì™€ ìµœëŒ€ ìœˆë„ìš° í¬ê¸° ì„¤ì • (ìµœëŒ€ëŠ” ë°˜ì›” ì „ì²´ ì¼ìˆ˜)
    max_window_size = total_days
    
    # ê³ ë ¤í•  ëª¨ë“  ìœˆë„ìš° í¬ê¸° ë²”ìœ„ ìƒì„±
    window_sizes = range(min_window_size, max_window_size + 1)
    
    print(f"ë‹¤ìŒ ë°˜ì›” ì˜ì—…ì¼: {total_days}ì¼, ê³ ë ¤í•  ìœˆë„ìš° í¬ê¸°: {list(window_sizes)}")
    
    # ê° ìœˆë„ìš° í¬ê¸°ë³„ ê²°ê³¼ ì €ì¥
    interval_averages = {}
    
    # ëª¨ë“  êµ¬ê°„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_intervals = []
    
    # ê° ìœˆë„ìš° í¬ê¸°ì— ëŒ€í•´ ëª¨ë“  ê°€ëŠ¥í•œ êµ¬ê°„ ê³„ì‚°
    for window_size in window_sizes:
        window_results = []
        
        # ê°€ëŠ¥í•œ ëª¨ë“  ì‹œì‘ì ì— ëŒ€í•´ ìœˆë„ìš° í‰ê·  ê³„ì‚°
        for i in range(len(sorted_days) - window_size + 1):
            interval_days = sorted_days[i:i+window_size]
            
            # ëª¨ë“  ë‚ ì§œì— ì˜ˆì¸¡ ê°€ê²©ì´ ìˆëŠ”ì§€ í™•ì¸
            if all(day in predictions_dict for day in interval_days):
                avg_price = np.mean([predictions_dict[day] for day in interval_days])
                
                interval_info = {
                    'start_date': interval_days[0],
                    'end_date': interval_days[-1],
                    'days': window_size,
                    'avg_price': avg_price,
                    'dates': interval_days.copy()
                }
                
                window_results.append(interval_info)
                all_intervals.append(interval_info)  # ëª¨ë“  êµ¬ê°„ ëª©ë¡ì—ë„ ì¶”ê°€
        
        # í•´ë‹¹ ìœˆë„ìš° í¬ê¸°ì— ëŒ€í•œ ê²°ê³¼ ì €ì¥ (ì°¸ê³ ìš©)
        if window_results:
            # í‰ê·  ê°€ê²© ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            window_results.sort(key=lambda x: x['avg_price'])
            interval_averages[window_size] = window_results
    
    # êµ¬ê°„ ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
    interval_scores = {}
    
    # Global Rank ì „ëµ: ëª¨ë“  êµ¬ê°„ì„ í†µí•©í•˜ì—¬ ê°€ê²© ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    all_intervals.sort(key=lambda x: x['avg_price'])
    
    # ìƒìœ„ 3ê°œ êµ¬ê°„ì—ë§Œ ì ìˆ˜ ë¶€ì—¬ (ì „ì²´ ì¤‘ì—ì„œ)
    for i, interval in enumerate(all_intervals[:min(3, len(all_intervals))]):
        score = 3 - i  # 1ë“±: 3ì , 2ë“±: 2ì , 3ë“±: 1ì 
        
        # êµ¬ê°„ ì‹ë³„ì„ ìœ„í•œ í‚¤ ìƒì„± (ë¬¸ìì—´ í‚¤ë¡œ ë³€ê²½)
        interval_key = f"{format_date(interval['start_date'])}-{format_date(interval['end_date'])}"
        
        # ì ìˆ˜ ì •ë³´ ì €ì¥
        interval_scores[interval_key] = {
            'start_date': format_date(interval['start_date']),  # í˜•ì‹ ì ìš©
            'end_date': format_date(interval['end_date']),      # í˜•ì‹ ì ìš©
            'days': interval['days'],
            'avg_price': interval['avg_price'],
            'dates': [format_date(d) for d in interval['dates']],  # ë‚ ì§œ ëª©ë¡ë„ í˜•ì‹ ì ìš©
            'score': score,
            'rank': i + 1
        }
    
    # ë¶„ì„ ì •ë³´ ì¶”ê°€
    analysis_info = {
        'total_days': total_days,
        'window_sizes': list(window_sizes),
        'total_intervals': len(all_intervals),
        'min_avg_price': min([interval['avg_price'] for interval in all_intervals]) if all_intervals else None,
        'max_avg_price': max([interval['avg_price'] for interval in all_intervals]) if all_intervals else None
    }
    
    # ê²°ê³¼ ì¶œë ¥ (ì°¸ê³ ìš©)
    if interval_scores:
        top_interval = max(interval_scores.values(), key=lambda x: x['score'])
        print(f"\nìµœê³  ì ìˆ˜ êµ¬ê°„: {top_interval['days']}ì¼ êµ¬ê°„ ({format_date(top_interval['start_date'])} ~ {format_date(top_interval['end_date'])})")
        print(f"ì ìˆ˜: {top_interval['score']}, ìˆœìœ„: {top_interval['rank']}, í‰ê· ê°€: {top_interval['avg_price']:.2f}")
    
    return interval_averages, interval_scores, analysis_info

# 7. ë‘ êµ¬ë§¤ ë°©ë²•ì˜ ê²°ê³¼ ë¹„êµ í•¨ìˆ˜
def decide_purchase_interval(interval_scores):
    """
    ì ìˆ˜ê°€ ë¶€ì—¬ëœ êµ¬ê°„ë“¤ ì¤‘ì—ì„œ ìµœì¢… êµ¬ë§¤ êµ¬ê°„ì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜
    - ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ êµ¬ê°„ ì„ íƒ
    - ë™ì ì¸ ê²½ìš° í‰ê·  ê°€ê²©ì´ ë” ë‚®ì€ êµ¬ê°„ ì„ íƒ
    
    Parameters:
    -----------
    interval_scores : dict
        êµ¬ê°„ë³„ ì ìˆ˜ ì •ë³´
    
    Returns:
    -----------
    dict
        ìµœì¢… ì„ íƒëœ êµ¬ë§¤ êµ¬ê°„ ì •ë³´
    """
    if not interval_scores:
        return None
    
    # ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ êµ¬ê°„ ì„ íƒ
    max_score = max(interval['score'] for interval in interval_scores.values())
    
    # ìµœê³  ì ìˆ˜ë¥¼ ê°€ì§„ ëª¨ë“  êµ¬ê°„ ì°¾ê¸°
    top_intervals = [interval for interval in interval_scores.values() 
                    if interval['score'] == max_score]
    
    # ë™ì ì´ ìˆëŠ” ê²½ìš°, í‰ê·  ê°€ê²©ì´ ë” ë‚®ì€ êµ¬ê°„ ì„ íƒ
    if len(top_intervals) > 1:
        best_interval = min(top_intervals, key=lambda x: x['avg_price'])
        best_interval['selection_reason'] = "ìµœê³  ì ìˆ˜ ì¤‘ ìµœì € í‰ê· ê°€ êµ¬ê°„"
    else:
        best_interval = top_intervals[0]
        best_interval['selection_reason'] = "ìµœê³  ì ìˆ˜ êµ¬ê°„"
    
    return best_interval

#######################################################################
# íŠ¹ì„± ì„ íƒ í•¨ìˆ˜
#######################################################################

def calculate_group_vif(df, variables):
    """ê·¸ë£¹ ë‚´ ë³€ìˆ˜ë“¤ì˜ VIF ê³„ì‚°"""
    # ë³€ìˆ˜ê°€ í•œ ê°œ ì´í•˜ë©´ VIF ê³„ì‚° ë¶ˆê°€
    if len(variables) <= 1:
        return pd.DataFrame({
            "Feature": variables,
            "VIF": [1.0] * len(variables)
        })
    
    # ëª¨ë“  ë³€ìˆ˜ê°€ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    available_vars = [var for var in variables if var in df.columns]
    if len(available_vars) <= 1:
        return pd.DataFrame({
            "Feature": available_vars,
            "VIF": [1.0] * len(available_vars)
        })
    
    try:
        from statsmodels.stats.outliers_influence import variance_inflation_factor
        
        vif_data = pd.DataFrame()
        vif_data["Feature"] = available_vars
        vif_data["VIF"] = [variance_inflation_factor(df[available_vars].values, i) 
                          for i in range(len(available_vars))]
        return vif_data.sort_values('VIF', ascending=False)
    except Exception as e:
        logger.error(f"Error calculating VIF: {str(e)}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return pd.DataFrame({
            "Feature": available_vars,
            "VIF": [float('nan')] * len(available_vars)
        })

def analyze_group_correlations(df, variable_groups, target_col='MOPJ'):
    """ê·¸ë£¹ë³„ ìƒê´€ê´€ê³„ ë¶„ì„"""
    logger.info("Analyzing correlations for each group:")
    group_correlations = {}
    
    for group_name, variables in variable_groups.items():
        # ê° ê·¸ë£¹ì˜ ë³€ìˆ˜ë“¤ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ìƒê´€ê´€ê³„ ê³„ì‚°
        # í•´ë‹¹ ê·¸ë£¹ì˜ ë³€ìˆ˜ë“¤ì´ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        available_vars = [var for var in variables if var in df.columns]
        if not available_vars:
            logger.warning(f"Warning: No variables from {group_name} group found in dataframe")
            continue
            
        if target_col not in df.columns:
            logger.warning(f"Warning: Target column {target_col} not found in dataframe")
            continue
            
        correlations = df[available_vars].corrwith(df[target_col]).abs().sort_values(ascending=False)
        group_correlations[group_name] = correlations
        
        logger.info(f"\n{group_name} group correlations with {target_col}:")
        logger.info(str(correlations))
    
    return group_correlations

def select_features_from_groups(df, variable_groups, target_col='MOPJ', vif_threshold=50.0, corr_threshold=0.8):
    """ê° ê·¸ë£¹ì—ì„œ ëŒ€í‘œ ë³€ìˆ˜ ì„ íƒ"""
    selected_features = []
    selection_process = {}
    
    logger.info(f"\nCorrelation threshold: {corr_threshold}")
    
    for group_name, variables in variable_groups.items():
        logger.info(f"\nProcessing {group_name} group:")
        
        # í•´ë‹¹ ê·¸ë£¹ì˜ ë³€ìˆ˜ë“¤ì´ dfì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        available_vars = [var for var in variables if var in df.columns]
        if not available_vars:
            logger.warning(f"Warning: No variables from {group_name} group found in dataframe")
            continue
            
        if target_col not in df.columns:
            logger.warning(f"Warning: Target column {target_col} not found in dataframe")
            continue
        
        # ê·¸ë£¹ ë‚´ ìƒê´€ê´€ê³„ ê³„ì‚°
        correlations = df[available_vars].corrwith(df[target_col]).abs().sort_values(ascending=False)
        logger.info(f"\nCorrelations with {target_col}:")
        logger.info(str(correlations))
        
        # ìƒê´€ê´€ê³„ê°€ ì„ê³„ê°’ ì´ìƒì¸ ë³€ìˆ˜ë§Œ í•„í„°ë§
        high_corr_vars = correlations[correlations >= corr_threshold].index.tolist()
        
        if not high_corr_vars:
            logger.warning(f"Warning: No variables in {group_name} group meet the correlation threshold of {corr_threshold}")
            continue
        
        # ìƒê´€ê´€ê³„ ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ë³€ìˆ˜ë“¤ì— ëŒ€í•´ VIF ê³„ì‚°
        if len(high_corr_vars) > 1:
            vif_data = calculate_group_vif(df[high_corr_vars], high_corr_vars)
            logger.info(f"\nVIF values for {group_name} group (high correlation vars only):")
            logger.info(str(vif_data))
            
            # VIF ê¸°ì¤€ ì ìš©í•˜ì—¬ ë‹¤ì¤‘ê³µì„ ì„± ë‚®ì€ ë³€ìˆ˜ ì„ íƒ
            low_vif_vars = vif_data[vif_data['VIF'] < vif_threshold]['Feature'].tolist()
            
            if low_vif_vars:
                # ë‚®ì€ VIF ë³€ìˆ˜ë“¤ ì¤‘ ìƒê´€ê´€ê³„ê°€ ê°€ì¥ ë†’ì€ ë³€ìˆ˜ ì„ íƒ
                for var in correlations.index:
                    if var in low_vif_vars:
                        selected_var = var
                        break
                else:
                    selected_var = high_corr_vars[0]
            else:
                selected_var = high_corr_vars[0]
        else:
            selected_var = high_corr_vars[0]
            vif_data = pd.DataFrame({"Feature": [selected_var], "VIF": [1.0]})
        
        # ì„ íƒëœ ë³€ìˆ˜ê°€ ìƒê´€ê´€ê³„ ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸ (ì•ˆì „ì¥ì¹˜)
        if correlations[selected_var] >= corr_threshold:
            selected_features.append(selected_var)
            
            selection_process[group_name] = {
                'selected_variable': selected_var,
                'correlation': correlations[selected_var],
                'all_correlations': correlations.to_dict(),
                'vif_data': vif_data.to_dict() if not vif_data.empty else {},
                'high_corr_vars': high_corr_vars
            }
            
            logger.info(f"\nSelected variable from {group_name}: {selected_var} (corr: {correlations[selected_var]:.4f})")
        else:
            logger.info(f"\nNo variable selected from {group_name}: correlation threshold not met")
    
    # ìƒê´€ê´€ê³„ ê¸°ì¤€ ì¬í™•ì¸ (ìµœì¢… ì•ˆì „ì¥ì¹˜)
    final_features = []
    for feature in selected_features:
        corr = abs(df[feature].corr(df[target_col]))
        if corr >= corr_threshold:
            final_features.append(feature)
            logger.info(f"Final selection: {feature} (corr: {corr:.4f})")
        else:
            logger.info(f"Excluded: {feature} (corr: {corr:.4f}) - below threshold")
    
    # íƒ€ê²Ÿ ì»¬ëŸ¼ì´ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ì¶”ê°€
    if target_col not in final_features:
        final_features.append(target_col)
        logger.info(f"Added target column: {target_col}")
    
    # ìµœì†Œ íŠ¹ì„± ìˆ˜ í™•ì¸
    if len(final_features) < 3:
        logger.warning(f"Selected features ({len(final_features)}) < 3, lowering threshold to 0.5")
        return select_features_from_groups(df, variable_groups, target_col, vif_threshold, 0.5)
    
    return final_features, selection_process

def optimize_hyperparameters_semimonthly_kfold(train_data, input_size, target_col_idx, device, current_period, file_path=None, n_trials=30, k_folds=5, use_cache=True):
    """
    ì‹œê³„ì—´ K-fold êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•˜ì—¬ ë°˜ì›”ë³„ ë°ì´í„°ì— ëŒ€í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    """
    logger.info(f"\n===== {current_period} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (ì‹œê³„ì—´ {k_folds}-fold êµì°¨ ê²€ì¦) =====")
    
    # ìºì‹œ íŒŒì¼ ê²½ë¡œ - íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
    file_cache_dir = get_file_cache_dirs(file_path)['models']
    cache_file = os.path.join(file_cache_dir, f"hyperparams_kfold_{current_period.replace('-', '_')}.json")
    logger.info(f"ğŸ“ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìºì‹œ íŒŒì¼: {cache_file}")
    
    # models ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(file_cache_dir, exist_ok=True)
    
    # ìºì‹œ í™•ì¸
    if use_cache and os.path.exists(cache_file):
        try:
            with open(cache_file, 'r') as f:
                cached_params = json.load(f)
            logger.info(f"{current_period} ìºì‹œëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ")
            return cached_params
        except Exception as e:
            logger.error(f"ìºì‹œ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
    
    # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ (ìµœì í™” ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
    default_params = {
        'sequence_length': 20,
        'hidden_size': 193,
        'num_layers': 6,
        'dropout': 0.2548844201860255,
        'batch_size': 77,
        'learning_rate': 0.00012082451343807303,
        'num_epochs': 107,
        'loss_alpha': 0.506378255841371,  # MSE weight
        'loss_beta': 0.17775383895727725,  # Volatility weight
        'loss_gamma': 0.07133778859895412, # Directional weight
        'loss_delta': 0.07027938312247926, # Continuity weight
        'patience': 26,
        'warmup_steps': 382,
        'lr_factor': 0.49185859987249164,
        'lr_patience': 8,
        'min_lr': 1.1304817036887926e-07
    }
    
    # ë°ì´í„° ê¸¸ì´ í™•ì¸ - ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ë°”ë¡œ ê¸°ë³¸ê°’ ë°˜í™˜
    MIN_DATA_SIZE = 100
    if len(train_data) < MIN_DATA_SIZE:
        logger.warning(f"í›ˆë ¨ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({len(train_data)} ë°ì´í„° í¬ì¸íŠ¸ < {MIN_DATA_SIZE}). ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return default_params
    
    # K-fold ë¶„í•  ë¡œì§
    predict_window = 23  # ì˜ˆì¸¡ ìœˆë„ìš° í¬ê¸°
    min_fold_size = 20 + predict_window + 5  # ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ + ì˜ˆì¸¡ ìœˆë„ìš° + ì—¬ìœ 
    max_possible_folds = len(train_data) // min_fold_size
    
    if max_possible_folds < 2:
        logger.warning(f"ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ k-foldë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ê°€ëŠ¥í•œ fold: {max_possible_folds} < 2). ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return default_params
    
    # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ fold ìˆ˜ ì¡°ì •
    k_folds = min(k_folds, max_possible_folds)
    fold_size = len(train_data) // (k_folds + 1)  # +1ì€ ì˜ˆì¸¡ ìœˆë„ìš°ë¥¼ ìœ„í•œ ì¶”ê°€ ë¶€ë¶„

    logger.info(f"ë°ì´í„° í¬ê¸°: {len(train_data)}, Fold ìˆ˜: {k_folds}, ê° Fold í¬ê¸°: {fold_size}")

    # fold ë¶„í• ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
    folds = []
    for i in range(k_folds):
        test_start = i * fold_size
        test_end = (i + 1) * fold_size
        
        train_indices = list(range(0, test_start)) + list(range(test_end, len(train_data)))
        test_indices = list(range(test_start, test_end))
        
        folds.append((train_indices, test_indices))
    
    # Optuna ëª©ì  í•¨ìˆ˜ ì •ì˜
    def objective(trial):
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ìˆ˜ì • - ì‹œí€€ìŠ¤ ê¸¸ì´ ìµœëŒ€ê°’ ì œí•œ
        max_seq_length = min(fold_size - predict_window - 5, 60)
        
        # ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ë„ ì œí•œ
        min_seq_length = min(10, max_seq_length)
        
        if max_seq_length <= min_seq_length:
            logger.warning(f"ì‹œí€€ìŠ¤ ê¸¸ì´ ë²”ìœ„ê°€ ë„ˆë¬´ ì œí•œì ì…ë‹ˆë‹¤ (min={min_seq_length}, max={max_seq_length}). í•´ë‹¹ trial ê±´ë„ˆë›°ê¸°.")
            return float('inf')
        
        params = {
            'sequence_length': trial.suggest_int('sequence_length', min_seq_length, max_seq_length),
            'hidden_size': trial.suggest_int('hidden_size', 32, 256),
            'num_layers': trial.suggest_int('num_layers', 2, 6),
            'dropout': trial.suggest_float('dropout', 0.1, 0.5),
            'batch_size': trial.suggest_int('batch_size', 16, min(128, len(train_data))),
            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
            'num_epochs': trial.suggest_int('num_epochs', 50, 200),
            'patience': trial.suggest_int('patience', 10, 30),
            'warmup_steps': trial.suggest_int('warmup_steps', 100, 1000),
            'lr_factor': trial.suggest_float('lr_factor', 0.1, 0.5),
            'lr_patience': trial.suggest_int('lr_patience', 3, 10),
            'min_lr': trial.suggest_float('min_lr', 1e-7, 1e-5, log=True),
            'loss_alpha': trial.suggest_float('loss_alpha', 0.5, 0.9),
            'loss_beta': trial.suggest_float('loss_beta', 0.1, 0.3),
            'loss_gamma': trial.suggest_float('loss_gamma', 0.05, 0.2),
            'loss_delta': trial.suggest_float('loss_delta', 0.01, 0.1)
        }
        
        # K-fold êµì°¨ ê²€ì¦
        fold_losses = []
        valid_fold_count = 0
        
        for fold_idx, (train_indices, test_indices) in enumerate(folds):
            try:
                # ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ fold í¬ê¸°ë³´ë‹¤ í¬ë©´ ê±´ë„ˆë›°ê¸°
                if params['sequence_length'] >= len(test_indices):
                    logger.warning(f"Fold {fold_idx+1}: ì‹œí€€ìŠ¤ ê¸¸ì´({params['sequence_length']})ê°€ í…ŒìŠ¤íŠ¸ ë°ì´í„°({len(test_indices)})ë³´ë‹¤ í½ë‹ˆë‹¤.")
                    continue
                
                # foldë³„ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
                fold_train_data = train_data[train_indices]
                fold_test_data = train_data[test_indices]
                
                # ë°ì´í„° ì¤€ë¹„
                X_train, y_train, prev_train, X_val, y_val, prev_val = prepare_data(
                    fold_train_data, fold_test_data, params['sequence_length'],
                    predict_window, target_col_idx, augment=False
                )
                
                # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
                if len(X_train) < params['batch_size'] or len(X_val) < 1:
                    logger.warning(f"Fold {fold_idx+1}: ë°ì´í„° ë¶ˆì¶©ë¶„ (í›ˆë ¨: {len(X_train)}, ê²€ì¦: {len(X_val)})")
                    continue
                
                # ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±
                train_dataset = TimeSeriesDataset(X_train, y_train, device, prev_train)
                batch_size = min(params['batch_size'], len(X_train))
                train_loader = DataLoader(
                    train_dataset,
                    batch_size=batch_size,
                    shuffle=True,
                    worker_init_fn=seed_worker,
                    generator=g
                )
                
                val_dataset = TimeSeriesDataset(X_val, y_val, device, prev_val)
                val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)
                
                # ëª¨ë¸ ìƒì„±
                model = ImprovedLSTMPredictor(
                    input_size=input_size,
                    hidden_size=params['hidden_size'],
                    num_layers=params['num_layers'],
                    dropout=params['dropout'],
                    output_size=predict_window
                ).to(device)

                optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])

                # best_val_loss ë³€ìˆ˜ ëª…ì‹œì  ì •ì˜
                best_val_loss = float('inf')
                patience_counter = 0
                
                for epoch in range(params['num_epochs']):
                    # í•™ìŠµ
                    model.train()
                    train_loss = 0
                    for X_batch, y_batch, prev_batch in train_loader:
                        optimizer.zero_grad()
                        y_pred = model(X_batch, prev_batch)
                        loss, _ = composite_loss(y_pred, y_batch, prev_batch,
                                            alpha=params['loss_alpha'],
                                            beta=params['loss_beta'],
                                            gamma=params['loss_gamma'],
                                            delta=params['loss_delta'])
                        loss.backward()
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        optimizer.step()
                        train_loss += loss.item()
                    
                    # ê²€ì¦
                    model.eval()
                    val_loss = 0
                    
                    with torch.no_grad():
                        for X_batch, y_batch, prev_batch in val_loader:
                            y_pred = model(X_batch, prev_batch)
                            loss, _ = composite_loss(y_pred, y_batch, prev_batch,
                                                alpha=params['loss_alpha'],
                                                beta=params['loss_beta'],
                                                gamma=params['loss_gamma'],
                                                delta=params['loss_delta'])
                            val_loss += loss.item()
                        
                        val_loss /= len(val_loader)
                        
                        if val_loss < best_val_loss:
                            best_val_loss = val_loss
                            patience_counter = 0
                        else:
                            patience_counter += 1
                        
                        if patience_counter >= params['patience']:
                            break
                
                valid_fold_count += 1
                fold_losses.append(best_val_loss)
                
            except Exception as e:
                logger.error(f"Error in fold {fold_idx+1}: {str(e)}")
                continue
        
        # ëª¨ë“  foldê°€ ì‹¤íŒ¨í•œ ê²½ìš° ë§¤ìš° í° ì†ì‹¤ê°’ ë°˜í™˜
        if not fold_losses:
            logger.warning("ëª¨ë“  foldê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì´ íŒŒë¼ë¯¸í„° ì¡°í•©ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
            return float('inf')
        
        # ì„±ê³µí•œ foldì˜ í‰ê·  ì†ì‹¤ê°’ ë°˜í™˜
        return sum(fold_losses) / len(fold_losses)
    
    # Optuna ìµœì í™” ì‹œë„
    try:
        import optuna
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=42)
        )
        
        study.optimize(objective, n_trials=n_trials)
        
        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
        if study.best_trial.value == float('inf'):
            logger.warning(f"ëª¨ë“  trialì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return default_params
            
        best_params = study.best_params
        logger.info(f"\n{current_period} ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° (K-fold):")
        for key, value in best_params.items():
            logger.info(f"  {key}: {value}")
        
        # ëª¨ë“  í•„ìˆ˜ í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
        required_keys = ['sequence_length', 'hidden_size', 'num_layers', 'dropout', 
                        'batch_size', 'learning_rate', 'num_epochs', 'patience',
                        'warmup_steps', 'lr_factor', 'lr_patience', 'min_lr',
                        'loss_alpha', 'loss_beta', 'loss_gamma', 'loss_delta']
        
        for key in required_keys:
            if key not in best_params:
                # ëˆ„ë½ëœ í‚¤ê°€ ìˆìœ¼ë©´ ê¸°ë³¸ê°’ í• ë‹¹
                if key == 'warmup_steps':
                    best_params[key] = 382
                elif key == 'lr_factor':
                    best_params[key] = 0.49
                elif key == 'lr_patience':
                    best_params[key] = 8
                elif key == 'min_lr':
                    best_params[key] = 1e-7
                elif key == 'loss_gamma':
                    best_params[key] = 0.07
                elif key == 'loss_delta':
                    best_params[key] = 0.07
                else:
                    best_params[key] = default_params[key]
        
        # ìºì‹œì— ì €ì¥
        with open(cache_file, 'w') as f:
            json.dump(best_params, f, indent=2)
        
        logger.info(f"í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ {cache_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return best_params
        
    except Exception as e:
        logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°˜í™˜
        return default_params

#######################################################################
# ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥/ë¡œë“œ í•¨ìˆ˜ë“¤
#######################################################################

def save_prediction_simple(prediction_results: dict, prediction_date):
    """ë¦¬ìŠ¤íŠ¸Â·ë”•íŠ¸ ì–´ë–¤ êµ¬ì¡°ë“  ì €ì¥ ê°€ëŠ¥í•œ ì•ˆì „ ë²„ì „ - íŒŒì¼ëª… í†µì¼"""
    try:
        preds_root = prediction_results.get("predictions")

        # â”€â”€ ì²« ì˜ˆì¸¡ ë ˆì½”ë“œ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if isinstance(preds_root, dict) and preds_root:
            preds_seq = preds_root.get("future") or []
        else:                                   # list í˜¹ì€ None
            preds_seq = preds_root or prediction_results.get("predictions_flat", [])

        if not preds_seq:
            raise ValueError("prediction_results ì•ˆì— ì˜ˆì¸¡ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        first_rec = preds_seq[0]
        first_date = pd.to_datetime(first_rec.get("date") or first_rec.get("Date"))
        if pd.isna(first_date):
            raise ValueError("ì²« ì˜ˆì¸¡ ë ˆì½”ë“œì— ë‚ ì§œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ğŸ¯ íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        cache_dirs = get_file_cache_dirs()
        file_predictions_dir = cache_dirs['predictions']
        
        # âœ… íŒŒì¼ ê²½ë¡œ ì„¤ì • (íŒŒì¼ë³„ ë””ë ‰í† ë¦¬ ë‚´)
        json_path = file_predictions_dir / f"prediction_start_{first_date:%Y%m%d}.json"
        csv_path = file_predictions_dir / f"prediction_start_{first_date:%Y%m%d}.csv"
        meta_path = file_predictions_dir / f"prediction_start_{first_date:%Y%m%d}_meta.json"
        attention_path = file_predictions_dir / f"prediction_start_{first_date:%Y%m%d}_attention.json"
        
        logger.info(f"ğŸ“ Using file cache directory: {cache_dirs['root'].name}")
        logger.info(f"  ğŸ“„ Predictions: {file_predictions_dir.name}")
        logger.info(f"  ğŸ“„ CSV: {csv_path.name}")
        logger.info(f"  ğŸ“„ Meta: {meta_path.name}")

        # â”€â”€ validation ê°œìˆ˜ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if isinstance(preds_root, dict):
            validation_cnt = len(preds_root.get("validation", []))
        else:
            validation_cnt = 0

        # â”€â”€ ë©”íƒ€ + ë³¸ë¬¸ êµ¬ì„± (íŒŒì¼ ìºì‹œ ì •ë³´ í¬í•¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        current_file_path = prediction_state.get('current_file', None)
        file_content_hash = get_data_content_hash(current_file_path) if current_file_path else None
        
        meta = {
            "prediction_start_date": first_date.strftime("%Y-%m-%d"),
            "data_end_date": str(prediction_date)[:10],
            "created_at": datetime.now().isoformat(),
            "semimonthly_period": prediction_results.get("semimonthly_period"),
            "next_semimonthly_period": prediction_results.get("next_semimonthly_period"),
            "selected_features": prediction_results.get("selected_features", []),
            "total_predictions": len(prediction_results.get("predictions_flat", preds_seq)),
            "validation_points": validation_cnt,
            "is_pure_future_prediction": prediction_results.get("summary", {}).get(
                "is_pure_future_prediction", validation_cnt == 0
            ),
            "metrics": prediction_results.get("metrics"),
            "interval_scores": prediction_results.get("interval_scores", {}),
            # ğŸ”‘ ìºì‹œ ì—°ë™ì„ ìœ„í•œ íŒŒì¼ ì •ë³´
            "file_path": current_file_path,
            "file_content_hash": file_content_hash
        }

        # âœ… CSV íŒŒì¼ ì €ì¥
        predictions_data = clean_predictions_data(
            prediction_results.get("predictions_flat", preds_seq)
        )
        
        if predictions_data:
            pred_df = pd.DataFrame(predictions_data)
            pred_df.to_csv(csv_path, index=False)
            logger.info(f"âœ… CSV saved: {csv_path}")

        # âœ… ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(meta_path, "w", encoding="utf-8") as fp:
            json.dump(meta, fp, ensure_ascii=False, indent=2)
        logger.info(f"âœ… Metadata saved: {meta_path}")

        # âœ… Attention ë°ì´í„° ì €ì¥ (ìˆëŠ” ê²½ìš°)
        attention_data = prediction_results.get("attention_data")
        if attention_data:
            attention_save_data = {
                "image_base64": attention_data.get("image", ""),
                "feature_importance": attention_data.get("feature_importance", {}),
                "temporal_importance": attention_data.get("temporal_importance", {})
            }
            
            with open(attention_path, "w", encoding="utf-8") as fp:
                json.dump(attention_save_data, fp, ensure_ascii=False, indent=2)
            logger.info(f"âœ… Attention saved: {attention_path}")

        # âœ… ì´ë™í‰ê·  ë°ì´í„° ì €ì¥ (ìˆëŠ” ê²½ìš°)
        ma_results = prediction_results.get("ma_results")
        ma_file = None
        if ma_results:
            ma_path = file_predictions_dir / f"prediction_start_{first_date:%Y%m%d}_ma.json"
            try:
                with open(ma_path, "w", encoding="utf-8") as fp:
                    json.dump(ma_results, fp, ensure_ascii=False, indent=2, default=str)
                logger.info(f"âœ… MA results saved: {ma_path}")
                ma_file = str(ma_path)
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to save MA results: {str(e)}")

        # âœ… ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        update_predictions_index_simple(meta)
        
        logger.info(f"âœ… Complete prediction save â†’ start date: {meta['prediction_start_date']}")
        return {
            "success": True, 
            "csv_file": str(csv_path),
            "meta_file": str(meta_path),
            "attention_file": str(attention_path) if attention_data else None,
            "ma_file": ma_file,
            "prediction_start_date": meta["prediction_start_date"]
        }

    except Exception as e:
        logger.error(f"âŒ save_prediction_simple ì˜¤ë¥˜: {e}")
        logger.error(traceback.format_exc())
        return {"success": False, "error": str(e)}

# 2. Attention ë°ì´í„°ë¥¼ í¬í•¨í•œ ë¡œë“œ í•¨ìˆ˜
def load_prediction_simple(prediction_start_date):
    """
    ë‹¨ìˆœí™”ëœ ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ í•¨ìˆ˜
    """
    try:
        predictions_dir = Path(PREDICTIONS_DIR)
        
        if isinstance(prediction_start_date, str):
            start_date = pd.to_datetime(prediction_start_date)
        else:
            start_date = prediction_start_date
        
        date_str = start_date.strftime('%Y%m%d')
        
        csv_filepath = predictions_dir / f"prediction_{date_str}.csv"
        meta_filepath = predictions_dir / f"prediction_{date_str}_meta.json"
        
        if not csv_filepath.exists() or not meta_filepath.exists():
            return {'success': False, 'error': f'Prediction files not found for {start_date.strftime("%Y-%m-%d")}'}
        
        # CSV ë¡œë“œ
        predictions_df = pd.read_csv(csv_filepath)
        predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])
        if 'Prediction_From' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['Prediction_From'])
        
        predictions = predictions_df.to_dict('records')
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(meta_filepath, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        logger.info(f"Simple prediction load completed: {len(predictions)} predictions")
        
        return {
            'success': True,
            'predictions': predictions,
            'metadata': metadata,
            'attention_data': {
                'image': metadata.get('attention_map', {}).get('image_base64', ''),
                'feature_importance': metadata.get('attention_map', {}).get('feature_importance', {}),
                'temporal_importance': metadata.get('attention_map', {}).get('temporal_importance', {})
            }
        }
        
    except Exception as e:
        logger.error(f"Error loading prediction: {str(e)}")
        return {'success': False, 'error': str(e)}

def update_predictions_index_simple(metadata):
    """ë‹¨ìˆœí™”ëœ ì˜ˆì¸¡ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ - íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©"""
    try:
        # ğŸ”§ metadataê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
        if metadata is None:
            logger.warning("âš ï¸ [INDEX] metadataê°€ Noneì…ë‹ˆë‹¤. ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
            
        # ğŸ¯ íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        cache_dirs = get_file_cache_dirs()
        predictions_index_file = cache_dirs['predictions'] / 'predictions_index.csv'
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ì½ê¸°
        index_data = []
        if predictions_index_file.exists():
            with open(predictions_index_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                index_data = list(reader)
        
        # ì¤‘ë³µ ì œê±°
        prediction_start_date = metadata.get('prediction_start_date')
        if not prediction_start_date:
            logger.warning("âš ï¸ [INDEX] metadataì— prediction_start_dateê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        index_data = [row for row in index_data 
                     if row.get('prediction_start_date') != prediction_start_date]
        
        # metricsê°€ Noneì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        metrics = metadata.get('metrics') or {}
        
        # ìƒˆ ë°ì´í„° ì¶”ê°€ (ğŸ”§ í•„ë“œëª… ìˆ˜ì •)
        new_row = {
            'prediction_start_date': metadata.get('prediction_start_date', ''),
            'data_end_date': metadata.get('data_end_date', ''),
            'created_at': metadata.get('created_at', ''),
            'semimonthly_period': metadata.get('semimonthly_period', ''),
            'next_semimonthly_period': metadata.get('next_semimonthly_period', ''),
            'prediction_count': metadata.get('total_predictions', metadata.get('prediction_count', 0)),  # ğŸ”§ ìˆ˜ì •
            'f1_score': metrics.get('f1', 0) if isinstance(metrics, dict) else 0,
            'accuracy': metrics.get('accuracy', 0) if isinstance(metrics, dict) else 0,
            'mape': metrics.get('mape', 0) if isinstance(metrics, dict) else 0,
            'weighted_score': metrics.get('weighted_score', 0) if isinstance(metrics, dict) else 0
        }
        index_data.append(new_row)
        
        # ë‚ ì§œìˆœ ì •ë ¬ í›„ ì €ì¥
        index_data.sort(key=lambda x: x.get('data_end_date', ''), reverse=True)
        
        if index_data:
            fieldnames = new_row.keys()  # ğŸ”§ ì¼ê´€ëœ í•„ë“œëª… ì‚¬ìš©
            with open(predictions_index_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(index_data)
            
            logger.info(f"âœ… Predictions index updated successfully: {len(index_data)} entries")
            logger.info(f"ğŸ“„ Index file: {predictions_index_file}")
            return True
        else:
            logger.warning("âš ï¸ No data to write to index file")
            return False
        
    except Exception as e:
        logger.error(f"âŒ Error updating simple predictions index: {str(e)}")
        logger.error(traceback.format_exc())
        return False

def rebuild_predictions_index_from_existing_files():
    """
    ê¸°ì¡´ ì˜ˆì¸¡ íŒŒì¼ë“¤ë¡œë¶€í„° predictions_index.csvë¥¼ ì¬ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    ğŸ”§ ëˆ„ì  ì˜ˆì¸¡ì´ ê¸°ì¡´ ë‹¨ì¼ ì˜ˆì¸¡ ìºì‹œë¥¼ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í•¨
    """
    try:
        current_file = prediction_state.get('current_file')
        if not current_file:
            logger.warning("âš ï¸ No current file set, cannot rebuild index")
            return False
        
        cache_dirs = get_file_cache_dirs(current_file)
        predictions_dir = cache_dirs['predictions']
        predictions_index_file = predictions_dir / 'predictions_index.csv'
        
        logger.info(f"ğŸ”„ Rebuilding predictions index from existing files in: {predictions_dir}")
        
        # ê¸°ì¡´ ë©”íƒ€ íŒŒì¼ë“¤ ì°¾ê¸°
        meta_files = list(predictions_dir.glob("*_meta.json"))
        logger.info(f"ğŸ“‹ Found {len(meta_files)} meta files")
        
        if not meta_files:
            logger.warning("âš ï¸ No meta files found to rebuild index")
            return False
        
        index_data = []
        
        for meta_file in meta_files:
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                # ì¸ë±ìŠ¤ ë ˆì½”ë“œ ìƒì„± (ë™ì¼í•œ í•„ë“œëª… ì‚¬ìš©)
                new_row = {
                    'prediction_start_date': metadata.get('prediction_start_date', ''),
                    'data_end_date': metadata.get('data_end_date', ''),
                    'created_at': metadata.get('created_at', ''),
                    'semimonthly_period': metadata.get('semimonthly_period', ''),
                    'next_semimonthly_period': metadata.get('next_semimonthly_period', ''),
                    'prediction_count': metadata.get('total_predictions', metadata.get('prediction_count', 0)),
                    'f1_score': metadata.get('metrics', {}).get('f1', 0),
                    'accuracy': metadata.get('metrics', {}).get('accuracy', 0),
                    'mape': metadata.get('metrics', {}).get('mape', 0),
                    'weighted_score': metadata.get('metrics', {}).get('weighted_score', 0)
                }
                
                index_data.append(new_row)
                logger.info(f"  âœ… {meta_file.name}: {new_row['prediction_start_date']}")
                
            except Exception as e:
                logger.warning(f"  âš ï¸  Error reading {meta_file.name}: {str(e)}")
                continue
        
        if not index_data:
            logger.error("âŒ No valid metadata found")
            return False
        
        # ë‚ ì§œìˆœ ì •ë ¬
        index_data.sort(key=lambda x: x.get('data_end_date', ''), reverse=True)
        
        # CSV íŒŒì¼ ìƒì„±
        fieldnames = index_data[0].keys()
        
        with open(predictions_index_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(index_data)
        
        logger.info(f"âœ… Successfully rebuilt predictions_index.csv with {len(index_data)} entries")
        logger.info(f"ğŸ“„ Index file: {predictions_index_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Error rebuilding predictions index: {str(e)}")
        logger.error(traceback.format_exc())
        return False

def load_prediction_from_csv(prediction_start_date_or_data_end_date):
    """
    í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ - ìë™ìœ¼ë¡œ ìƒˆë¡œìš´ í•¨ìˆ˜ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    """
    logger.info("Using compatibility wrapper - redirecting to new smart cache function")
    return load_prediction_with_attention_from_csv(prediction_start_date_or_data_end_date)

def load_prediction_with_attention_from_csv_in_dir(prediction_start_date, file_predictions_dir):
    """
    íŒŒì¼ë³„ ë””ë ‰í† ë¦¬ì—ì„œ ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ì™€ attention ë°ì´í„°ë¥¼ í•¨ê»˜ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    """
    try:
        if isinstance(prediction_start_date, str):
            start_date = pd.to_datetime(prediction_start_date)
        else:
            start_date = prediction_start_date
        
        date_str = start_date.strftime('%Y%m%d')
        
        # íŒŒì¼ë³„ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        csv_filepath = file_predictions_dir / f"prediction_start_{date_str}.csv"
        meta_filepath = file_predictions_dir / f"prediction_start_{date_str}_meta.json"
        attention_filepath = file_predictions_dir / f"prediction_start_{date_str}_attention.json"
        ma_filepath = file_predictions_dir / f"prediction_start_{date_str}_ma.json"
        
        logger.info(f"ğŸ“‚ Loading from file directory: {file_predictions_dir.name}")
        logger.info(f"  ğŸ“„ CSV: {csv_filepath.name}")
        
        if not csv_filepath.exists() or not meta_filepath.exists():
            logger.warning(f"  âŒ Required files missing in {file_predictions_dir.name}")
            return {'success': False, 'error': f'Prediction files not found for {start_date.strftime("%Y-%m-%d")}'}
        
        # CSV ë¡œë“œ
        predictions_df = pd.read_csv(csv_filepath)
        
        # ğŸ”§ ì»¬ëŸ¼ëª… í˜¸í™˜ì„± ì²˜ë¦¬: ì†Œë¬¸ìë¡œ ì €ì¥ëœ ì»¬ëŸ¼ì„ ëŒ€ë¬¸ìë¡œ ë³€í™˜
        if 'date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['date'])
        elif 'Date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])
        
        if 'prediction' in predictions_df.columns:
            predictions_df['Prediction'] = predictions_df['prediction']
        
        if 'prediction_from' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['prediction_from'])
        elif 'Prediction_From' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['Prediction_From'])
        
        predictions = predictions_df.to_dict('records')
        
        # âœ… ìºì‹œì—ì„œ ë¡œë“œí•  ë•Œ ì‹¤ì œê°’ ë‹¤ì‹œ ì„¤ì • (í˜„ì¬ íŒŒì¼ ë°ì´í„° ì‚¬ìš©)
        try:
            current_file = prediction_state.get('current_file')
            if current_file:
                df = load_data(current_file)
                if df is not None and not df.empty:
                    last_data_date = df.index.max()
                    updated_count = 0
                    
                    # ê° ì˜ˆì¸¡ì— ëŒ€í•´ ì‹¤ì œê°’ í™•ì¸ ë° ì„¤ì •
                    for pred in predictions:
                        pred_date = pd.to_datetime(pred['Date'])
                        
                        # ì‹¤ì œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œë©´ ì‹¤ì œê°’ ì„¤ì •
                        if (pred_date in df.index and 
                            pd.notna(df.loc[pred_date, 'MOPJ']) and 
                            pred_date <= last_data_date):
                            actual_val = float(df.loc[pred_date, 'MOPJ'])
                            pred['Actual'] = actual_val
                            updated_count += 1
                            logger.debug(f"  ğŸ“Š Set actual value for {pred_date.strftime('%Y-%m-%d')}: {actual_val:.2f}")
                        elif 'Actual' not in pred or pred['Actual'] is None:
                            pred['Actual'] = None
                    
                    if updated_count > 0:
                        logger.info(f"  ğŸ”„ Updated {updated_count} actual values from current data file")
                else:
                    logger.warning(f"  âš ï¸  Could not load current data file for actual values")
            else:
                logger.warning(f"  âš ï¸  No current file set for actual value update")
        except Exception as e:
            logger.warning(f"  âš ï¸  Error updating actual values: {str(e)}")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(meta_filepath, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Attention ë°ì´í„° ë¡œë“œ
        attention_data = {}
        if attention_filepath.exists():
            try:
                with open(attention_filepath, 'r', encoding='utf-8') as f:
                    attention_raw = json.load(f)
                attention_data = {
                    'image': attention_raw.get('image_base64', ''),
                    'feature_importance': attention_raw.get('feature_importance', {}),
                    'temporal_importance': attention_raw.get('temporal_importance', {})
                }
                logger.info(f"  ğŸ§  Attention data loaded successfully")
            except Exception as e:
                logger.warning(f"  âš ï¸  Failed to load attention data: {str(e)}")
        
        # ì´ë™í‰ê·  ë°ì´í„° ë¡œë“œ
        ma_results = {}
        if ma_filepath.exists():
            try:
                with open(ma_filepath, 'r', encoding='utf-8') as f:
                    ma_results = json.load(f)
                logger.info(f"  ğŸ“Š MA results loaded successfully")
            except Exception as e:
                logger.warning(f"  âš ï¸  Failed to load MA results: {str(e)}")
        
        logger.info(f"âœ… File directory cache load completed: {len(predictions)} predictions")
        
        return {
            'success': True,
            'predictions': predictions,
            'metadata': metadata,
            'attention_data': attention_data,
            'ma_results': ma_results
        }
        
    except Exception as e:
        logger.error(f"âŒ Error loading prediction from file directory: {str(e)}")
        return {'success': False, 'error': str(e)}

def load_prediction_with_attention_from_csv(prediction_start_date):
    """
    ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ì™€ attention ë°ì´í„°ë¥¼ í•¨ê»˜ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ - íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš©
    """
    try:
        # ğŸ¯ íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        current_file = prediction_state.get('current_file')
        if not current_file:
            logger.error("âŒ No current file set in prediction_state")
            return {'success': False, 'error': 'No current file context available'}
            
        cache_dirs = get_file_cache_dirs(current_file)
        predictions_dir = cache_dirs['predictions']
        
        if isinstance(prediction_start_date, str):
            start_date = pd.to_datetime(prediction_start_date)
        else:
            start_date = prediction_start_date
        
        date_str = start_date.strftime('%Y%m%d')
        
        # íŒŒì¼ ê²½ë¡œë“¤
        csv_filepath = predictions_dir / f"prediction_start_{date_str}.csv"
        meta_filepath = predictions_dir / f"prediction_start_{date_str}_meta.json"
        attention_filepath = predictions_dir / f"prediction_start_{date_str}_attention.json"
        
        # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not csv_filepath.exists() or not meta_filepath.exists():
            return {
                'success': False,
                'error': f'Prediction files not found for start date {start_date.strftime("%Y-%m-%d")}'
            }
        
        # CSV íŒŒì¼ ì½ê¸°
        predictions_df = pd.read_csv(csv_filepath)
        
        # ğŸ”§ ì»¬ëŸ¼ëª… í˜¸í™˜ì„± ì²˜ë¦¬: ì†Œë¬¸ìë¡œ ì €ì¥ëœ ì»¬ëŸ¼ì„ ëŒ€ë¬¸ìë¡œ ë³€í™˜
        if 'date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['date'])
        elif 'Date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])
        
        if 'prediction' in predictions_df.columns:
            predictions_df['Prediction'] = predictions_df['prediction']
        
        if 'prediction_from' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['prediction_from'])
        elif 'Prediction_From' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['Prediction_From'])
        
        predictions = predictions_df.to_dict('records')
        
        # âœ… ìºì‹œì—ì„œ ë¡œë“œí•  ë•Œ ì‹¤ì œê°’ ë‹¤ì‹œ ì„¤ì • (í˜„ì¬ íŒŒì¼ ë°ì´í„° ì‚¬ìš©)
        try:
            if current_file:
                df = load_data(current_file)
                if df is not None and not df.empty:
                    last_data_date = df.index.max()
                    updated_count = 0
                    
                    # ê° ì˜ˆì¸¡ì— ëŒ€í•´ ì‹¤ì œê°’ í™•ì¸ ë° ì„¤ì •
                    for pred in predictions:
                        pred_date = pd.to_datetime(pred['Date'])
                        
                        # ì‹¤ì œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œë©´ ì‹¤ì œê°’ ì„¤ì •
                        if (pred_date in df.index and 
                            pd.notna(df.loc[pred_date, 'MOPJ']) and 
                            pred_date <= last_data_date):
                            actual_val = float(df.loc[pred_date, 'MOPJ'])
                            pred['Actual'] = actual_val
                            updated_count += 1
                            logger.debug(f"  ğŸ“Š Set actual value for {pred_date.strftime('%Y-%m-%d')}: {actual_val:.2f}")
                        elif 'Actual' not in pred or pred['Actual'] is None:
                            pred['Actual'] = None
                    
                    if updated_count > 0:
                        logger.info(f"  ğŸ”„ Updated {updated_count} actual values from current data file")
                else:
                    logger.warning(f"  âš ï¸  Could not load current data file for actual values")
            else:
                logger.warning(f"  âš ï¸  No current file set for actual value update")
        except Exception as e:
            logger.warning(f"  âš ï¸  Error updating actual values: {str(e)}")
        
        # ë©”íƒ€ë°ì´í„° ì½ê¸°
        with open(meta_filepath, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Attention ë°ì´í„° ì½ê¸° (ìˆëŠ” ê²½ìš°)
        attention_data = None
        if attention_filepath.exists():
            try:
                with open(attention_filepath, 'r', encoding='utf-8') as f:
                    stored_attention = json.load(f)
                
                attention_data = {
                    'image': stored_attention.get('image_base64', ''),
                    'file_path': None,  # ì´ë¯¸ì§€ëŠ” base64ë¡œ ì €ì¥ë¨
                    'feature_importance': stored_attention.get('feature_importance', {}),
                    'temporal_importance': stored_attention.get('temporal_importance', {})
                }
                logger.info(f"Attention data loaded from: {attention_filepath}")
            except Exception as e:
                logger.warning(f"Failed to load attention data: {str(e)}")
                attention_data = None

        # ğŸ”„ ì´ë™í‰ê·  ë°ì´í„° ì½ê¸° (ìˆëŠ” ê²½ìš°)
        ma_filepath = predictions_dir / f"prediction_start_{date_str}_ma.json"
        ma_results = None
        if ma_filepath.exists():
            try:
                with open(ma_filepath, 'r', encoding='utf-8') as f:
                    ma_results = json.load(f)
                logger.info(f"MA results loaded from: {ma_filepath} ({len(ma_results)} windows)")
            except Exception as e:
                logger.warning(f"Failed to load MA results: {str(e)}")
                ma_results = None
        
        logger.info(f"Complete prediction data loaded: {csv_filepath} ({len(predictions)} predictions)")
        
        return {
            'success': True,
            'predictions': predictions,
            'metadata': metadata,
            'attention_data': attention_data,
            'ma_results': ma_results,  # ğŸ”‘ ì´ë™í‰ê·  ë°ì´í„° ì¶”ê°€
            'prediction_start_date': start_date.strftime('%Y-%m-%d'),
            'data_end_date': metadata.get('data_end_date'),
            'semimonthly_period': metadata['semimonthly_period'],
            'next_semimonthly_period': metadata['next_semimonthly_period'],
            'metrics': metadata['metrics'],
            'interval_scores': metadata['interval_scores'],
            'selected_features': metadata['selected_features'],
            'has_cached_attention': attention_data is not None,
            'has_cached_ma': ma_results is not None  # ğŸ”‘ MA ìºì‹œ ì—¬ë¶€ ì¶”ê°€
        }
        
    except Exception as e:
        logger.error(f"Error loading prediction with attention: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'success': False,
            'error': str(e)
        }

def get_saved_predictions_list_for_file(file_path, limit=100):
    """
    íŠ¹ì • íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ëª©ë¡ì„ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    file_path : str
        í˜„ì¬ íŒŒì¼ ê²½ë¡œ
    limit : int
        ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
    
    Returns:
    --------
    list : ì €ì¥ëœ ì˜ˆì¸¡ ëª©ë¡
    """
    try:
        predictions_list = []
        
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ êµ¬ì„±
        cache_dirs = get_file_cache_dirs(file_path)
        predictions_dir = Path(cache_dirs['predictions'])
        predictions_index_file = predictions_dir / 'predictions_index.csv'
        
        logger.info(f"ğŸ” [CACHE] Searching predictions in: {predictions_dir}")
        
        if predictions_index_file.exists():
            with open(predictions_index_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if len(predictions_list) >= limit:
                        break
                    
                    prediction_start_date = row.get('prediction_start_date', row.get('first_prediction_date'))
                    data_end_date = row.get('data_end_date', row.get('prediction_base_date', row.get('prediction_date')))
                    
                    if prediction_start_date and data_end_date:
                        pred_info = {
                            'prediction_start_date': prediction_start_date,
                            'data_end_date': data_end_date,
                            'prediction_date': data_end_date,
                            'first_prediction_date': prediction_start_date,
                            'created_at': row.get('created_at'),
                            'semimonthly_period': row.get('semimonthly_period'),
                            'next_semimonthly_period': row.get('next_semimonthly_period'),
                            'prediction_count': row.get('prediction_count'),
                            'actual_business_days': row.get('actual_business_days'),
                            'csv_file': row.get('csv_file'),
                            'meta_file': row.get('meta_file'),
                            'f1_score': float(row.get('f1_score', 0)),
                            'accuracy': float(row.get('accuracy', 0)),
                            'mape': float(row.get('mape', 0)),
                            'weighted_score': float(row.get('weighted_score', 0)),
                            'naming_scheme': row.get('naming_scheme', 'file_based'),
                            'source_file': os.path.basename(file_path),
                            'cache_system': 'file_based'
                        }
                        predictions_list.append(pred_info)
            
            logger.info(f"ğŸ¯ [CACHE] Found {len(predictions_list)} predictions in file-specific cache")
        else:
            logger.info(f"ğŸ“‚ [CACHE] No predictions index found in {predictions_index_file}")
        
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹  ìˆœ)
        predictions_list.sort(key=lambda x: x['data_end_date'], reverse=True)
        
        return predictions_list
        
    except Exception as e:
        logger.error(f"Error reading file-specific predictions list: {str(e)}")
        return []

def get_saved_predictions_list(limit=100):
    """
    ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ëª©ë¡ì„ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜ (ìƒˆë¡œìš´ íŒŒì¼ ì²´ê³„ í˜¸í™˜)
    
    Parameters:
    -----------
    limit : int
        ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
    
    Returns:
    --------
    list : ì €ì¥ëœ ì˜ˆì¸¡ ëª©ë¡
    """
    try:
        predictions_list = []
        
        # 1. íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œì—ì„œ ì˜ˆì¸¡ ê²€ìƒ‰
        cache_root = Path(CACHE_ROOT_DIR)
        if cache_root.exists():
            for file_dir in cache_root.iterdir():
                if not file_dir.is_dir():
                    continue
                
                predictions_dir = file_dir / 'predictions'
                predictions_index_file = predictions_dir / 'predictions_index.csv'
                
                if predictions_index_file.exists():
                    with open(predictions_index_file, 'r', encoding='utf-8') as f:
                        reader = csv.DictReader(f)
                        for row in reader:
                            if len(predictions_list) >= limit:
                                break
                            
                            prediction_start_date = row.get('prediction_start_date', row.get('first_prediction_date'))
                            data_end_date = row.get('data_end_date', row.get('prediction_base_date', row.get('prediction_date')))
                            
                            if prediction_start_date and data_end_date:
                                pred_info = {
                                    'prediction_start_date': prediction_start_date,
                                    'data_end_date': data_end_date,
                                    'prediction_date': data_end_date,
                                    'first_prediction_date': prediction_start_date,
                                    'created_at': row.get('created_at'),
                                    'semimonthly_period': row.get('semimonthly_period'),
                                    'next_semimonthly_period': row.get('next_semimonthly_period'),
                                    'prediction_count': row.get('prediction_count'),
                                    'actual_business_days': row.get('actual_business_days'),
                                    'csv_file': row.get('csv_file'),
                                    'meta_file': row.get('meta_file'),
                                    'f1_score': float(row.get('f1_score', 0)),
                                    'accuracy': float(row.get('accuracy', 0)),
                                    'mape': float(row.get('mape', 0)),
                                    'weighted_score': float(row.get('weighted_score', 0)),
                                    'naming_scheme': row.get('naming_scheme', 'file_based'),
                                    'source_file': file_dir.name,
                                    'cache_system': 'file_based'
                                }
                                predictions_list.append(pred_info)
        
        if len(predictions_list) == 0:
            logger.info("No predictions found in file-based cache system")
        
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹  ìˆœ)
        predictions_list.sort(key=lambda x: x['data_end_date'], reverse=True)
        
        logger.info(f"Retrieved {len(predictions_list)} predictions from cache systems")
        return predictions_list
        
    except Exception as e:
        logger.error(f"Error reading predictions list: {str(e)}")
        return []

def load_accumulated_predictions_from_csv(start_date, end_date=None, limit=None, file_path=None):
    """
    CSVì—ì„œ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    ìƒˆë¡œìš´ íŒŒì¼ëª… ì²´ê³„ì™€ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš©
    
    Parameters:
    -----------
    start_date : str or datetime
        ì‹œì‘ ë‚ ì§œ (ë°ì´í„° ê¸°ì¤€ì¼)
    end_date : str or datetime, optional
        ì¢…ë£Œ ë‚ ì§œ (ë°ì´í„° ê¸°ì¤€ì¼)
    limit : int, optional
        ìµœëŒ€ ë¡œë“œí•  ì˜ˆì¸¡ ê°œìˆ˜
    file_path : str, optional
        í˜„ì¬ íŒŒì¼ ê²½ë¡œ (í•´ë‹¹ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œë§Œ ê²€ìƒ‰)
    
    Returns:
    --------
    list : ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    try:
        logger.info(f"ğŸ” [CACHE_LOAD] Starting accumulated predictions load")
        logger.info(f"ğŸ” [CACHE_LOAD] Input params: start_date={start_date}, end_date={end_date}, file_path={file_path}")
        
        # ë‚ ì§œ í˜•ì‹ í†µì¼
        if isinstance(start_date, str):
            start_date = pd.to_datetime(start_date)
        if end_date and isinstance(end_date, str):
            end_date = pd.to_datetime(end_date)
        
        logger.info(f"ğŸ” [CACHE_LOAD] Loading accumulated predictions from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d') if end_date else 'latest'}")
        
        # ì €ì¥ëœ ì˜ˆì¸¡ ëª©ë¡ ì¡°íšŒ (íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
        all_predictions = []
        if file_path:
            logger.info(f"ğŸ” [CACHE_LOAD] Searching in file-specific cache directory for {os.path.basename(file_path)}")
            try:
                all_predictions = get_saved_predictions_list_for_file(file_path, limit=1000)  # âœ… íŒŒì¼ë³„ ê²€ìƒ‰
                logger.info(f"ğŸ¯ [CACHE_LOAD] Found {len(all_predictions)} prediction files in cache")
            except Exception as e:
                logger.error(f"âŒ [CACHE_LOAD] Error in get_saved_predictions_list_for_file: {str(e)}")
                logger.error(traceback.format_exc())
                return []
        else:
            logger.info(f"ğŸ” [CACHE_LOAD] Searching in global cache directory (legacy mode)")
            try:
                all_predictions = get_saved_predictions_list(limit=1000)  # ì „ì²´ ê²€ìƒ‰ (í•˜ìœ„ í˜¸í™˜)
                logger.info(f"ğŸ¯ [CACHE_LOAD] Found {len(all_predictions)} prediction files in legacy cache")
            except Exception as e:
                logger.error(f"âŒ [CACHE_LOAD] Error in get_saved_predictions_list: {str(e)}")
                logger.error(traceback.format_exc())
                return []
        
        # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ (ë°ì´í„° ê¸°ì¤€ì¼ ê¸°ì¤€)
        filtered_predictions = []
        for pred_info in all_predictions:
            # ì¸ë±ìŠ¤ì—ì„œ ë°ì´í„° ê¸°ì¤€ì¼ í™•ì¸
            data_end_date = pd.to_datetime(pred_info.get('data_end_date', pred_info.get('prediction_date')))
            
            # ë‚ ì§œ ë²”ìœ„ í™•ì¸
            if data_end_date >= start_date:
                if end_date is None or data_end_date <= end_date:
                    filtered_predictions.append(pred_info)
            
            # ì œí•œ ê°œìˆ˜ í™•ì¸
            if limit and len(filtered_predictions) >= limit:
                break
        
        logger.info(f"ğŸ“‹ [CACHE] Found {len(filtered_predictions)} matching prediction files in date range")
        if len(filtered_predictions) > 0:
            logger.info(f"ğŸ“… [CACHE] Available cached dates:")
            for pred in filtered_predictions:
                data_end_date = pred.get('data_end_date', pred.get('prediction_date'))
                logger.info(f"    - {data_end_date}")
        
        # ê° ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ
        accumulated_results = []
        for i, pred_info in enumerate(filtered_predictions):
            try:
                # ë°ì´í„° ê¸°ì¤€ì¼ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ì‹œì‘ì¼ ê³„ì‚°
                data_end_date = pd.to_datetime(pred_info.get('data_end_date', pred_info.get('prediction_date')))
                
                # ë°ì´í„° ê¸°ì¤€ì¼ë¡œë¶€í„° ì˜ˆì¸¡ ì‹œì‘ì¼ ê³„ì‚°
                prediction_start_date = data_end_date + pd.Timedelta(days=1)
                while prediction_start_date.weekday() >= 5 or is_holiday(prediction_start_date):
                    prediction_start_date += pd.Timedelta(days=1)
                
                # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                if file_path:
                    cache_dirs = get_file_cache_dirs(file_path)
                    loaded_result = load_prediction_with_attention_from_csv_in_dir(prediction_start_date, cache_dirs['predictions'])
                else:
                    loaded_result = load_prediction_with_attention_from_csv(prediction_start_date)
                
                if loaded_result['success']:
                    logger.info(f"  âœ… [CACHE] Successfully loaded cached prediction for {data_end_date.strftime('%Y-%m-%d')}")
                    # ëˆ„ì  ì˜ˆì¸¡ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
                    # ì•ˆì „í•œ ë°ì´í„° êµ¬ì¡° ìƒì„±
                    predictions = loaded_result.get('predictions', [])
                    
                    # ì˜ˆì¸¡ ë°ì´í„°ê°€ ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ì¸ ê²½ìš° ì²˜ë¦¬
                    if isinstance(predictions, dict):
                        if 'future' in predictions:
                            predictions = predictions['future']
                        elif 'predictions' in predictions:
                            predictions = predictions['predictions']
                    
                    if not isinstance(predictions, list):
                        logger.warning(f"Loaded predictions is not a list for {data_end_date.strftime('%Y-%m-%d')}: {type(predictions)}")
                        predictions = []
                    
                    metadata = loaded_result.get('metadata', {})
                    if not isinstance(metadata, dict):
                        metadata = {}
                    
                    # ğŸ”§ metrics ì•ˆì „ì„± ì²˜ë¦¬: Noneì´ë©´ ê¸°ë³¸ê°’ ì„¤ì •
                    cached_metrics = metadata.get('metrics')
                    if not cached_metrics or not isinstance(cached_metrics, dict):
                        cached_metrics = {'f1': 0.0, 'accuracy': 0.0, 'mape': 0.0, 'weighted_score': 0.0}
                    
                    accumulated_item = {
                        'date': data_end_date.strftime('%Y-%m-%d'),  # ë°ì´í„° ê¸°ì¤€ì¼
                        'prediction_start_date': loaded_result.get('prediction_start_date'),  # ì˜ˆì¸¡ ì‹œì‘ì¼
                        'predictions': predictions,
                        'metrics': cached_metrics,
                        'interval_scores': metadata.get('interval_scores', {}),
                        'next_semimonthly_period': metadata.get('next_semimonthly_period'),
                        'actual_business_days': metadata.get('actual_business_days'),
                        'original_interval_scores': metadata.get('interval_scores', {}),
                        'has_attention': loaded_result.get('has_cached_attention', False)
                    }
                    accumulated_results.append(accumulated_item)
                    logger.info(f"  âœ… [CACHE] Added to results {i+1}/{len(filtered_predictions)}: {data_end_date.strftime('%Y-%m-%d')}")
                else:
                    logger.warning(f"  âŒ [CACHE] Failed to load prediction {i+1}/{len(filtered_predictions)}: {loaded_result.get('error')}")
                    
            except Exception as e:
                logger.error(f"  âŒ Error loading prediction {i+1}/{len(filtered_predictions)}: {str(e)}")
                continue
        
        logger.info(f"ğŸ¯ [CACHE] Successfully loaded {len(accumulated_results)} predictions from CSV cache files")
        return accumulated_results
        
    except Exception as e:
        logger.error(f"Error loading accumulated predictions from CSV: {str(e)}")
        logger.error(traceback.format_exc())
        return []

def delete_saved_prediction(prediction_date):
    """
    ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    prediction_date : str or datetime
        ì‚­ì œí•  ì˜ˆì¸¡ ë‚ ì§œ
    
    Returns:
    --------
    dict : ì‚­ì œ ê²°ê³¼
    """
    try:
        # ë‚ ì§œ í˜•ì‹ í†µì¼
        if isinstance(prediction_date, str):
            pred_date = pd.to_datetime(prediction_date)
        else:
            pred_date = prediction_date
        
        date_str = pred_date.strftime('%Y%m%d')
        
        # íŒŒì¼ ê²½ë¡œë“¤ (TARGET_DATE ë°©ì‹)
        csv_filepath = os.path.join(PREDICTIONS_DIR, f"prediction_target_{date_str}.csv")
        meta_filepath = os.path.join(PREDICTIONS_DIR, f"prediction_target_{date_str}_meta.json")
        
        # íŒŒì¼ ì‚­ì œ
        deleted_files = []
        if os.path.exists(csv_filepath):
            os.remove(csv_filepath)
            deleted_files.append(csv_filepath)
        
        if os.path.exists(meta_filepath):
            os.remove(meta_filepath)
            deleted_files.append(meta_filepath)
        
        # ğŸš« ë ˆê±°ì‹œ ì¸ë±ìŠ¤ ì œê±° ê¸°ëŠ¥ì€ íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œì—ì„œ ì œê±°ë¨
        # íŒŒì¼ë³„ ìºì‹œì—ì„œëŠ” ê° íŒŒì¼ì˜ predictions_index.csvê°€ ìë™ìœ¼ë¡œ ê´€ë¦¬ë¨
        logger.info("âš ï¸ Legacy delete_saved_prediction function called - not supported in file-based cache system")
        
        return {
            'success': True,
            'deleted_files': deleted_files,
            'message': f'Prediction for {pred_date.strftime("%Y-%m-%d")} deleted successfully'
        }
        
    except Exception as e:
        logger.error(f"Error deleting saved prediction: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }

#######################################################################
# ì˜ˆì¸¡ ì‹ ë¢°ë„ ë° êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚° í•¨ìˆ˜
#######################################################################

def calculate_prediction_consistency(accumulated_predictions, target_period):
    """
    ë‹¤ìŒ ë°˜ì›”ì— ëŒ€í•œ ì—¬ëŸ¬ ë‚ ì§œì˜ ì˜ˆì¸¡ ì¼ê´€ì„±ì„ ê³„ì‚°
    
    Parameters:
    -----------
    accumulated_predictions: list
        ì—¬ëŸ¬ ë‚ ì§œì— ìˆ˜í–‰í•œ ì˜ˆì¸¡ ê²°ê³¼ ëª©ë¡
    target_period: str
        ë‹¤ìŒ ë°˜ì›” ê¸°ê°„ (ì˜ˆ: "2025-01-SM1")
    
    Returns:
    -----------
    dict: ì¼ê´€ì„± ì ìˆ˜ì™€ ê´€ë ¨ ë©”íŠ¸ë¦­
    """
    import numpy as np
    
    # ë‚ ì§œë³„ ì˜ˆì¸¡ ë°ì´í„° ì¶”ì¶œ
    period_predictions = {}
    
    for prediction in accumulated_predictions:
        # ì•ˆì „í•œ ë°ì´í„° ì ‘ê·¼
        if not isinstance(prediction, dict):
            continue
            
        prediction_date = prediction.get('date')
        next_period = prediction.get('next_semimonthly_period')
        predictions_list = prediction.get('predictions', [])
        
        if next_period != target_period:
            continue
            
        if prediction_date not in period_predictions:
            period_predictions[prediction_date] = []
        
        # predictions_listê°€ ë°°ì—´ì¸ì§€ í™•ì¸
        if not isinstance(predictions_list, list):
            logger.warning(f"predictions_list is not a list for {prediction_date}: {type(predictions_list)}")
            continue
            
        for pred in predictions_list:
            # predê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            if not isinstance(pred, dict):
                logger.warning(f"Prediction item is not a dict for {prediction_date}: {type(pred)}")
                continue
                
            pred_date = pred.get('Date') or pred.get('date')
            pred_value = pred.get('Prediction') or pred.get('prediction')
            
            # ê°’ì´ ìœ íš¨í•œì§€ í™•ì¸
            if pred_date and pred_value is not None:
                period_predictions[prediction_date].append({
                    'date': pred_date,
                    'value': pred_value
                })
    
    # ë‚ ì§œë³„ë¡œ ì •ë ¬
    prediction_dates = sorted(period_predictions.keys())
    
    if len(prediction_dates) < 2:
        return {
            "consistency_score": None,
            "message": "Insufficient prediction data (min 2 required)",
            "period": target_period,
            "dates_count": len(prediction_dates)
        }
    
    # ì¼ê´€ì„± ë¶„ì„ì„ ìœ„í•œ ë‚ ì§œ ë§¤í•‘
    date_predictions = {}
    
    for pred_date in prediction_dates:
        for p in period_predictions[pred_date]:
            target_date = p['date']
            if target_date not in date_predictions:
                date_predictions[target_date] = []
            
            date_predictions[target_date].append({
                'prediction_date': pred_date,
                'value': p['value']
            })
    
    # ê° íƒ€ê²Ÿ ë‚ ì§œë³„ ì˜ˆì¸¡ê°’ ë³€ë™ì„± ê³„ì‚°
    overall_variations = []
    
    for target_date, predictions in date_predictions.items():
        if len(predictions) >= 2:
            # ì˜ˆì¸¡ê°’ ì¶”ì¶œ (None ê°’ í•„í„°ë§)
            values = [p['value'] for p in predictions if p['value'] is not None]
            
            if len(values) < 2:
                continue
                
            # ê°’ì´ ëª¨ë‘ ê°™ì€ ê²½ìš° CVë¥¼ 0ìœ¼ë¡œ ì²˜ë¦¬
            if all(v == values[0] for v in values):
                cv = 0.0
                overall_variations.append(cv)
                continue
            
            mean_value = np.mean(values)
            std_value = np.std(values)
            
            # ë³€ë™ ê³„ìˆ˜ (Coefficient of Variation)
            cv = std_value / abs(mean_value) if mean_value != 0 else float('inf')
            overall_variations.append(cv)
    
    # ì „ì²´ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (ë³€ë™ ê³„ìˆ˜ í‰ê· ì„ 0-100 ì ìˆ˜ë¡œ ë³€í™˜)
    if overall_variations:
        avg_cv = np.mean(overall_variations)
        consistency_score = max(0, min(100, 100 - (avg_cv * 100)))
    else:
        consistency_score = None
    
    # ì‹ ë¢°ë„ ë“±ê¸‰ ë¶€ì—¬
    if consistency_score is not None:
        if consistency_score >= 90:
            grade = "Very High"
        elif consistency_score >= 75:
            grade = "High"
        elif consistency_score >= 60:
            grade = "Medium"
        elif consistency_score >= 40:
            grade = "Low"
        else:
            grade = "Very Low"
    else:
        grade = "Unable to determine"
    
    return {
        "consistency_score": consistency_score,
        "consistency_grade": grade,
        "target_period": target_period,
        "prediction_count": len(prediction_dates),
        "average_variation": avg_cv * 100 if overall_variations else None,
        "message": f"Consistency for period {target_period} based on {len(prediction_dates)} predictions"
    }

# ëˆ„ì  ì˜ˆì¸¡ì˜ êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚° í•¨ìˆ˜ (ì˜¬ë°”ë¥¸ ë²„ì „)
def calculate_accumulated_purchase_reliability(accumulated_predictions):
    """
    ëˆ„ì  ì˜ˆì¸¡ì˜ êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚°
    ê° ì˜ˆì¸¡ì—ì„œ ì–»ì€ ìµœê³  ì ìˆ˜ì˜ í•© / (ì˜ˆì¸¡ íšŸìˆ˜ Ã— 3ì )
    
    - ê° ì˜ˆì¸¡ ë‚ ì§œë§ˆë‹¤ ìµœëŒ€ 3ì ì„ ë°›ì„ ìˆ˜ ìˆìŒ
    - ì „ì²´ ìµœëŒ€ ì ìˆ˜ = ì˜ˆì¸¡ íšŸìˆ˜ Ã— 3ì 
    - êµ¬ë§¤ ì‹ ë¢°ë„ = ì´ íšë“ ì ìˆ˜ / ì „ì²´ ìµœëŒ€ ì ìˆ˜ Ã— 100%
    """
    print(f"ğŸ” [RELIABILITY] Function called with {len(accumulated_predictions) if accumulated_predictions else 0} predictions")
    
    if not accumulated_predictions or not isinstance(accumulated_predictions, list):
        print(f"âš ï¸ [RELIABILITY] Invalid input: accumulated_predictions is empty or not a list")
        return 0.0
    
    try:
        total_best_score = 0
        prediction_count = len(accumulated_predictions)
        print(f"ğŸ“Š [RELIABILITY] Processing {prediction_count} predictions...")
        
        for i, pred in enumerate(accumulated_predictions):
            if not isinstance(pred, dict):
                continue
                
            interval_scores = pred.get('interval_scores', {})
            
            if interval_scores and isinstance(interval_scores, dict):
                # ìœ íš¨í•œ interval score ì°¾ê¸°
                valid_scores = []
                for score_data in interval_scores.values():
                    if isinstance(score_data, dict) and 'score' in score_data:
                        score_value = score_data.get('score', 0)
                        if isinstance(score_value, (int, float)):
                            valid_scores.append(score_value)
                
                if valid_scores:
                    best_score = max(valid_scores)
                    # ì ìˆ˜ê°€ 3ì ì„ ì´ˆê³¼í•˜ë©´ 3ì ìœ¼ë¡œ ì œí•œ (3ì ì´ ë§Œì )
                    capped_score = min(best_score, 3.0)
                    total_best_score += capped_score
                    
                    print(f"ğŸ“Š [RELIABILITY] Prediction {i+1} ({pred.get('date')}): original_score={best_score:.1f}, capped_score={capped_score:.1f}, valid_scores={len(valid_scores)}")
                    logger.info(f"ğŸ“Š ë‚ ì§œ {pred.get('date')}: ì›ë³¸ì ìˆ˜={best_score:.1f}, ì ìš©ì ìˆ˜={capped_score:.1f}")
        
        # ì „ì²´ ëˆ„ì  êµ¬ë§¤ ì‹ ë¢°ë„ = ì´ íšë“ ì ìˆ˜ / (ì˜ˆì¸¡ íšŸìˆ˜ Ã— 3ì )
        max_possible_total_score = prediction_count * 3
        
        if max_possible_total_score > 0:
            reliability_percentage = (total_best_score / max_possible_total_score) * 100
        else:
            reliability_percentage = 0.0
        
        print(f"ğŸ¯ [RELIABILITY] FINAL CALCULATION:")
        print(f"  - ì˜ˆì¸¡ íšŸìˆ˜: {prediction_count}ê°œ")
        print(f"  - ì´ íšë“ ì ìˆ˜: {total_best_score:.1f}ì ")
        print(f"  - ìµœëŒ€ ê°€ëŠ¥ ì ìˆ˜: {max_possible_total_score}ì  ({prediction_count} Ã— 3)")
        print(f"  - êµ¬ë§¤ ì‹ ë¢°ë„: {reliability_percentage:.1f}%")
        
        logger.info(f"ğŸ¯ ì˜¬ë°”ë¥¸ êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚°:")
        logger.info(f"  - ì˜ˆì¸¡ íšŸìˆ˜: {prediction_count}ê°œ")
        logger.info(f"  - ì´ íšë“ ì ìˆ˜: {total_best_score:.1f}ì ")
        logger.info(f"  - ìµœëŒ€ ê°€ëŠ¥ ì ìˆ˜: {max_possible_total_score}ì  ({prediction_count} Ã— 3)")
        logger.info(f"  - êµ¬ë§¤ ì‹ ë¢°ë„: {reliability_percentage:.1f}%")
        
        # âœ… ì¶”ê°€ ê²€ì¦ ë¡œê¹…
        if reliability_percentage == 100.0:
            logger.warning("âš ï¸ [SIMPLE_RELIABILITY] êµ¬ë§¤ ì‹ ë¢°ë„ê°€ 100%ì…ë‹ˆë‹¤. ê° ì˜ˆì¸¡ë³„ ì ìˆ˜ í™•ì¸ í•„ìš”")
        elif reliability_percentage == 0.0:
            logger.warning("âš ï¸ [SIMPLE_RELIABILITY] êµ¬ë§¤ ì‹ ë¢°ë„ê°€ 0%ì…ë‹ˆë‹¤. ì ìˆ˜ ë°ì´í„° í™•ì¸ í•„ìš”")
        
        return reliability_percentage
            
    except Exception as e:
        logger.error(f"Error calculating accumulated purchase reliability: {str(e)}")
        return 0.0

def calculate_accumulated_purchase_reliability_with_debug(accumulated_predictions):
    """
    ë””ë²„ê·¸ ì •ë³´ì™€ í•¨ê»˜ ëˆ„ì  ì˜ˆì¸¡ì˜ êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚°
    """
    if not accumulated_predictions or not isinstance(accumulated_predictions, list):
        return 0.0, {}
    
    debug_info = {
        'prediction_count': len(accumulated_predictions),
        'individual_scores': [],
        'total_best_score': 0,
        'max_possible_total_score': 0
    }
    
    try:
        total_best_score = 0
        prediction_count = len(accumulated_predictions)
        
        for i, pred in enumerate(accumulated_predictions):
            if not isinstance(pred, dict):
                continue
                
            pred_date = pred.get('date')
            interval_scores = pred.get('interval_scores', {})
            
            best_score = 0
            capped_score = 0  # âœ… ì´ˆê¸°í™” ì¶”ê°€
            valid_scores = []  # âœ… valid_scoresë„ ì™¸ë¶€ì—ì„œ ì´ˆê¸°í™”
            
            if interval_scores and isinstance(interval_scores, dict):
                # ìœ íš¨í•œ interval score ì°¾ê¸°
                for score_data in interval_scores.values():
                    if isinstance(score_data, dict) and 'score' in score_data:
                        score_value = score_data.get('score', 0)
                        if isinstance(score_value, (int, float)):
                            valid_scores.append(score_value)
                
                if valid_scores:
                    best_score = max(valid_scores)
                    # ì ìˆ˜ê°€ 3ì ì„ ì´ˆê³¼í•˜ë©´ 3ì ìœ¼ë¡œ ì œí•œ (3ì ì´ ë§Œì )
                    capped_score = min(best_score, 3.0)
                    total_best_score += capped_score
            
            debug_info['individual_scores'].append({
                'date': pred_date,
                'original_best_score': best_score,
                'capped_score': capped_score,
                'max_score_per_prediction': 3,
                'has_valid_scores': len(valid_scores) > 0
            })
        
        # ì „ì²´ ê³„ì‚° - 3ì ì´ ë§Œì 
        max_possible_total_score = prediction_count * 3
        reliability_percentage = (total_best_score / max_possible_total_score) * 100 if max_possible_total_score > 0 else 0.0
        
        debug_info['total_best_score'] = total_best_score
        debug_info['max_possible_total_score'] = max_possible_total_score
        debug_info['reliability_percentage'] = reliability_percentage
        
        logger.info(f"ğŸ¯ ì˜¬ë°”ë¥¸ ëˆ„ì  êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚°:")
        logger.info(f"  - ì˜ˆì¸¡ íšŸìˆ˜: {prediction_count}íšŒ")
        
        # ğŸ” ê°œë³„ ì ìˆ˜ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        for score_info in debug_info['individual_scores']:
            logger.info(f"ğŸ“Š ë‚ ì§œ {score_info['date']}: ì›ë³¸ì ìˆ˜={score_info['original_best_score']}, ì ìš©ì ìˆ˜={score_info['capped_score']}, ìœ íš¨ì ìˆ˜ìˆìŒ={score_info['has_valid_scores']}")
        
        logger.info(f"  - ì´ íšë“ ì ìˆ˜: {total_best_score:.1f}ì ")
        logger.info(f"  - ìµœëŒ€ ê°€ëŠ¥ ì ìˆ˜: {max_possible_total_score}ì  ({prediction_count} Ã— 3)")
        logger.info(f"  - êµ¬ë§¤ ì‹ ë¢°ë„: {reliability_percentage:.1f}%")
        
        # âœ… ì¶”ê°€ ê²€ì¦ ë¡œê¹…
        if reliability_percentage == 100.0:
            logger.warning("âš ï¸ [RELIABILITY] êµ¬ë§¤ ì‹ ë¢°ë„ê°€ 100%ì…ë‹ˆë‹¤. ê³„ì‚° ê²€ì¦:")
            logger.warning(f"   - total_best_score: {total_best_score}")
            logger.warning(f"   - max_possible_total_score: {max_possible_total_score}")
            logger.warning(f"   - prediction_count: {prediction_count}")
            for i, score_info in enumerate(debug_info['individual_scores']):
                logger.warning(f"   - ì˜ˆì¸¡ {i+1}: {score_info}")
        elif reliability_percentage == 0.0:
            logger.warning("âš ï¸ [RELIABILITY] êµ¬ë§¤ ì‹ ë¢°ë„ê°€ 0%ì…ë‹ˆë‹¤. ê³„ì‚° ê²€ì¦:")
            logger.warning(f"   - total_best_score: {total_best_score}")
            logger.warning(f"   - max_possible_total_score: {max_possible_total_score}")
            logger.warning(f"   - prediction_count: {prediction_count}")
        
        return reliability_percentage, debug_info
            
    except Exception as e:
        logger.error(f"Error calculating accumulated purchase reliability: {str(e)}")
        return 0.0, {'error': str(e)}

def calculate_actual_business_days(predictions):
    """
    ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ ì‹¤ì œ ì˜ì—…ì¼ ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    """
    if not predictions:
        return 0
    
    try:
        actual_days = len([p for p in predictions 
                          if p.get('Date') and not p.get('is_synthetic', False)])
        return actual_days
    except Exception as e:
        logger.error(f"Error calculating actual business days: {str(e)}")
        return 0

def get_previous_semimonthly_period(semimonthly_period):
    """
    ì£¼ì–´ì§„ ë°˜ì›” ê¸°ê°„ì˜ ì´ì „ ë°˜ì›” ê¸°ê°„ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    semimonthly_period : str
        "YYYY-MM-SM1" ë˜ëŠ” "YYYY-MM-SM2" í˜•ì‹ì˜ ë°˜ì›” ê¸°ê°„
    
    Returns:
    --------
    str
        ì´ì „ ë°˜ì›” ê¸°ê°„
    """
    parts = semimonthly_period.split('-')
    year = int(parts[0])
    month = int(parts[1])
    half = parts[2]
    
    if half == "SM1":
        # ìƒë°˜ì›”ì¸ ê²½ìš° ì´ì „ ì›”ì˜ í•˜ë°˜ì›”ë¡œ
        if month == 1:
            return f"{year-1}-12-SM2"
        else:
            return f"{year}-{month-1:02d}-SM2"
    else:
        # í•˜ë°˜ì›”ì¸ ê²½ìš° ê°™ì€ ì›”ì˜ ìƒë°˜ì›”ë¡œ
        return f"{year}-{month:02d}-SM1"

#######################################################################
# ì‹œê°í™” í•¨ìˆ˜
#######################################################################

def get_global_y_range(original_df, test_dates, predict_window):
    """
    í…ŒìŠ¤íŠ¸ êµ¬ê°„ì˜ ëª¨ë“  MOPJ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì „ì—­ yì¶• ë²”ìœ„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        original_df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        test_dates: í…ŒìŠ¤íŠ¸ ë‚ ì§œ ë°°ì—´
        predict_window: ì˜ˆì¸¡ ê¸°ê°„
    
    Returns:
        tuple: (y_min, y_max) ì „ì—­ ë²”ìœ„ ê°’
    """
    # í…ŒìŠ¤íŠ¸ êµ¬ê°„ ë°ì´í„° ì¶”ì¶œ
    test_values = []
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì‹¤ì œ ê°’ ìˆ˜ì§‘
    for date in test_dates:
        if date in original_df.index and not pd.isna(original_df.loc[date, 'MOPJ']):
            test_values.append(original_df.loc[date, 'MOPJ'])
    
    # ì•ˆì „ì¥ì¹˜: ë°ì´í„°ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜
    if not test_values:
        return None, None
    
    # ìµœì†Œ/ìµœëŒ€ ê³„ì‚° (ì•½ê°„ì˜ ë§ˆì§„ ì¶”ê°€)
    y_min = min(test_values) * 0.95
    y_max = max(test_values) * 1.05
    
    return y_min, y_max

def visualize_attention_weights(model, features, prev_value, sequence_start_date, feature_names=None):
    """ëª¨ë¸ì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜"""
    model.eval()
    
    # íŠ¹ì„± ì´ë¦„ì´ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ë¡œ ìƒì„±
    if feature_names is None:
        feature_names = [f"Feature {i}" for i in range(features.shape[2])]
    else:
        # íŠ¹ì„± ìˆ˜ì— ë§ê²Œ ì¡°ì •
        feature_names = feature_names[:features.shape[2]]
    
    # í…ì„œê°€ ì•„ë‹ˆë©´ ë³€í™˜
    if not isinstance(features, torch.Tensor):
        features = torch.FloatTensor(features).to(next(model.parameters()).device)
    
    # prev_value ì²˜ë¦¬
    if prev_value is not None:
        if not isinstance(prev_value, torch.Tensor):
            try:
                prev_value = float(prev_value)
                prev_value = torch.FloatTensor([prev_value]).to(next(model.parameters()).device)
            except (TypeError, ValueError):
                logger.warning("Warning: prev_valueë¥¼ ìˆ«ìë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                prev_value = torch.FloatTensor([0.0]).to(next(model.parameters()).device)
    
    # ì‹œí€€ìŠ¤ ê¸¸ì´
    seq_len = features.shape[1]
    
    # ë‚ ì§œ ë¼ë²¨ ìƒì„± (ì‹œí€€ìŠ¤ ì‹œì‘ì¼ë¡œë¶€í„°)
    date_labels = []
    for i in range(seq_len):
        try:
            date = sequence_start_date - timedelta(days=seq_len-i-1)
            date_labels.append(format_date(date, '%Y-%m-%d'))
        except:
            date_labels.append(f"T-{seq_len-i-1}")
    
    # 1x2 ê·¸ë˜í”„ ìƒì„± (íŠ¹ì„± ì¤‘ìš”ë„, ì‹œê°„ ì¤‘ìš”ë„)
    fig, axes = plt.subplots(1, 2, figsize=(24, 12))
    fig.suptitle(f"Model Importance Analysis - {format_date(sequence_start_date, '%Y-%m-%d')}", 
                fontsize=16)
    
    # íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°
    feature_importance = np.mean(np.abs(features[0].cpu().numpy()), axis=0)
    
    # ì •ê·œí™”
    if np.sum(feature_importance) > 0:
        feature_importance = feature_importance / np.sum(feature_importance)
    
    # íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_idx = np.argsort(feature_importance)[::-1]
    sorted_features = [feature_names[i] for i in sorted_idx]
    sorted_importance = feature_importance[sorted_idx]
    
    # ìƒìœ„ 10ê°œ íŠ¹ì„±ë§Œ í‘œì‹œ
    top_n = min(10, len(sorted_features))
    
    # í”Œë¡¯ 1: íŠ¹ì„±ë³„ ì¤‘ìš”ë„ (ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„)
    ax1 = axes[0]
    
    try:
        # ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ í‘œì‹œ
        y_pos = range(top_n)
        ax1.barh(y_pos, sorted_importance[:top_n], color='#3498db')
        ax1.set_yticks(y_pos)
        ax1.set_yticklabels(sorted_features[:top_n])
        ax1.set_title("Top Feature Importance")
        ax1.set_xlabel("Relative Importance")
        
        # ì¤‘ìš”ë„ ê°’ í‘œì‹œ
        for i, v in enumerate(sorted_importance[:top_n]):
            ax1.text(v + 0.01, i, f"{v:.3f}", va='center')
    except Exception as e:
        logger.error(f"Feature importance visualization error: {str(e)}")
        ax1.text(0.5, 0.5, "Visualization error", ha='center', va='center')
    
    # í”Œë¡¯ 2: ì‹œê°„ì  ì¤‘ìš”ë„
    ax2 = axes[1]
    
    # ê° ì‹œì ì˜ í‰ê·  ì ˆëŒ€ê°’ìœ¼ë¡œ ì‹œê°„ì  ì¤‘ìš”ë„ ì¶”ì •
    temporal_importance = np.mean(np.abs(features[0].cpu().numpy()), axis=1)
    if np.sum(temporal_importance) > 0:
        temporal_importance = temporal_importance / np.sum(temporal_importance)
    
    try:
        # ì‹œê°„ì  ì¤‘ìš”ë„ í‘œì‹œ - ë§‰ëŒ€ ê·¸ë˜í”„
        ax2.bar(range(len(date_labels)), temporal_importance, color='#2ecc71')
        ax2.set_xticks(range(len(date_labels)))
        ax2.set_xticklabels(date_labels, rotation=45, ha='right')
        ax2.set_title("Time Sequence Importance")
        ax2.set_xlabel("Date")
        ax2.set_ylabel("Relative Importance")
        
        # ë§ˆì§€ë§‰ ì‹œì  ê°•ì¡°
        ax2.bar(len(date_labels)-1, temporal_importance[-1], color='#e74c3c')
    except Exception as e:
        logger.error(f"Time importance visualization error: {str(e)}")
        ax2.text(0.5, 0.5, "Visualization error", ha='center', va='center')
    
    plt.tight_layout()
    
    # ì´ë¯¸ì§€ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
    img_buf = io.BytesIO()
    plt.savefig(img_buf, format='png', dpi=600)
    plt.close()
    img_buf.seek(0)
    
    # Base64ë¡œ ì¸ì½”ë”©
    img_str = base64.b64encode(img_buf.read()).decode('utf-8')
    
    # íŒŒì¼ ì €ì¥ - íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
    try:
        cache_dirs = get_file_cache_dirs()  # í˜„ì¬ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        attn_dir = cache_dirs['plots']  # plots ë””ë ‰í† ë¦¬ì— ì €ì¥
        
        filename = os.path.join(attn_dir, f"attention_{format_date(sequence_start_date, '%Y%m%d')}.png")
        with open(filename, 'wb') as f:
            f.write(base64.b64decode(img_str))
    except Exception as e:
        logger.error(f"Error saving attention image: {str(e)}")
        filename = None
    
    return filename, img_str, {
        'feature_importance': dict(zip(sorted_features, sorted_importance.tolist())),
        'temporal_importance': dict(zip(date_labels, temporal_importance.tolist()))
    }

def plot_prediction_basic(sequence_df, prediction_start_date, start_day_value, 
                         f1, accuracy, mape, weighted_score_pct, 
                         current_date=None,  # ğŸ”‘ ì¶”ê°€: ë°ì´í„° ì»·ì˜¤í”„ ë‚ ì§œ
                         save_prefix=None, title_prefix="Basic Prediction Graph",
                         y_min=None, y_max=None, file_path=None):
    """
    ê¸°ë³¸ ì˜ˆì¸¡ ê·¸ë˜í”„ ì‹œê°í™” - ê³¼ê±°/ë¯¸ë˜ ëª…í™• êµ¬ë¶„
    ğŸ”‘ current_date ì´í›„ëŠ” ë¯¸ë˜ ì˜ˆì¸¡ìœ¼ë¡œë§Œ í‘œì‹œ (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)
    """
    
    fig = None
    
    try:
        logger.info(f"Creating prediction graph for prediction starting {format_date(prediction_start_date)}")
        
        # ğŸ“ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì • (íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
        if save_prefix is None:
            try:
                cache_dirs = get_file_cache_dirs(file_path)
                save_dir = cache_dirs['plots']
            except Exception as e:
                logger.warning(f"Could not get cache directories for plots: {str(e)}")
                save_dir = Path("temp_plots")
                save_dir.mkdir(parents=True, exist_ok=True)
        else:
            save_dir = Path(save_prefix)
            save_dir.mkdir(parents=True, exist_ok=True)
        
        # DataFrameì˜ ë‚ ì§œ ì—´ì´ ë¬¸ìì—´ì¸ ê²½ìš° ë‚ ì§œ ê°ì²´ë¡œ ë³€í™˜
        if 'Date' in sequence_df.columns and isinstance(sequence_df['Date'].iloc[0], str):
            sequence_df['Date'] = pd.to_datetime(sequence_df['Date'])
        
        # âœ… current_date ê¸°ì¤€ìœ¼ë¡œ ê³¼ê±°/ë¯¸ë˜ ë¶„í• 
        if current_date is not None:
            current_date = pd.to_datetime(current_date)
            
            # ê³¼ê±° ë°ì´í„° (current_date ì´ì „): ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ëª¨ë‘ í‘œì‹œ ê°€ëŠ¥
            past_df = sequence_df[sequence_df['Date'] <= current_date].copy()
            # ë¯¸ë˜ ë°ì´í„° (current_date ì´í›„): ì˜ˆì¸¡ê°’ë§Œ í‘œì‹œ
            future_df = sequence_df[sequence_df['Date'] > current_date].copy()
            
            # ê³¼ê±° ë°ì´í„°ì—ì„œ ì‹¤ì œê°’ì´ ìˆëŠ” ê²ƒë§Œ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©
            valid_df = past_df.dropna(subset=['Actual']) if 'Actual' in past_df.columns else pd.DataFrame()
            
            logger.info(f"  ğŸ“Š Data split - Past: {len(past_df)}, Future: {len(future_df)}, Validation: {len(valid_df)}")
        else:
            # current_dateê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)
            valid_df = sequence_df.dropna(subset=['Actual']) if 'Actual' in sequence_df.columns else pd.DataFrame()
            future_df = sequence_df
            past_df = valid_df
        
        pred_df = sequence_df.dropna(subset=['Prediction'])
        
        # ê·¸ë˜í”„ ìƒì„±
        fig = plt.figure(figsize=(16, 9))
        plt.subplots_adjust(top=0.85)
        gs = GridSpec(2, 1, height_ratios=[3, 1], hspace=0.3, figure=fig)
        
        # ê·¸ë˜í”„ íƒ€ì´í‹€ê³¼ ì„œë¸Œíƒ€ì´í‹€
        if isinstance(prediction_start_date, str):
            main_title = f"{title_prefix} - Start: {prediction_start_date}"
        else:
            main_title = f"{title_prefix} - Start: {prediction_start_date.strftime('%Y-%m-%d')}"
        
        # âœ… ê³¼ê±°/ë¯¸ë˜ êµ¬ë¶„ ì •ë³´ê°€ í¬í•¨ëœ ì„œë¸Œíƒ€ì´í‹€
        if current_date is not None:
            validation_count = len(valid_df)
            future_count = len(future_df)
            subtitle = f"Data Cutoff: {current_date.strftime('%Y-%m-%d')} | Validation: {validation_count} pts | Future: {future_count} pts"
            if validation_count > 0:
                subtitle += f" | F1: {f1:.3f}, Acc: {accuracy:.1f}%, MAPE: {mape:.1f}%"
        else:
            # ê¸°ì¡´ ë°©ì‹
            if f1 == 0 and accuracy == 0 and mape == 0 and weighted_score_pct == 0:
                subtitle = "Future Prediction Only (No Validation Data Available)"
            else:
                subtitle = f"F1: {f1:.2f}, Acc: {accuracy:.2f}%, MAPE: {mape:.2f}%, Score: {weighted_score_pct:.2f}%"

        fig.text(0.5, 0.94, main_title, ha='center', fontsize=14, weight='bold')
        fig.text(0.5, 0.90, subtitle, ha='center', fontsize=12)
        
        # (1) ìƒë‹¨: ê°€ê²© ì˜ˆì¸¡ ê·¸ë˜í”„
        ax1 = fig.add_subplot(gs[0])
        ax1.set_title("Price Prediction: Past Validation vs Future Forecast", fontsize=13)
        ax1.grid(True, linestyle='--', alpha=0.5)

        if y_min is not None and y_max is not None:
            ax1.set_ylim(y_min, y_max)
        
        # ì˜ˆì¸¡ ì‹œì‘ ë‚ ì§œ ì²˜ë¦¬
        if isinstance(prediction_start_date, str):
            start_date = pd.to_datetime(prediction_start_date)
        else:
            start_date = prediction_start_date
        
        # ì‹œì‘ì¼ ì´ì „ ë‚ ì§œ ê³„ì‚° (ì—°ê²°ì ìš©)
        prev_date = start_date - pd.Timedelta(days=1)
        while prev_date.weekday() >= 5 or is_holiday(prev_date):
            prev_date -= pd.Timedelta(days=1)
        
        # âœ… 1. ê³¼ê±° ì‹¤ì œê°’ (íŒŒë€ìƒ‰ ì‹¤ì„ ) - ê°€ì¥ ì¤‘ìš”í•œ ê¸°ì¤€ì„ 
        if not valid_df.empty:
            real_dates = [prev_date] + valid_df['Date'].tolist()
            real_values = [start_day_value] + valid_df['Actual'].tolist()
            ax1.plot(real_dates, real_values, marker='o', color='blue', 
                    label='Actual (Past)', linewidth=2.5, markersize=5, zorder=3)
        
        # âœ… 2. ê³¼ê±° ì˜ˆì¸¡ê°’ (íšŒìƒ‰ ì ì„ ) - ëª¨ë¸ ì„±ëŠ¥ í™•ì¸ìš©
        if not valid_df.empty:
            past_pred_dates = [prev_date] + valid_df['Date'].tolist()
            past_pred_values = [start_day_value] + valid_df['Prediction'].tolist()
            ax1.plot(past_pred_dates, past_pred_values, marker='x', color='gray', 
                    label='Predicted (Past)', linewidth=1.5, linestyle=':', markersize=4, alpha=0.8, zorder=2)
        
        # âœ… 3. ë¯¸ë˜ ì˜ˆì¸¡ê°’ (ë¹¨ê°„ìƒ‰ ì ì„ ) - í•µì‹¬ ì˜ˆì¸¡
        if not future_df.empty:
            future_dates = future_df['Date'].tolist()
            future_values = future_df['Prediction'].tolist()
            
            # ì—°ê²°ì„  (ë§ˆì§€ë§‰ ì‹¤ì œê°’ â†’ ì²« ë¯¸ë˜ ì˜ˆì¸¡ê°’)
            if not valid_df.empty and future_dates:
                # ë§ˆì§€ë§‰ ê²€ì¦ ë°ì´í„°ì˜ ì‹¤ì œê°’ì—ì„œ ì²« ë¯¸ë˜ ì˜ˆì¸¡ìœ¼ë¡œ ì—°ê²°
                connection_x = [valid_df['Date'].iloc[-1], future_dates[0]]
                connection_y = [valid_df['Actual'].iloc[-1], future_values[0]]
                ax1.plot(connection_x, connection_y, '--', color='orange', alpha=0.6, linewidth=1.5, zorder=1)
            elif start_day_value is not None and future_dates:
                # ê²€ì¦ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì‹œì‘ê°’ì—ì„œ ì—°ê²°
                connection_x = [prev_date, future_dates[0]]
                connection_y = [start_day_value, future_values[0]]
                ax1.plot(connection_x, connection_y, '--', color='orange', alpha=0.6, linewidth=1.5, zorder=1)
            
            ax1.plot(future_dates, future_values, marker='o', color='red', 
                    label='Predicted (Future)', linewidth=2.5, linestyle='--', markersize=5, zorder=3)
        
        # âœ… 4. ë°ì´í„° ì»·ì˜¤í”„ ë¼ì¸ (ì´ˆë¡ìƒ‰ ì„¸ë¡œì„ )
        if current_date is not None:
            ax1.axvline(x=current_date, color='green', linestyle='-', alpha=0.8, 
                       linewidth=2.5, label=f'Data Cutoff', zorder=4)
            
            # ì»·ì˜¤í”„ ë‚ ì§œ í…ìŠ¤íŠ¸ ì¶”ê°€
            ax1.text(current_date, ax1.get_ylim()[1] * 0.95, 
                    f'{current_date.strftime("%m/%d")}', 
                    ha='center', va='top', fontsize=10, 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor='lightgreen', alpha=0.7))
        else:
            # ì˜ˆì¸¡ ì‹œì‘ì ì— ìˆ˜ì§ì„  í‘œì‹œ (ê¸°ì¡´ ë°©ì‹)
            ax1.axvline(x=start_date, color='green', linestyle='--', alpha=0.7, 
                       linewidth=2, label='Prediction Start', zorder=4)
        
        # âœ… 5. ë°°ê²½ ìƒ‰ì¹  (ë°©í–¥ì„± ì¼ì¹˜ ì—¬ë¶€) - ê²€ì¦ ë°ì´í„°ë§Œ
        if not valid_df.empty and len(valid_df) > 1:
            for i in range(len(valid_df) - 1):
                curr_date = valid_df['Date'].iloc[i]
                next_date = valid_df['Date'].iloc[i + 1]
                
                curr_actual = valid_df['Actual'].iloc[i]
                next_actual = valid_df['Actual'].iloc[i + 1]
                curr_pred = valid_df['Prediction'].iloc[i]
                next_pred = valid_df['Prediction'].iloc[i + 1]
                
                # ë°©í–¥ ê³„ì‚°
                actual_dir = np.sign(next_actual - curr_actual)
                pred_dir = np.sign(next_pred - curr_pred)
                
                # ë°©í–¥ ì¼ì¹˜ ì—¬ë¶€ì— ë”°ë¥¸ ìƒ‰ìƒ
                color = 'lightblue' if actual_dir == pred_dir else 'lightcoral'
                ax1.axvspan(curr_date, next_date, color=color, alpha=0.15, zorder=0)
        
        ax1.set_xlabel("")
        ax1.set_ylabel("Price (USD/MT)", fontsize=11)
        ax1.legend(loc='upper left', fontsize=10)
        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        
        # âœ… (2) í•˜ë‹¨: ì˜¤ì°¨ ë¶„ì„ - ê²€ì¦ ë°ì´í„°ë§Œ ë˜ëŠ” ë³€í™”ëŸ‰
        ax2 = fig.add_subplot(gs[1])
        ax2.grid(True, linestyle='--', alpha=0.5)
        
        if not valid_df.empty and len(valid_df) > 0:
            # ê²€ì¦ ë°ì´í„°ì˜ ì ˆëŒ€ ì˜¤ì°¨
            error_dates = valid_df['Date'].tolist()
            error_values = [abs(row['Actual'] - row['Prediction']) for _, row in valid_df.iterrows()]
            
            if error_dates and error_values:
                bars = ax2.bar(error_dates, error_values, width=0.6, color='salmon', alpha=0.7, edgecolor='darkred', linewidth=0.5)
                ax2.set_title(f"Prediction Error - Validation Period ({len(error_dates)} points)", fontsize=11)
                
                # í‰ê·  ì˜¤ì°¨ ë¼ì¸
                avg_error = np.mean(error_values)
                ax2.axhline(y=avg_error, color='red', linestyle='--', alpha=0.8, 
                           label=f'Avg Error: {avg_error:.2f}')
                ax2.legend(fontsize=9)
            else:
                ax2.text(0.5, 0.5, "No validation errors to display", 
                        ha='center', va='center', transform=ax2.transAxes, fontsize=10)
                ax2.set_title("Error Analysis")
        else:
            # ì‹¤ì œê°’ì´ ì—†ëŠ” ê²½ìš°: ë¯¸ë˜ ì˜ˆì¸¡ì˜ ì¼ì¼ ë³€í™”ëŸ‰ í‘œì‹œ
            if not future_df.empty and len(future_df) > 1:
                change_dates = future_df['Date'].iloc[1:].tolist()
                change_values = np.diff(future_df['Prediction'].values)
                
                # ìƒìŠ¹/í•˜ë½ì— ë”°ë¥¸ ìƒ‰ìƒ êµ¬ë¶„
                colors = ['green' if change >= 0 else 'red' for change in change_values]
                
                bars = ax2.bar(change_dates, change_values, width=0.6, color=colors, alpha=0.7)
                ax2.set_title("Daily Price Changes - Future Predictions", fontsize=11)
                ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5, linewidth=1)
                
                # ë²”ë¡€ ì¶”ê°€
                from matplotlib.patches import Patch
                legend_elements = [Patch(facecolor='green', alpha=0.7, label='Price Up'),
                                 Patch(facecolor='red', alpha=0.7, label='Price Down')]
                ax2.legend(handles=legend_elements, fontsize=9)
            else:
                ax2.text(0.5, 0.5, "Insufficient data for change analysis", 
                        ha='center', va='center', transform=ax2.transAxes, fontsize=10)
                ax2.set_title("Change Analysis")
        
        ax2.set_xlabel("Date", fontsize=11)
        ax2.set_ylabel("Value", fontsize=11)
        ax2.xaxis.set_major_locator(mdates.AutoDateLocator())
        ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')
        
        plt.tight_layout()
        
        # íŒŒì¼ ê²½ë¡œ ìƒì„±
        if isinstance(prediction_start_date, str):
            date_str = pd.to_datetime(prediction_start_date).strftime('%Y%m%d')
        else:
            date_str = prediction_start_date.strftime('%Y%m%d')
        
        filename = f"prediction_start_{date_str}.png"
        full_path = save_dir / filename
        
        # ì´ë¯¸ì§€ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
        img_buf = io.BytesIO()
        plt.savefig(img_buf, format='png', dpi=300, bbox_inches='tight')
        img_buf.seek(0)
        
        # Base64ë¡œ ì¸ì½”ë”©
        img_str = base64.b64encode(img_buf.read()).decode('utf-8')
        
        # íŒŒì¼ë¡œ ì €ì¥
        plt.savefig(str(full_path), dpi=300, bbox_inches='tight')
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        plt.close(fig)
        plt.clf()
        img_buf.close()
        
        logger.info(f"Enhanced prediction graph saved: {full_path}")
        logger.info(f"  - Past validation points: {len(valid_df) if not valid_df.empty else 0}")
        logger.info(f"  - Future prediction points: {len(future_df) if not future_df.empty else 0}")
        
        return str(full_path), img_str
        
    except Exception as e:
        if fig is not None:
            plt.close(fig)
        plt.close('all')
        plt.clf()
        
        logger.error(f"Error in enhanced graph creation: {str(e)}")
        logger.error(traceback.format_exc())
        return None, None
    
def plot_moving_average_analysis(ma_results, sequence_start_date, save_prefix=None,
                               title_prefix="Moving Average Analysis", y_min=None, y_max=None, file_path=None):
    """ì´ë™í‰ê·  ë¶„ì„ ì‹œê°í™”"""
    try:
        # ì…ë ¥ ë°ì´í„° ê²€ì¦
        if not ma_results or len(ma_results) == 0:
            logger.warning("No moving average results to plot")
            return None, None
            
        # ma_results í˜•ì‹: {'ma5': [{'date': '...', 'prediction': X, 'actual': Y, 'ma': Z}, ...], 'ma10': [...]}
        windows = sorted(ma_results.keys())
        
        if len(windows) == 0:
            logger.warning("No moving average windows found")
            return None, None
        
        # ìœ íš¨í•œ ìœˆë„ìš° í•„í„°ë§
        valid_windows = []
        for window_key in windows:
            if window_key in ma_results and ma_results[window_key] and len(ma_results[window_key]) > 0:
                valid_windows.append(window_key)
        
        if len(valid_windows) == 0:
            logger.warning("No valid moving average data found")
            return None, None
        
        fig = plt.figure(figsize=(12, max(4, 4 * len(valid_windows))))
        
        if isinstance(sequence_start_date, str):
            title = f"{title_prefix} Starting {sequence_start_date}"
        else:
            title = f"{title_prefix} Starting {sequence_start_date.strftime('%Y-%m-%d')}"
            
        fig.suptitle(title, fontsize=16)
        
        for idx, window_key in enumerate(valid_windows):
            window_num = window_key.replace('ma', '')
            ax = fig.add_subplot(len(valid_windows), 1, idx+1)
            
            window_data = ma_results[window_key]
            
            # ë°ì´í„° ê²€ì¦
            if not window_data or len(window_data) == 0:
                ax.text(0.5, 0.5, f"No data for {window_key}", 
                       ha='center', va='center', transform=ax.transAxes)
                continue
            
            # ë‚ ì§œ, ì˜ˆì¸¡, ì‹¤ì œê°’, MA ì¶”ì¶œ
            dates = []
            predictions = []
            actuals = []
            ma_preds = []
            
            for item in window_data:
                try:
                    # ì•ˆì „í•œ ë°ì´í„° ì¶”ì¶œ
                    if isinstance(item['date'], str):
                        dates.append(pd.to_datetime(item['date']))
                    else:
                        dates.append(item['date'])
                    
                    # None ê°’ ì²˜ë¦¬
                    predictions.append(item.get('prediction', 0))
                    actuals.append(item.get('actual', None))
                    ma_preds.append(item.get('ma', None))
                except Exception as e:
                    logger.warning(f"Error processing MA data item: {str(e)}")
                    continue
            
            if len(dates) == 0:
                ax.text(0.5, 0.5, f"No valid data for {window_key}", 
                       ha='center', va='center', transform=ax.transAxes)
                continue
            
            # yì¶• ë²”ìœ„ ì„¤ì •
            if y_min is not None and y_max is not None:
                ax.set_ylim(y_min, y_max)
            
            # ì›ë³¸ ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ (ì˜…ê²Œ)
            ax.plot(dates, actuals, marker='o', color='blue', alpha=0.3, label='Actual')
            ax.plot(dates, predictions, marker='o', color='red', alpha=0.3, label='Predicted')
            
            # ì´ë™í‰ê· 
            # ì‹¤ì œê°’(actuals)ê³¼ ì´ë™í‰ê· (ma_preds) ëª¨ë‘ Noneì´ ì•„ë‹Œ ì¸ë±ìŠ¤ë¥¼ ì„ íƒ
            valid_indices = [
                i for i in range(len(ma_preds))
                if (ma_preds[i] is not None and actuals[i] is not None)
            ]

            if valid_indices:
                valid_dates = [dates[i] for i in valid_indices]
                valid_ma = [ma_preds[i] for i in valid_indices]
                valid_actuals = [actuals[i] for i in valid_indices]
                
                # ë°°ì—´ë¡œ ë³€í™˜
                valid_actuals_arr = np.array(valid_actuals)
                valid_ma_arr = np.array(valid_ma)
                
                # ì‹¤ì œê°’ì´ 0ì¸ í•­ëª©ì€ ì œì™¸í•˜ì—¬ MAPE ê³„ì‚°
                non_zero_mask = valid_actuals_arr != 0
                if np.sum(non_zero_mask) > 0:
                    ma_mape = np.mean(np.abs((valid_actuals_arr[non_zero_mask] - valid_ma_arr[non_zero_mask]) /
                                           valid_actuals_arr[non_zero_mask])) * 100
                else:
                    ma_mape = 0.0
                
                ax.set_title(f"MA-{window_num} Analysis (MAPE: {ma_mape:.2f}%, Count: {len(valid_indices)})")
            else:
                ax.set_title(f"MA-{window_num} Analysis (Insufficient data)")
            
            ax.legend()
            ax.grid(True, linestyle='--', alpha=0.5)
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')
            ax.set_xlabel("Date")
            ax.set_ylabel("Price")
        
        plt.tight_layout()
        
        # ğŸ“ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì • (íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
        if save_prefix is None:
            try:
                cache_dirs = get_file_cache_dirs(file_path)
                save_dir = cache_dirs['ma_plots']
            except Exception as e:
                logger.warning(f"Could not get cache directories for MA plots: {str(e)}")
                save_dir = Path("temp_ma_plots")
                save_dir.mkdir(parents=True, exist_ok=True)
        else:
            save_dir = Path(save_prefix)
            save_dir.mkdir(parents=True, exist_ok=True)
        
        if isinstance(sequence_start_date, str):
            date_str = pd.to_datetime(sequence_start_date).strftime('%Y%m%d')
        else:
            date_str = sequence_start_date.strftime('%Y%m%d')
            
        filename = save_dir / f"ma_analysis_{date_str}.png"
        
        # ì´ë¯¸ì§€ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
        img_buf = io.BytesIO()
        plt.savefig(img_buf, format='png', dpi=300)
        img_buf.seek(0)
        
        # Base64ë¡œ ì¸ì½”ë”©
        img_str = base64.b64encode(img_buf.read()).decode('utf-8')
        
        # íŒŒì¼ ì €ì¥
        plt.savefig(str(filename), dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Moving Average graph saved: {filename}")
        return str(filename), img_str
        
    except Exception as e:
        logger.error(f"Error in moving average visualization: {str(e)}")
        logger.error(traceback.format_exc())
        return None, None

def compute_performance_metrics_improved(validation_data, start_day_value):
    """
    ê²€ì¦ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•œ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    """
    try:
        if not validation_data or len(validation_data) < 1:
            logger.info("No validation data available - this is normal for pure future predictions")
            return None
        
        # ê²€ì¦ ë°ì´í„°ì—ì„œ ê°’ ì¶”ì¶œ
        actual_vals = [start_day_value] + [item['actual'] for item in validation_data]
        pred_vals = [start_day_value] + [item['prediction'] for item in validation_data]
        
        # F1 ì ìˆ˜ ê³„ì‚°
        f1, f1_report = calculate_f1_score(actual_vals, pred_vals)
        direction_accuracy = calculate_direction_accuracy(actual_vals, pred_vals)
        weighted_score, max_score = calculate_direction_weighted_score(actual_vals[1:], pred_vals[1:])
        weighted_score_pct = (weighted_score / max_score) * 100 if max_score > 0 else 0.0
        mape = calculate_mape(actual_vals[1:], pred_vals[1:])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        cosine_similarity = None
        if len(actual_vals) > 1:
            diff_actual = np.diff(actual_vals)
            diff_pred = np.diff(pred_vals)
            norm_actual = np.linalg.norm(diff_actual)
            norm_pred = np.linalg.norm(diff_pred)
            if norm_actual > 0 and norm_pred > 0:
                cosine_similarity = np.dot(diff_actual, diff_pred) / (norm_actual * norm_pred)
        
        return {
            'f1': float(f1),
            'accuracy': float(direction_accuracy),
            'mape': float(mape),
            'weighted_score': float(weighted_score_pct),
            'cosine_similarity': float(cosine_similarity) if cosine_similarity is not None else None,
            'f1_report': f1_report,
            'validation_points': len(validation_data)
        }
        
    except Exception as e:
        logger.error(f"Error computing improved metrics: {str(e)}")
        return None

def calculate_f1_score(actual, predicted):
    """ë°©í–¥ì„± ì˜ˆì¸¡ì˜ F1 ì ìˆ˜ ê³„ì‚°"""
    actual_directions = np.sign(np.diff(actual))
    predicted_directions = np.sign(np.diff(predicted))

    if len(actual_directions) < 2:
        return 0.0, "Insufficient data for classification report"
        
    try:
        # zero_division=0 íŒŒë¼ë¯¸í„° ì¶”ê°€
        f1 = f1_score(actual_directions, predicted_directions, average='macro', zero_division=0)
        report = classification_report(actual_directions, predicted_directions, 
                                    digits=2, zero_division=0)
    except Exception as e:
        logger.error(f"Error in calculating F1 score: {str(e)}")
        return 0.0, "Error in calculation"
        
    return f1, report

def calculate_direction_accuracy(actual, predicted):
    """ë“±ë½ ë°©í–¥ ì˜ˆì¸¡ì˜ ì •í™•ë„ ê³„ì‚°"""
    if len(actual) <= 1:
        return 0.0
        
    try:
        actual_directions = np.sign(np.diff(actual))
        predicted_directions = np.sign(np.diff(predicted))
        
        correct_predictions = (actual_directions == predicted_directions).sum()
        total_predictions = len(actual_directions)
        
        accuracy = (correct_predictions / total_predictions) * 100
        return accuracy
    except Exception as e:
        logger.error(f"Error in calculating direction accuracy: {str(e)}")
        return 0.0
    
def calculate_direction_weighted_score(actual, predicted):
    """ë³€í™”ìœ¨ ê¸°ë°˜ì˜ ê°€ì¤‘ ì ìˆ˜ ê³„ì‚°"""
    if len(actual) <= 1:
        return 0.0, 1.0
        
    try:
        actual_changes = 100 * np.diff(actual) / actual[:-1]
        predicted_changes = 100 * np.diff(predicted) / predicted[:-1]

        def assign_class(change):
            if change > 6:
                return 1
            elif 4 < change <= 6:
                return 2
            elif 2 < change <= 4:
                return 3
            elif -2 <= change <= 2:
                return 4
            elif -4 <= change < -2:
                return 5
            elif -6 <= change < -4:
                return 6
            else:
                return 7

        actual_classes = np.array([assign_class(x) for x in actual_changes])
        predicted_classes = np.array([assign_class(x) for x in predicted_changes])

        score = 0
        for ac, pc in zip(actual_classes, predicted_classes):
            diff = abs(ac - pc)
            score += max(0, 3 - diff)

        max_score = 3 * len(actual_classes)
        return score, max_score
    except Exception as e:
        logger.error(f"Error in calculating weighted score: {str(e)}")
        return 0.0, 1.0

def calculate_mape(actual, predicted):
    """MAPE ê³„ì‚° í•¨ìˆ˜"""
    try:
        if len(actual) == 0:
            return 0.0
        # inf ë°©ì§€ë¥¼ ìœ„í•´ 0ì´ ì•„ë‹Œ ê°’ë§Œ ì‚¬ìš©
        mask = actual != 0
        if not any(mask):
            return 0.0
        return np.mean(np.abs((actual[mask] - predicted[mask]) / actual[mask])) * 100
    except Exception as e:
        logger.error(f"Error in MAPE calculation: {str(e)}")
        return 0.0

def calculate_moving_averages_with_history(predictions, historical_data, target_col='MOPJ', windows=[5, 10, 23]):
    """ì˜ˆì¸¡ ë°ì´í„°ì™€ ê³¼ê±° ë°ì´í„°ë¥¼ ëª¨ë‘ í™œìš©í•œ ì´ë™í‰ê·  ê³„ì‚°"""
    try:
        # ì…ë ¥ ë°ì´í„° ê²€ì¦
        if not predictions or len(predictions) == 0:
            logger.warning("No predictions provided for moving average calculation")
            return {}
            
        if historical_data is None or historical_data.empty:
            logger.warning("No historical data provided for moving average calculation")
            return {}
            
        if target_col not in historical_data.columns:
            logger.warning(f"Target column {target_col} not found in historical data")
            return {}
        
        results = {}
        
        # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ë° ì •ë ¬
        try:
            pred_df = pd.DataFrame(predictions) if not isinstance(predictions, pd.DataFrame) else predictions.copy()
            
            # Date ì»¬ëŸ¼ ê²€ì¦
            if 'Date' not in pred_df.columns:
                logger.error("Date column not found in predictions")
                return {}
                
            pred_df['Date'] = pd.to_datetime(pred_df['Date'])
            pred_df = pred_df.sort_values('Date')
            
            # Prediction ì»¬ëŸ¼ ê²€ì¦
            if 'Prediction' not in pred_df.columns:
                logger.error("Prediction column not found in predictions")
                return {}
                
        except Exception as e:
            logger.error(f"Error processing prediction data: {str(e)}")
            return {}
        
        # ì˜ˆì¸¡ ì‹œì‘ì¼ í™•ì¸
        prediction_start_date = pred_df['Date'].min()
        logger.info(f"MA calculation - prediction start date: {prediction_start_date}")
        
        # ê³¼ê±° ë°ì´í„°ì—ì„œ íƒ€ê²Ÿ ì—´ ì¶”ì¶œ (ì˜ˆì¸¡ ì‹œì‘ì¼ ì´ì „)
        historical_series = pd.Series(
            data=historical_data.loc[historical_data.index < prediction_start_date, target_col],
            index=historical_data.loc[historical_data.index < prediction_start_date].index
        )
        
        # ìµœê·¼ 30ì¼ë§Œ ì‚¬ìš© (ì´ë™í‰ê·  ê³„ì‚°ì— ì¶©ë¶„)
        historical_series = historical_series.sort_index().tail(30)
        
        # ì˜ˆì¸¡ ë°ì´í„°ì—ì„œ ì‹œë¦¬ì¦ˆ ìƒì„±
        prediction_series = pd.Series(
            data=pred_df['Prediction'].values,
            index=pred_df['Date']
        )
        
        # ê³¼ê±°ì™€ ì˜ˆì¸¡ ë°ì´í„° ê²°í•©
        combined_series = pd.concat([historical_series, prediction_series])
        combined_series = combined_series[~combined_series.index.duplicated(keep='last')]
        combined_series = combined_series.sort_index()
        
        logger.info(f"Combined series for MA: {len(combined_series)} data points "
                   f"({len(historical_series)} historical, {len(prediction_series)} predicted)")
        
        # ê° ìœˆë„ìš° í¬ê¸°ë³„ ì´ë™í‰ê·  ê³„ì‚°
        for window in windows:
            # ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ì´ë™í‰ê·  ê³„ì‚°
            rolling_avg = combined_series.rolling(window=window, min_periods=1).mean()
            
            # ì˜ˆì¸¡ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ë§Œ ì¶”ì¶œ
            window_results = []
            
            for i, date in enumerate(pred_df['Date']):
                # í•´ë‹¹ ë‚ ì§œì˜ ì˜ˆì¸¡ ë° ì‹¤ì œê°’
                pred_value = pred_df['Prediction'].iloc[i]
                actual_value = pred_df['Actual'].iloc[i] if 'Actual' in pred_df.columns else None
                
                # í•´ë‹¹ ë‚ ì§œì˜ ì´ë™í‰ê·  ê°’
                ma_value = rolling_avg.loc[date] if date in rolling_avg.index else None
                
                window_results.append({
                    'date': date,
                    'prediction': pred_value,
                    'actual': actual_value,
                    'ma': ma_value
                })
            
            results[f'ma{window}'] = window_results
            logger.info(f"MA{window} calculated: {len(window_results)} data points")
        
        logger.info(f"Moving average calculation completed with {len(results)} windows")
        return results
        
    except Exception as e:
        logger.error(f"Error calculating moving averages with history: {str(e)}")
        logger.error(traceback.format_exc())
        return {}

# 2. ì—¬ëŸ¬ ë‚ ì§œì— ëŒ€í•œ ëˆ„ì  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€
def run_accumulated_predictions_with_save(file_path, start_date, end_date=None, save_to_csv=True, use_saved_data=True):
    """
    ì‹œì‘ ë‚ ì§œë¶€í„° ì¢…ë£Œ ë‚ ì§œê¹Œì§€ ê° ë‚ ì§œë³„ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ëˆ„ì í•©ë‹ˆë‹¤. (ìˆ˜ì •ë¨)
    """
    global prediction_state

    try:
        # ìƒíƒœ ì´ˆê¸°í™”
        prediction_state['is_predicting'] = True
        prediction_state['prediction_progress'] = 5
        prediction_state['error'] = None
        prediction_state['accumulated_predictions'] = []
        prediction_state['accumulated_metrics'] = {}
        prediction_state['prediction_dates'] = []
        prediction_state['accumulated_consistency_scores'] = {}
        prediction_state['current_file'] = file_path  # âœ… í˜„ì¬ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        
        logger.info(f"Running accumulated predictions from {start_date} to {end_date}")

        # ì…ë ¥ ë‚ ì§œë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
        if isinstance(start_date, str):
            start_date = pd.to_datetime(start_date)
        if end_date is not None and isinstance(end_date, str):
            end_date = pd.to_datetime(end_date)

        # ì €ì¥ëœ ë°ì´í„° í™œìš© ì˜µì…˜ì´ ì¼œì ¸ ìˆìœ¼ë©´ ë¨¼ì € CSVì—ì„œ ë¡œë“œ ì‹œë„
        loaded_predictions = []
        if use_saved_data:
            logger.info("ğŸ” [CACHE] Attempting to load existing predictions from CSV files...")
            
            # ğŸ”§ ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ íŒŒì¼ë“¤ë¡œë¶€í„° ì¬ìƒì„±
            cache_dirs = get_file_cache_dirs(file_path)
            predictions_index_file = cache_dirs['predictions'] / 'predictions_index.csv'
            
            if not predictions_index_file.exists():
                logger.warning("âš ï¸ [CACHE] predictions_index.csv not found, attempting to rebuild from existing files...")
                if rebuild_predictions_index_from_existing_files():
                    logger.info("âœ… [CACHE] Successfully rebuilt predictions index")
                else:
                    logger.warning("âš ï¸ [CACHE] Failed to rebuild predictions index")
            
            loaded_predictions = load_accumulated_predictions_from_csv(start_date, end_date, file_path=file_path)  # âœ… íŒŒì¼ ê²½ë¡œ ì¶”ê°€
            logger.info(f"ğŸ“¦ [CACHE] Successfully loaded {len(loaded_predictions)} predictions from CSV cache")
            if len(loaded_predictions) > 0:
                logger.info(f"ğŸ’¡ [CACHE] Using cached predictions will significantly speed up processing!")

        # ë°ì´í„° ë¡œë“œ
        df = load_data(file_path)
        prediction_state['current_data'] = df
        prediction_state['prediction_progress'] = 10

        # ì¢…ë£Œ ë‚ ì§œê°€ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ ì‚¬ìš©
        if end_date is None:
            end_date = df.index.max()

        # ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ì¶”ì¶œ í›„ ì •ë ¬
        available_dates = [date for date in df.index if start_date <= date <= end_date]
        available_dates.sort()
        
        if not available_dates:
            raise ValueError(f"ì§€ì •ëœ ê¸°ê°„ ë‚´ì— ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤: {start_date} ~ {end_date}")

        total_dates = len(available_dates)
        logger.info(f"Accumulated prediction: {total_dates} dates from {start_date} to {end_date}")

        # ëˆ„ì  ì„±ëŠ¥ ì§€í‘œ ì´ˆê¸°í™”
        accumulated_metrics = {
            'f1': 0.0,
            'accuracy': 0.0,
            'mape': 0.0,
            'weighted_score': 0.0,
            'total_predictions': 0
        }

        # ì´ë¯¸ ë¡œë“œëœ ì˜ˆì¸¡ ê²°ê³¼ë“¤ì„ ë‚ ì§œë³„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        loaded_by_date = {}
        for pred in loaded_predictions:
            loaded_by_date[pred['date']] = pred

        # âœ… ìºì‹œ í™œìš© í†µê³„ ì´ˆê¸°í™”
        cache_statistics = {
            'total_dates': 0,
            'cached_dates': 0,
            'new_predictions': 0,
            'cache_hit_rate': 0.0
        }

        all_predictions = []
        accumulated_interval_scores = {}

        # ê° ë‚ ì§œë³„ ì˜ˆì¸¡ ìˆ˜í–‰ ë˜ëŠ” ë¡œë“œ
        for i, current_date in enumerate(available_dates):
            current_date_str = format_date(current_date)
            cache_statistics['total_dates'] += 1
            
            logger.info(f"Processing date {i+1}/{total_dates}: {current_date_str}")
            
            # ì´ë¯¸ ë¡œë“œëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if current_date_str in loaded_by_date:
                cache_statistics['cached_dates'] += 1  # âœ… ìºì‹œ ì‚¬ìš© ì‹œ ì¹´ìš´í„° ì¦ê°€
                logger.info(f"âš¡ [CACHE] Using cached prediction for {current_date_str} (skipping computation)")
                date_result = loaded_by_date[current_date_str]
                
                # ğŸ”§ ìºì‹œëœ metrics ì•ˆì „ì„± ì²˜ë¦¬
                metrics = date_result.get('metrics')
                if not metrics or not isinstance(metrics, dict):
                    logger.warning(f"âš ï¸ [CACHE] Invalid metrics for {current_date_str}, using defaults")
                    metrics = {'f1': 0.0, 'accuracy': 0.0, 'mape': 0.0, 'weighted_score': 0.0}
                
                # ëˆ„ì  ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
                accumulated_metrics['f1'] += metrics.get('f1', 0.0)
                accumulated_metrics['accuracy'] += metrics.get('accuracy', 0.0)
                accumulated_metrics['mape'] += metrics.get('mape', 0.0)
                accumulated_metrics['weighted_score'] += metrics.get('weighted_score', 0.0)
                accumulated_metrics['total_predictions'] += 1
                
            else:
                # ìƒˆë¡œìš´ ì˜ˆì¸¡ ìˆ˜í–‰
                cache_statistics['new_predictions'] += 1
                logger.info(f"ğŸš€ [COMPUTE] Running new prediction for {current_date_str} (not in cache)")
                try:
                    # âœ… ëˆ„ì  ì˜ˆì¸¡ì—ì„œë„ ëª¨ë“  ìƒˆ ì˜ˆì¸¡ì„ ì €ì¥í•˜ë„ë¡ ë³´ì¥
                    results = generate_predictions_with_save(df, current_date, save_to_csv=True, file_path=file_path)
                    
                    # ì˜ˆì¸¡ ë°ì´í„° íƒ€ì… ì•ˆì „ í™•ì¸
                    predictions = results.get('predictions_flat', results.get('predictions', []))
                    
                    # ì˜ˆì¸¡ ë°ì´í„°ê°€ ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ì¸ ê²½ìš° ì²˜ë¦¬
                    if isinstance(predictions, dict):
                        if 'future' in predictions:
                            predictions = predictions['future']
                        elif 'predictions' in predictions:
                            predictions = predictions['predictions']
                    
                    if not predictions or not isinstance(predictions, list):
                        logger.warning(f"No valid predictions found for {current_date_str}: {type(predictions)}")
                        continue
                        
                    # ì‹¤ì œ ì˜ˆì¸¡í•œ ì˜ì—…ì¼ ìˆ˜ ê³„ì‚° (ì•ˆì „í•œ ë°©ì‹)
                    actual_business_days = 0
                    try:
                        for p in predictions:
                            # pê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                            if isinstance(p, dict):
                                date_key = p.get('Date') or p.get('date')
                                is_synthetic = p.get('is_synthetic', False)
                                if date_key and not is_synthetic:
                                    actual_business_days += 1
                            else:
                                logger.warning(f"Prediction item is not dict for {current_date_str}: {type(p)}")
                    except Exception as calc_error:
                        logger.error(f"Error calculating business days: {str(calc_error)}")
                        actual_business_days = len(predictions)  # ê¸°ë³¸ê°’
                    
                    metrics = results.get('metrics', {})
                    if not metrics:
                        # ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
                        metrics = {'f1': 0.0, 'accuracy': 0.0, 'mape': 0.0, 'weighted_score': 0.0}
                    
                    # ëˆ„ì  ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
                    accumulated_metrics['f1'] += metrics.get('f1', 0.0)
                    accumulated_metrics['accuracy'] += metrics.get('accuracy', 0.0)
                    accumulated_metrics['mape'] += metrics.get('mape', 0.0)
                    accumulated_metrics['weighted_score'] += metrics.get('weighted_score', 0.0)
                    accumulated_metrics['total_predictions'] += 1

                    # ì•ˆì „í•œ ë°ì´í„° êµ¬ì¡° ìƒì„±
                    safe_predictions = predictions if isinstance(predictions, list) else []
                    safe_interval_scores = results.get('interval_scores', {})
                    if not isinstance(safe_interval_scores, dict):
                        safe_interval_scores = {}
                    
                    date_result = {
                        'date': current_date_str,
                        'predictions': safe_predictions,
                        'metrics': metrics,
                        'interval_scores': safe_interval_scores,
                        'actual_business_days': actual_business_days,
                        'next_semimonthly_period': results.get('next_semimonthly_period'),
                        'original_interval_scores': safe_interval_scores,
                        'ma_results': results.get('ma_results', {}),  # ğŸ”‘ ì´ë™í‰ê·  ë°ì´í„° ì¶”ê°€
                        'attention_data': results.get('attention_data', {})  # ğŸ”‘ Attention ë°ì´í„° ì¶”ê°€
                    }
                    
                except Exception as e:
                    logger.error(f"Error in prediction for date {current_date}: {str(e)}")
                    logger.error(traceback.format_exc())
                    continue

            # êµ¬ê°„ ì ìˆ˜ ëˆ„ì  ì²˜ë¦¬ (ì•ˆì „í•œ ë°©ì‹)
            interval_scores = date_result.get('interval_scores', {})
            if isinstance(interval_scores, dict):
                for interval in interval_scores.values():
                    if not interval or not isinstance(interval, dict) or 'days' not in interval or interval['days'] is None:
                        continue
                    interval_key = f"{format_date(interval['start_date'])}-{format_date(interval['end_date'])}"
                    if interval_key in accumulated_interval_scores:
                        accumulated_interval_scores[interval_key]['score'] += interval['score']
                        accumulated_interval_scores[interval_key]['count'] += 1
                        accumulated_interval_scores[interval_key]['avg_price'] = (
                            (accumulated_interval_scores[interval_key]['avg_price'] *
                             (accumulated_interval_scores[interval_key]['count'] - 1) +
                             interval['avg_price']) / accumulated_interval_scores[interval_key]['count']
                        )
                    else:
                        accumulated_interval_scores[interval_key] = interval.copy()
                        accumulated_interval_scores[interval_key]['count'] = 1

            all_predictions.append(date_result)
            prediction_state['prediction_progress'] = 10 + int(90 * (i + 1) / total_dates)

        # í‰ê·  ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        if accumulated_metrics['total_predictions'] > 0:
            count = accumulated_metrics['total_predictions']
            accumulated_metrics['f1'] /= count
            accumulated_metrics['accuracy'] /= count
            accumulated_metrics['mape'] /= count
            accumulated_metrics['weighted_score'] /= count

        # ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°
        logger.info("Calculating prediction consistency scores...")
        unique_periods = set()
        for pred in all_predictions:
            if 'next_semimonthly_period' in pred and pred['next_semimonthly_period']:
                unique_periods.add(pred['next_semimonthly_period'])
        
        accumulated_consistency_scores = {}
        for period in unique_periods:
            try:
                consistency_data = calculate_prediction_consistency(all_predictions, period)
                accumulated_consistency_scores[period] = consistency_data
                logger.info(f"Consistency score for {period}: {consistency_data.get('consistency_score', 'N/A')}")
            except Exception as e:
                logger.error(f"Error calculating consistency for period {period}: {str(e)}")

        # accumulated_interval_scores ì²˜ë¦¬
        accumulated_scores_list = [
            v for v in accumulated_interval_scores.values()
            if v is not None and isinstance(v, dict) and 'days' in v and v['days'] is not None
        ]
        accumulated_scores_list.sort(key=lambda x: x['score'], reverse=True)

        accumulated_purchase_reliability, debug_info = calculate_accumulated_purchase_reliability_with_debug(all_predictions)
        
        # âœ… ìºì‹œ í™œìš©ë¥  ê³„ì‚°
        cache_statistics['cache_hit_rate'] = (cache_statistics['cached_dates'] / cache_statistics['total_dates'] * 100) if cache_statistics['total_dates'] > 0 else 0.0
        logger.info(f"ğŸ¯ [CACHE] Final statistics: {cache_statistics['cached_dates']}/{cache_statistics['total_dates']} cached ({cache_statistics['cache_hit_rate']:.1f}%), {cache_statistics['new_predictions']} new predictions computed")
        
        # ê²°ê³¼ ì €ì¥
        prediction_state['accumulated_predictions'] = all_predictions
        prediction_state['accumulated_metrics'] = accumulated_metrics
        prediction_state['prediction_dates'] = [p['date'] for p in all_predictions]
        prediction_state['accumulated_interval_scores'] = accumulated_scores_list
        prediction_state['accumulated_consistency_scores'] = accumulated_consistency_scores
        prediction_state['accumulated_purchase_reliability'] = accumulated_purchase_reliability
        prediction_state['accumulated_purchase_debug'] = debug_info
        prediction_state['cache_statistics'] = cache_statistics  # âœ… ìºì‹œ í†µê³„ ì¶”ê°€

        if all_predictions:
            latest = all_predictions[-1]
            prediction_state['latest_predictions'] = latest['predictions']
            prediction_state['current_date'] = latest['date']

        prediction_state['is_predicting'] = False
        prediction_state['prediction_progress'] = 100
        logger.info(f"Accumulated prediction completed for {len(all_predictions)} dates")
        
    except Exception as e:
        logger.error(f"Error in accumulated prediction: {str(e)}")
        logger.error(traceback.format_exc())
        prediction_state['error'] = str(e)
        prediction_state['is_predicting'] = False
        prediction_state['prediction_progress'] = 0
        prediction_state['accumulated_consistency_scores'] = {}

# 3. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëˆ„ì  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
def background_accumulated_prediction(file_path, start_date, end_date=None):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëˆ„ì  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
    thread = Thread(target=run_accumulated_predictions_with_save, args=(file_path, start_date, end_date))
    thread.daemon = True
    thread.start()
    return thread

# 6. ëˆ„ì  ê²°ê³¼ ë³´ê³ ì„œ ìƒì„± í•¨ìˆ˜
def generate_accumulated_report():
    """ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
    global prediction_state
    
    if not prediction_state['accumulated_predictions']:
        return None
    
    try:
        metrics = prediction_state['accumulated_metrics']
        all_preds = prediction_state['accumulated_predictions']
        
        # ë³´ê³ ì„œ íŒŒì¼ ì´ë¦„ ìƒì„± - íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        start_date = all_preds[0]['date']
        end_date = all_preds[-1]['date']
        try:
            cache_dirs = get_file_cache_dirs()
            report_dir = cache_dirs['predictions']
            report_filename = os.path.join(report_dir, f"accumulated_report_{start_date}_to_{end_date}.txt")
        except Exception as e:
            logger.warning(f"Could not get cache directories for accumulated report: {str(e)}")
            report_filename = f"accumulated_report_{start_date}_to_{end_date}.txt"
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(f"===== Accumulated Prediction Report =====\n")
            f.write(f"Period: {start_date} to {end_date}\n")
            f.write(f"Total Predictions: {metrics['total_predictions']}\n\n")
            
            # ëˆ„ì  ì„±ëŠ¥ ì§€í‘œ
            f.write("Average Performance Metrics:\n")
            f.write(f"- F1 Score: {metrics['f1']:.4f}\n")
            f.write(f"- Direction Accuracy: {metrics['accuracy']:.2f}%\n")
            f.write(f"- MAPE: {metrics['mape']:.2f}%\n")
            f.write(f"- Weighted Score: {metrics['weighted_score']:.2f}%\n\n")
            
            # ë‚ ì§œë³„ ìƒì„¸ ì •ë³´
            f.write("Performance By Date:\n")
            for pred in all_preds:
                date = pred['date']
                m = pred['metrics']
                f.write(f"\n* {date}:\n")
                f.write(f"  - F1 Score: {m['f1']:.4f}\n")
                f.write(f"  - Accuracy: {m['accuracy']:.2f}%\n")
                f.write(f"  - MAPE: {m['mape']:.2f}%\n")
                f.write(f"  - Weighted Score: {m['weighted_score']:.2f}%\n")
                
                # êµ¬ë§¤ êµ¬ê°„ ì •ë³´
                if pred['interval_scores']:
                    best_interval = decide_purchase_interval(pred['interval_scores'])
                    f.write("Best Purchase Interval:\n")
                    f.write(f"- Start Date: {best_interval['start_date']}\n")
                    f.write(f"- End Date: {best_interval['end_date']}\n")
                    f.write(f"- Duration: {best_interval['days']} days\n")
                    f.write(f"- Average Price: {best_interval['avg_price']:.2f}\n")
                    f.write(f"- Score: {best_interval['score']}\n")
                    f.write(f"- Selection Reason: {best_interval.get('selection_reason', '')}\n\n")
        
        return report_filename
    
    except Exception as e:
        logger.error(f"Error generating accumulated report: {str(e)}")
        return None

# 9. ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™” í•¨ìˆ˜
def visualize_accumulated_metrics():
    """ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
    global prediction_state
    
    if not prediction_state['accumulated_predictions']:
        return None, None
    
    try:
        # ë°ì´í„° ì¤€ë¹„
        dates = []
        f1_scores = []
        accuracies = []
        mapes = []
        weighted_scores = []
        
        for pred in prediction_state['accumulated_predictions']:
            dates.append(pred['date'])
            m = pred['metrics']
            f1_scores.append(m['f1'])
            accuracies.append(m['accuracy'])
            mapes.append(m['mape'])
            weighted_scores.append(m['weighted_score'])
        
        # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
        dates = [pd.to_datetime(d) for d in dates]
        
        # ê·¸ë˜í”„ ìƒì„±
        fig, axs = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Accumulated Prediction Metrics', fontsize=16)
        
        # F1 Score
        axs[0, 0].plot(dates, f1_scores, marker='o', color='blue')
        axs[0, 0].set_title('F1 Score')
        axs[0, 0].set_ylim(0, 1)
        axs[0, 0].grid(True)
        plt.setp(axs[0, 0].xaxis.get_majorticklabels(), rotation=45)
        
        # Accuracy
        axs[0, 1].plot(dates, accuracies, marker='o', color='green')
        axs[0, 1].set_title('Direction Accuracy (%)')
        axs[0, 1].set_ylim(0, 100)
        axs[0, 1].grid(True)
        plt.setp(axs[0, 1].xaxis.get_majorticklabels(), rotation=45)
        
        # MAPE
        axs[1, 0].plot(dates, mapes, marker='o', color='red')
        axs[1, 0].set_title('MAPE (%)')
        axs[1, 0].set_ylim(0, max(mapes) * 1.2)
        axs[1, 0].grid(True)
        plt.setp(axs[1, 0].xaxis.get_majorticklabels(), rotation=45)
        
        # Weighted Score
        axs[1, 1].plot(dates, weighted_scores, marker='o', color='purple')
        axs[1, 1].set_title('Weighted Score (%)')
        axs[1, 1].set_ylim(0, 100)
        axs[1, 1].grid(True)
        plt.setp(axs[1, 1].xaxis.get_majorticklabels(), rotation=45)
        
        plt.tight_layout()
        
        # ì´ë¯¸ì§€ ì €ì¥
        img_buf = io.BytesIO()
        plt.savefig(img_buf, format='png', dpi=300)
        img_buf.seek(0)
        
        # Base64ë¡œ ì¸ì½”ë”©
        img_str = base64.b64encode(img_buf.read()).decode('utf-8')
        
        # íŒŒì¼ë¡œ ì €ì¥ - íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        try:
            cache_dirs = get_file_cache_dirs()
            plots_dir = cache_dirs['plots']
            filename = os.path.join(plots_dir, 'accumulated_metrics.png')
        except Exception as e:
            logger.warning(f"Could not get cache directories for accumulated metrics: {str(e)}")
            filename = 'accumulated_metrics.png'
        
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        return filename, img_str
        
    except Exception as e:
        logger.error(f"Error visualizing accumulated metrics: {str(e)}")
        return None, None

#######################################################################
# ì˜ˆì¸¡ ë° ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
#######################################################################

def prepare_data(train_data, val_data, sequence_length, predict_window, target_col_idx, augment=False):
    """í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ì¤€ë¹„"""
    X_train, y_train, prev_train = [], [], []
    for i in range(len(train_data) - sequence_length - predict_window + 1):
        seq = train_data[i:i+sequence_length]
        target = train_data[i+sequence_length:i+sequence_length+predict_window, target_col_idx]
        prev_value = train_data[i+sequence_length-1, target_col_idx]
        X_train.append(seq)
        y_train.append(target)
        prev_train.append(prev_value)
        if augment:
            # ê°„ë‹¨í•œ ë°ì´í„° ì¦ê°•
            noise = np.random.normal(0, 0.001, seq.shape)
            aug_seq = seq + noise
            X_train.append(aug_seq)
            y_train.append(target)
            prev_train.append(prev_value)
    
    X_val, y_val, prev_val = [], [], []
    for i in range(len(val_data) - sequence_length - predict_window + 1):
        X_val.append(val_data[i:i+sequence_length])
        y_val.append(val_data[i+sequence_length:i+sequence_length+predict_window, target_col_idx])
        prev_val.append(val_data[i+sequence_length-1, target_col_idx])
    
    return map(np.array, [X_train, y_train, prev_train, X_val, y_val, prev_val])

def train_model(features, target_col, current_date, historical_data, device, params):
    """LSTM ëª¨ë¸ í•™ìŠµ"""
    try:
        # íŠ¹ì„± ì´ë¦„ í™•ì¸
        if target_col not in features:
            features.append(target_col)
        
        # í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (í˜„ì¬ ë‚ ì§œê¹Œì§€)
        train_df = historical_data[features].copy()
        target_col_idx = train_df.columns.get_loc(target_col)
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        train_data = scaler.fit_transform(train_df)
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        sequence_length = params.get('sequence_length', 20)
        hidden_size = params.get('hidden_size', 128)
        num_layers = params.get('num_layers', 2)
        dropout = params.get('dropout', 0.2)
        learning_rate = params.get('learning_rate', 0.001)
        num_epochs = params.get('num_epochs', 100)
        batch_size = params.get('batch_size', 32)
        alpha = params.get('loss_alpha', 0.6)  # MSE ê°€ì¤‘ì¹˜
        beta = params.get('loss_beta', 0.2)    # Volatility ê°€ì¤‘ì¹˜
        gamma = params.get('loss_gamma', 0.15)  # ë°©í–¥ì„± ê°€ì¤‘ì¹˜
        delta = params.get('loss_delta', 0.05)  # ì—°ì†ì„± ê°€ì¤‘ì¹˜
        patience = params.get('patience', 20)   # ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´
        predict_window = params.get('predict_window', 23)  # ì˜ˆì¸¡ ê¸°ê°„
        
        # 80/20 ë¶„í•  (ì—°ëŒ€ìˆœ)
        train_size = int(len(train_data) * 0.8)
        train_set = train_data[:train_size]
        val_set = train_data[train_size:]
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
        X_train, y_train, prev_train, X_val, y_val, prev_val = prepare_data(
            train_set, val_set, sequence_length, predict_window, target_col_idx
        )
        
        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if len(X_train) < batch_size:
            batch_size = max(1, len(X_train) // 2)
            logger.warning(f"Batch size reduced to {batch_size} due to limited data")
        
        if len(X_train) == 0 or len(X_val) == 0:
            raise ValueError("Insufficient data for training")
        
        # ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±
        train_dataset = TimeSeriesDataset(X_train, y_train, device, prev_train)
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            worker_init_fn=seed_worker,
            generator=g
        )
        
        val_dataset = TimeSeriesDataset(X_val, y_val, device, prev_val)
        val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)
        
        # ëª¨ë¸ ìƒì„±
        model = ImprovedLSTMPredictor(
            input_size=train_data.shape[1],
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            output_size=predict_window
        ).to(device)
        
        # ìµœì í™”ê¸° ë° ì†ì‹¤ í•¨ìˆ˜
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        
        # í•™ìŠµ
        best_val_loss = float('inf')
        best_model_state = None
        patience_counter = 0
        
        for epoch in range(num_epochs):
            # í•™ìŠµ ëª¨ë“œ
            model.train()
            train_loss = 0
            
            for X_batch, y_batch, prev_batch in train_loader:
                optimizer.zero_grad()
                y_pred = model(X_batch, prev_batch)
                loss, _ = composite_loss(
                    y_pred, y_batch, prev_batch,
                    alpha=alpha, beta=beta, gamma=gamma, delta=delta
                )
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                train_loss += loss.item()
            
            # ê²€ì¦ ëª¨ë“œ
            model.eval()
            val_loss = 0
            
            with torch.no_grad():
                for X_batch, y_batch, prev_batch in val_loader:
                    y_pred = model(X_batch, prev_batch)
                    loss, _ = composite_loss(
                        y_pred, y_batch, prev_batch,
                        alpha=alpha, beta=beta, gamma=gamma, delta=delta
                    )
                    val_loss += loss.item()
                
                val_loss /= len(val_loader)
                
                # ëª¨ë¸ ì €ì¥ (ìµœì )
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model_state = model.state_dict().copy()
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                # ì¡°ê¸° ì¢…ë£Œ
                if patience_counter >= patience:
                    logger.info(f"Early stopping at epoch {epoch+1}")
                    break
        
        # ìµœì  ëª¨ë¸ ë³µì›
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        logger.info(f"Model training completed with best validation loss: {best_val_loss:.4f}")
        
        # ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, íŒŒë¼ë¯¸í„° ë°˜í™˜
        return model, scaler, target_col_idx
    
    except Exception as e:
        logger.error(f"Error in model training: {str(e)}")
        logger.error(traceback.format_exc())
        raise e
    
# generate_predictions í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
# 'sequence_df' ë³€ìˆ˜ ì •ì˜ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

# generate_predictions í•¨ìˆ˜ ìˆ˜ì • (ê°„ë‹¨í•˜ê³  ì •í™•í•œ ë²„ì „)

def generate_predictions(df, current_date, predict_window=23, features=None, target_col='MOPJ', file_path=None):
    """
    ê°œì„ ëœ ì˜ˆì¸¡ ìˆ˜í–‰ í•¨ìˆ˜ - ì˜ˆì¸¡ ì‹œì‘ì¼ì˜ ë°˜ì›” ê¸°ê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
    ğŸ”‘ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€: current_date ì´í›„ì˜ ì‹¤ì œê°’ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    """
    try:
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Using device: {device}")
        
        # í˜„ì¬ ë‚ ì§œê°€ ë¬¸ìì—´ì´ë©´ datetimeìœ¼ë¡œ ë³€í™˜
        if isinstance(current_date, str):
            current_date = pd.to_datetime(current_date)
        
        # í˜„ì¬ ë‚ ì§œ ê²€ì¦ (ë°ì´í„° ê¸°ì¤€ì¼)
        if current_date not in df.index:
            closest_date = df.index[df.index <= current_date][-1]
            logger.warning(f"Current date {current_date} not found in dataframe. Using closest date: {closest_date}")
            current_date = closest_date
        
        # ì˜ˆì¸¡ ì‹œì‘ì¼ ê³„ì‚°
        prediction_start_date = current_date + pd.Timedelta(days=1)
        while prediction_start_date.weekday() >= 5 or is_holiday(prediction_start_date):
            prediction_start_date += pd.Timedelta(days=1)
        
        # ë°˜ì›” ê¸°ê°„ ê³„ì‚°
        data_semimonthly_period = get_semimonthly_period(current_date)
        prediction_semimonthly_period = get_semimonthly_period(prediction_start_date)
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ì˜ˆì¸¡ ì‹œì‘ì¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë°˜ì›” ê³„ì‚°
        next_semimonthly_period = get_next_semimonthly_period(prediction_start_date)
        
        logger.info(f"ğŸ¯ Prediction Setup:")
        logger.info(f"  ğŸ“… Data base date: {current_date} (period: {data_semimonthly_period})")
        logger.info(f"  ğŸš€ Prediction start date: {prediction_start_date} (period: {prediction_semimonthly_period})")
        logger.info(f"  ğŸ¯ Purchase interval target period: {next_semimonthly_period}")
        
        # 23ì¼ì¹˜ ì˜ˆì¸¡ì„ ìœ„í•œ ë‚ ì§œ ìƒì„±
        all_business_days = get_next_n_business_days(current_date, df, predict_window)
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ì˜ˆì¸¡ ì‹œì‘ì¼ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë§¤ êµ¬ê°„ ê³„ì‚°
        semimonthly_business_days, purchase_target_period = get_next_semimonthly_dates(prediction_start_date, df)
        
        logger.info(f"  ğŸ“Š Total predictions: {len(all_business_days)} days")
        logger.info(f"  ğŸ›’ Purchase target period: {purchase_target_period}")
        logger.info(f"  ğŸ“ˆ Purchase interval business days: {len(semimonthly_business_days)}")
        
        if not all_business_days:
            raise ValueError(f"No future business days found after {current_date}")

        # âœ… í•µì‹¬ ìˆ˜ì •: ë‚ ì§œë³„ë¡œ ë‹¤ë¥¸ í•™ìŠµ ë°ì´í„° ì‚¬ìš© ë³´ì¥
        historical_data = df[df.index <= current_date].copy()
        
        logger.info(f"  ğŸ“Š Training data: {len(historical_data)} records up to {format_date(current_date)}")
        logger.info(f"  ğŸ“Š Training data range: {format_date(historical_data.index.min())} ~ {format_date(historical_data.index.max())}")
        
        # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ í™•ì¸
        if len(historical_data) < 50:
            raise ValueError(f"Insufficient training data: {len(historical_data)} records (minimum 50 required)")
        
        if features is None:
            selected_features, _ = select_features_from_groups(
                historical_data, 
                variable_groups,
                target_col=target_col,
                vif_threshold=50.0,
                corr_threshold=0.8
            )
        else:
            selected_features = features
            
        if target_col not in selected_features:
            selected_features.append(target_col)
        
        logger.info(f"  ğŸ”§ Selected features ({len(selected_features)}): {selected_features}")
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ë‚ ì§œë³„ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ë§ ë³´ì¥
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(historical_data[selected_features])
        target_col_idx = selected_features.index(target_col)
        
        logger.info(f"  âš–ï¸  Scaler fitted on data up to {format_date(current_date)}")
        logger.info(f"  ğŸ“Š Scaled data shape: {scaled_data.shape}")
        
        # âœ… í•µì‹¬: ì˜ˆì¸¡ ì‹œì‘ì¼ì˜ ë°˜ì›” ê¸°ê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
        optimized_params = optimize_hyperparameters_semimonthly_kfold(
            train_data=scaled_data,
            input_size=len(selected_features),
            target_col_idx=target_col_idx,
            device=device,
            current_period=prediction_semimonthly_period,  # âœ… ì˜ˆì¸¡ ì‹œì‘ì¼ì˜ ë°˜ì›” ê¸°ê°„
            file_path=file_path,  # ğŸ”‘ íŒŒì¼ ê²½ë¡œ ì „ë‹¬
            n_trials=30,
            k_folds=5,
            use_cache=True
        )
        
        logger.info(f"âœ… Using hyperparameters for prediction start period: {prediction_semimonthly_period}")
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ëª¨ë¸ í•™ìŠµ ì‹œ í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë¶„í•  ë³´ì¥
        logger.info(f"  ğŸš€ Training model with data up to {format_date(current_date)}")
        model, model_scaler, model_target_col_idx = train_model(
            selected_features,
            target_col,
            current_date,
            historical_data,
            device,
            optimized_params
        )
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì¼ê´€ì„± í™•ì¸
        if model_target_col_idx != target_col_idx:
            logger.warning(f"Target column index mismatch: {model_target_col_idx} vs {target_col_idx}")
            target_col_idx = model_target_col_idx
        
        logger.info(f"  âœ… Model trained successfully for prediction starting {format_date(prediction_start_date)}")
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„ ì‹œ ë‚ ì§œë³„ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ë³´ì¥
        seq_len = optimized_params['sequence_length']
        current_idx = df.index.get_loc(current_date)
        start_idx = max(0, current_idx - seq_len + 1)
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ì¶”ì¶œ (current_dateê¹Œì§€ë§Œ!)
        sequence = df.iloc[start_idx:current_idx+1][selected_features].values
        
        logger.info(f"  ğŸ“Š Sequence data: {sequence.shape} from {format_date(df.index[start_idx])} to {format_date(current_date)}")
        
        # ëª¨ë¸ì—ì„œ ë°˜í™˜ëœ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš© (ì¼ê´€ì„± ë³´ì¥)
        sequence = model_scaler.transform(sequence)
        prev_value = sequence[-1, target_col_idx]
        
        logger.info(f"  ğŸ“ˆ Previous value (scaled): {prev_value:.4f}")
        logger.info(f"  ğŸ“Š Sequence length used: {len(sequence)} (required: {seq_len})")
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        future_predictions = []  # ë¯¸ë˜ ì˜ˆì¸¡ (ì‹¤ì œê°’ ì—†ìŒ)
        validation_data = []     # ê²€ì¦ ë°ì´í„° (ì‹¤ì œê°’ ìˆìŒ)
        
        with torch.no_grad():
            # 23ì˜ì—…ì¼ ì „ì²´ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
            max_pred_days = min(predict_window, len(all_business_days))
            current_sequence = sequence.copy()
            
            # í…ì„œë¡œ ë³€í™˜
            X = torch.FloatTensor(current_sequence).unsqueeze(0).to(device)
            prev_tensor = torch.FloatTensor([prev_value]).to(device)
            
            # ì „ì²´ ì‹œí€€ìŠ¤ ì˜ˆì¸¡
            pred = model(X, prev_tensor).cpu().numpy()[0]
            
            # âœ… í•µì‹¬ ìˆ˜ì •: ê° ë‚ ì§œë³„ ì˜ˆì¸¡ ìƒì„± (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)
            for j, pred_date in enumerate(all_business_days[:max_pred_days]):
                # âœ… ìŠ¤ì¼€ì¼ ì—­ë³€í™˜ ì‹œ ì¼ê´€ëœ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©
                dummy_matrix = np.zeros((1, len(selected_features)))
                dummy_matrix[0, target_col_idx] = pred[j]
                pred_value = model_scaler.inverse_transform(dummy_matrix)[0, target_col_idx]
                
                # ì˜ˆì¸¡ê°’ ê²€ì¦ ë° ì •ë¦¬
                if np.isnan(pred_value) or np.isinf(pred_value):
                    logger.warning(f"Invalid prediction value for {pred_date}: {pred_value}, skipping")
                    continue
                
                pred_value = float(pred_value)
                
                # ê¸°ë³¸ ì˜ˆì¸¡ ì •ë³´
                prediction_item = {
                    'date': format_date(pred_date, '%Y-%m-%d'),
                    'prediction': pred_value,
                    'prediction_from': format_date(current_date, '%Y-%m-%d'),
                    'day_offset': j + 1,
                    'is_business_day': pred_date.weekday() < 5 and not is_holiday(pred_date),
                    'is_synthetic': pred_date not in df.index,
                    'semimonthly_period': data_semimonthly_period,
                    'next_semimonthly_period': next_semimonthly_period
                }
                
                # âœ… ì‹¤ì œ ë°ì´í„° ë§ˆì§€ë§‰ ë‚ ì§œ í™•ì¸ (ê²€ì¦ìš©)
                last_data_date = df.index.max()
                
                # âœ… ê²€ì¦ ì¡°ê±´: ì˜ˆì¸¡ ë‚ ì§œê°€ ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë‚´ì— ìˆê³ , current_date ì´í›„ë¼ë©´ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©
                if (pred_date in df.index and 
                    pd.notna(df.loc[pred_date, target_col]) and 
                    pred_date <= last_data_date):  # ğŸ”‘ ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë‚´ì—ì„œ ê²€ì¦ í—ˆìš©
                    
                    actual_value = float(df.loc[pred_date, target_col])
                    
                    if not (np.isnan(actual_value) or np.isinf(actual_value)):
                        validation_item = {
                            **prediction_item,
                            'actual': actual_value,
                            'error': abs(pred_value - actual_value),
                            'error_pct': abs(pred_value - actual_value) / actual_value * 100 if actual_value != 0 else 0.0
                        }
                        validation_data.append(validation_item)
                        
                        # ğŸ“Š ê²€ì¦ íƒ€ì… êµ¬ë¶„ ë¡œê·¸
                        if pred_date <= current_date:
                            logger.debug(f"  âœ… Training validation: {format_date(pred_date)} - Pred: {pred_value:.2f}, Actual: {actual_value:.2f}")
                        else:
                            logger.debug(f"  ğŸ¯ Test validation: {format_date(pred_date)} - Pred: {pred_value:.2f}, Actual: {actual_value:.2f}")
                elif pred_date > last_data_date:
                    logger.debug(f"  ğŸ”® Future: {format_date(pred_date)} - Pred: {pred_value:.2f} (no actual - beyond data)")
                
                future_predictions.append(prediction_item)
        
        # ğŸ“Š ê²€ì¦ ë°ì´í„° í†µê³„
        training_validation = len([v for v in validation_data if pd.to_datetime(v['date']) <= current_date])
        test_validation = len([v for v in validation_data if pd.to_datetime(v['date']) > current_date])
        
        logger.info(f"ğŸ“Š Prediction Results:")
        logger.info(f"  ğŸ“ˆ Total predictions: {len(future_predictions)}")
        logger.info(f"  âœ… Training validation (â‰¤ {format_date(current_date)}): {training_validation}")
        logger.info(f"  ğŸ¯ Test validation (> {format_date(current_date)}): {test_validation}")
        logger.info(f"  ğŸ“‹ Total validation points: {len(validation_data)}")
        logger.info(f"  ğŸ”® Pure future predictions (> {format_date(df.index.max())}): {len(future_predictions) - len(validation_data)}")
        
        if len(validation_data) == 0:
            logger.info("  â„¹ï¸  Pure future prediction - no validation data available")
        
        # âœ… êµ¬ê°„ í‰ê·  ë° ì ìˆ˜ ê³„ì‚° - ì˜¬ë°”ë¥¸ êµ¬ë§¤ ëŒ€ìƒ ê¸°ê°„ ì‚¬ìš©
        temp_predictions_for_interval = []
        for pred in future_predictions:
            pred_date = pd.to_datetime(pred['date'])
            if pred_date in semimonthly_business_days:  # ì´ì œ ì˜¬ë°”ë¥¸ ë‹¤ìŒ ë°˜ì›” ë‚ ì§œë“¤
                temp_predictions_for_interval.append({
                    'Date': pred_date,
                    'Prediction': pred['prediction']
                })
        
        logger.info(f"  ğŸ›’ Predictions for interval calculation: {len(temp_predictions_for_interval)} (target period: {purchase_target_period})")
        
        interval_averages, interval_scores, analysis_info = calculate_interval_averages_and_scores(
            temp_predictions_for_interval, 
            semimonthly_business_days
        )

        # ìµœì¢… êµ¬ë§¤ êµ¬ê°„ ê²°ì •
        best_interval = decide_purchase_interval(interval_scores)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° (ê²€ì¦ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
        metrics = None
        if validation_data:
            start_day_value = df.loc[current_date, target_col]
            if not (pd.isna(start_day_value) or np.isnan(start_day_value) or np.isinf(start_day_value)):
                try:
                    temp_df_for_metrics = pd.DataFrame([
                        {
                            'Date': pd.to_datetime(item['date']),
                            'Prediction': item['prediction'],
                            'Actual': item['actual']
                        } for item in validation_data
                    ])
                    
                    if not temp_df_for_metrics.empty:
                        metrics = compute_performance_metrics_improved(temp_df_for_metrics, start_day_value)
                        logger.info(f"  ğŸ“Š Computed metrics from {len(validation_data)} validation points")
                    else:
                        logger.info("  âš ï¸  No valid data for metrics computation")
                except Exception as e:
                    logger.error(f"Error computing metrics: {str(e)}")
                    metrics = None
            else:
                logger.warning("Invalid start_day_value for metrics computation")
        else:
            logger.info("  â„¹ï¸  No validation data available - pure future prediction")
        
        # âœ… ì´ë™í‰ê·  ê³„ì‚° ì‹œ ì‹¤ì œê°’ë„ í¬í•¨ (ê²€ì¦ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        temp_predictions_for_ma = []
        for pred in future_predictions:
            pred_date = pd.to_datetime(pred['date'])
            actual_val = None
            
            # ì‹¤ì œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œë©´ ì‹¤ì œê°’ ì„¤ì •
            if (pred_date in df.index and 
                pd.notna(df.loc[pred_date, target_col]) and 
                pred_date <= df.index.max()):
                actual_val = float(df.loc[pred_date, target_col])
            
            temp_predictions_for_ma.append({
                'Date': pred_date,
                'Prediction': pred['prediction'],
                'Actual': actual_val
            })
        
        logger.info(f"  ğŸ“ˆ Calculating moving averages with historical data up to {format_date(current_date)}")
        ma_results = calculate_moving_averages_with_history(
            temp_predictions_for_ma, 
            historical_data,  # ì´ë¯¸ current_dateê¹Œì§€ë¡œ í•„í„°ë§ë¨
            target_col=target_col
        )
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        attention_data = None
        try:
            sequence_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)
            prev_tensor = torch.FloatTensor([float(prev_value)]).to(device)
            
            attention_file, attention_img, feature_importance = visualize_attention_weights(
                model, sequence_tensor, prev_tensor, prediction_start_date, selected_features
            )
            
            attention_data = {
                'image': attention_img,
                'file_path': attention_file,
                'feature_importance': feature_importance
            }
        except Exception as e:
            logger.error(f"Error in attention analysis: {str(e)}")
        
        # ì‹œê°í™” ìƒì„±
        plots = {
            'basic_plot': {'file': None, 'image': None},
            'ma_plot': {'file': None, 'image': None}
        }

        start_day_value = None
        if current_date in df.index and not pd.isna(df.loc[current_date, target_col]):
            start_day_value = df.loc[current_date, target_col]

        # ğŸ“Š ì‹œê°í™”ìš© ë°ì´í„° ì¤€ë¹„ - ì‹¤ì œê°’ í¬í•¨
        temp_df_for_plot_data = []
        for item in future_predictions:
            pred_date = pd.to_datetime(item['date'])
            actual_val = None
            
            # ì‹¤ì œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œë©´ ì‹¤ì œê°’ ì„¤ì •
            if (pred_date in df.index and 
                pd.notna(df.loc[pred_date, target_col]) and 
                pred_date <= df.index.max()):
                actual_val = float(df.loc[pred_date, target_col])
            
            temp_df_for_plot_data.append({
                'Date': pred_date,
                'Prediction': item['prediction'],
                'Actual': actual_val
            })
        
        temp_df_for_plot = pd.DataFrame(temp_df_for_plot_data)

        if metrics:
            f1_score = metrics['f1']
            accuracy = metrics['accuracy']
            mape = metrics['mape']
            weighted_score = metrics['weighted_score']
            visualization_type = "with validation data"
        else:
            f1_score = accuracy = mape = weighted_score = 0.0
            visualization_type = "future prediction only"

        if start_day_value is not None and not temp_df_for_plot.empty:
            try:
                # âœ… current_date ì „ë‹¬ ì¶”ê°€
                basic_plot_file, basic_plot_img = plot_prediction_basic(
                    temp_df_for_plot, 
                    prediction_start_date, 
                    start_day_value,
                    f1_score,
                    accuracy,
                    mape,
                    weighted_score,
                    current_date=current_date,  # ğŸ”‘ ì¶”ê°€
                    save_prefix=None,  # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ìë™ ì‚¬ìš©
                    title_prefix="Future Price Prediction" if not validation_data else "Prediction with Validation",
                    file_path=file_path  # ğŸ”‘ ì¶”ê°€
                )
                
                ma_plot_file, ma_plot_img = plot_moving_average_analysis(
                    ma_results, 
                    prediction_start_date,
                    save_prefix=None,  # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ìë™ ì‚¬ìš©
                    title_prefix="Moving Average Analysis (Future Prediction)" if not validation_data else "Moving Average Analysis",
                    file_path=file_path  # ğŸ”‘ ì¶”ê°€
                )
                
                plots = {
                    'basic_plot': {'file': basic_plot_file, 'image': basic_plot_img},
                    'ma_plot': {'file': ma_plot_file, 'image': ma_plot_img}
                }
                
                logger.info(f"  ğŸ¨ Visualization generated ({visualization_type}) - {len(future_predictions)} predictions")
                
            except Exception as e:
                logger.error(f"Error generating visualization: {str(e)}")
                plots = {
                    'basic_plot': {'file': None, 'image': None},
                    'ma_plot': {'file': None, 'image': None}
                }
        else:
            logger.warning(f"Cannot generate visualization: start_value={start_day_value}, predictions={len(temp_df_for_plot)}")

        # ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± - íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        try:
            cache_dirs = get_file_cache_dirs(file_path)
            report_dir = cache_dirs['predictions']  # predictions ë””ë ‰í† ë¦¬ì— ì €ì¥
            report_filename = os.path.join(report_dir, f"prediction_report_{format_date(prediction_start_date, '%Y%m%d')}.txt")
        except Exception as e:
            logger.warning(f"Could not get cache directories for report: {str(e)}")
            report_filename = f"prediction_report_{format_date(prediction_start_date, '%Y%m%d')}.txt"
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(f"===== Prediction Report for {format_date(prediction_start_date)} =====\n\n")
            
            f.write(f"Data Base Date: {format_date(current_date)}\n")
            f.write(f"Prediction Start Date: {format_date(prediction_start_date)}\n")
            f.write(f"Data Semimonthly Period: {data_semimonthly_period}\n")
            f.write(f"Prediction Semimonthly Period: {prediction_semimonthly_period}\n")
            f.write(f"Hyperparameters Used: {prediction_semimonthly_period}\n")
            f.write(f"Next Semimonthly Period: {next_semimonthly_period}\n")
            f.write(f"Total Predictions: {len(future_predictions)}\n")
            f.write(f"Validation Points: {len(validation_data)}\n\n")
            
            if metrics:
                f.write("Performance Metrics:\n")
                f.write(f"- F1 Score: {metrics['f1']:.4f}\n")
                f.write(f"- Direction Accuracy: {metrics['accuracy']:.2f}%\n")
                f.write(f"- MAPE: {metrics['mape']:.2f}%\n")
                f.write(f"- Weighted Score: {metrics['weighted_score']:.2f}%\n")
                if metrics['cosine_similarity'] is not None:
                    f.write(f"- Cosine Similarity: {metrics['cosine_similarity']:.4f}\n")
                f.write("\n")
            else:
                f.write("Performance Metrics: Not available (pure future prediction)\n\n")
            
            f.write(f"Selected Features ({len(selected_features)}):\n")
            for feature in selected_features:
                f.write(f"- {feature}\n")
            f.write("\n")
            
            if best_interval:
                f.write("Best Purchase Interval:\n")
                f.write(f"- Start Date: {best_interval['start_date']}\n")
                f.write(f"- End Date: {best_interval['end_date']}\n")
                f.write(f"- Duration: {best_interval['days']} days\n")
                f.write(f"- Average Price: {best_interval['avg_price']:.2f}\n")
                f.write(f"- Score: {best_interval['score']}\n")
                f.write(f"- Selection Reason: {best_interval.get('selection_reason', '')}\n\n")
        
        # ì²« ë²ˆì§¸ ì˜ˆì¸¡ ë‚ ì§œë¥¼ ì‹¤ì œ ê¸°ì¤€ ì‹œì ìœ¼ë¡œ ì„¤ì •
        first_prediction_date = all_business_days[0] if all_business_days else prediction_start_date

        # ê²°ê³¼ì— ì˜¬ë°”ë¥¸ êµ¬ë§¤ ëŒ€ìƒ ê¸°ê°„ ì •ë³´ ì¶”ê°€
        return {
            'predictions': {
                'future': future_predictions,
                'validation': validation_data
            },
            
            'predictions_flat': future_predictions,
            
            'summary': {
                'total_predictions': len(future_predictions),
                'validation_points': len(validation_data),
                'prediction_start_date': format_date(prediction_start_date, '%Y-%m-%d'),
                'prediction_end_date': format_date(all_business_days[-1], '%Y-%m-%d') if all_business_days else None,
                'data_end_date': format_date(current_date, '%Y-%m-%d'),
                'is_pure_future_prediction': len(validation_data) == 0,
                'hyperparameter_period_used': prediction_semimonthly_period,
                'purchase_target_period': purchase_target_period  # âœ… ì¶”ê°€
            },
            
            'interval_scores': interval_scores,
            'interval_averages': interval_averages,
            'best_interval': best_interval,
            'ma_results': ma_results,
            'metrics': metrics,
            'selected_features': selected_features,
            'attention_data': attention_data,
            'plots': plots,
            'report_file': report_filename,
            'current_date': format_date(prediction_start_date, '%Y-%m-%d'),
            'data_end_date': format_date(current_date, '%Y-%m-%d'),
            'semimonthly_period': data_semimonthly_period,
            'next_semimonthly_period': purchase_target_period,  # âœ… ìˆ˜ì •: ì˜¬ë°”ë¥¸ êµ¬ë§¤ ëŒ€ìƒ ê¸°ê°„
            'prediction_semimonthly_period': prediction_semimonthly_period,
            'hyperparameter_period_used': prediction_semimonthly_period,
            'purchase_target_period': purchase_target_period  # âœ… ì¶”ê°€
        }
        
    except Exception as e:
        logger.error(f"Error in prediction generation: {str(e)}")
        logger.error(traceback.format_exc())
        raise e

def generate_predictions_compatible(df, current_date, predict_window=23, features=None, target_col='MOPJ'):
    """
    ê¸°ì¡´ í”„ë¡ íŠ¸ì—”ë“œì™€ í˜¸í™˜ë˜ëŠ” ì˜ˆì¸¡ í•¨ìˆ˜
    (ìƒˆë¡œìš´ êµ¬ì¡° + ê¸°ì¡´ í˜•íƒœ ë³€í™˜)
    """
    try:
        # ìƒˆë¡œìš´ generate_predictions í•¨ìˆ˜ ì‹¤í–‰
        new_results = generate_predictions(df, current_date, predict_window, features, target_col)
        
        # ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
        if isinstance(new_results.get('predictions'), dict):
            # ìƒˆë¡œìš´ êµ¬ì¡°ì¸ ê²½ìš°
            future_predictions = new_results['predictions']['future']
            validation_data = new_results['predictions']['validation']
            
            # futureì™€ validationì„ í•©ì³ì„œ ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
            all_predictions = future_predictions + validation_data
        else:
            # ê¸°ì¡´ êµ¬ì¡°ì¸ ê²½ìš°
            all_predictions = new_results.get('predictions_flat', new_results.get('predictions', []))
        
        # ê¸°ì¡´ í•„ë“œëª…ìœ¼ë¡œ ë³€í™˜
        compatible_predictions = convert_to_legacy_format(all_predictions)
        
        # ê²°ê³¼ì— ê¸°ì¡´ í˜•íƒœ ì¶”ê°€
        new_results['predictions'] = compatible_predictions  # ê¸°ì¡´ í˜¸í™˜ì„±
        new_results['predictions_new'] = new_results.get('predictions')  # ìƒˆë¡œìš´ êµ¬ì¡°ë„ ìœ ì§€
        
        logger.info(f"Generated {len(compatible_predictions)} compatible predictions")
        
        return new_results
        
    except Exception as e:
        logger.error(f"Error in compatible prediction generation: {str(e)}")
        raise e

def generate_predictions_with_save(df, current_date, predict_window=23, features=None, target_col='MOPJ', save_to_csv=True, file_path=None):
    """
    ì˜ˆì¸¡ ìˆ˜í–‰ ë° ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥ì´ í¬í•¨ëœ í•¨ìˆ˜ (ìˆ˜ì •ë¨)
    """
    try:
        logger.info(f"Starting prediction with smart cache save for {current_date}")
        
        # ê¸°ì¡´ generate_predictions í•¨ìˆ˜ ì‹¤í–‰
        results = generate_predictions(df, current_date, predict_window, features, target_col, file_path)
        
        # ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
        if save_to_csv:
            logger.info("Saving prediction with smart cache system...")
            
            # ìƒˆë¡œìš´ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥ í•¨ìˆ˜ ì‚¬ìš©
            save_result = save_prediction_simple(results, current_date)
            results['save_info'] = save_result
            
            if save_result['success']:
                logger.info(f"âœ… Smart cache save completed successfully")
                logger.info(f"  - Prediction Start Date: {save_result.get('prediction_start_date')}")
                logger.info(f"  - File: {save_result.get('file', 'N/A')}")
                
                # ìºì‹œ ì •ë³´ ì¶”ê°€ (ì•ˆì „í•œ í‚¤ ì ‘ê·¼)
                results['cache_info'] = {
                    'saved': True,
                    'prediction_start_date': save_result.get('prediction_start_date'),
                    'file': save_result.get('file'),
                    'success': save_result.get('success', False)
                }
            else:
                logger.warning(f"âŒ Failed to save prediction with smart cache: {save_result.get('error')}")
                results['cache_info'] = {
                    'saved': False,
                    'error': save_result.get('error')
                }
        else:
            logger.info("Skipping smart cache save (save_to_csv=False)")
            results['save_info'] = {'success': False, 'reason': 'save_to_csv=False'}
            results['cache_info'] = {
                'saved': False,
                'reason': 'save_to_csv=False'
            }
        
        return results
        
    except Exception as e:
        logger.error(f"Error in generate_predictions_with_save: {str(e)}")
        logger.error(traceback.format_exc())
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ë°˜í™˜í•˜ë˜, ì €ì¥ ì‹¤íŒ¨ ì •ë³´ í¬í•¨
        if 'results' in locals():
            results['save_info'] = {'success': False, 'error': str(e)}
            results['cache_info'] = {'saved': False, 'error': str(e)}
            return results
        else:
            # ì˜ˆì¸¡ ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
            raise e

def generate_predictions_with_attention_save(df, current_date, predict_window=23, features=None, target_col='MOPJ', save_to_csv=True, file_path=None):
    """
    ì˜ˆì¸¡ ìˆ˜í–‰ ë° attention í¬í•¨ CSV ì €ì¥ í•¨ìˆ˜
    
    Parameters:
    -----------
    df : pandas.DataFrame
        ì „ì²´ ë°ì´í„°
    current_date : str or datetime
        í˜„ì¬ ë‚ ì§œ (ë°ì´í„° ê¸°ì¤€ì¼)
    predict_window : int
        ì˜ˆì¸¡ ê¸°ê°„ (ê¸°ë³¸ 23ì¼)
    features : list, optional
        ì‚¬ìš©í•  íŠ¹ì„± ëª©ë¡
    target_col : str
        íƒ€ê²Ÿ ì»¬ëŸ¼ëª… (ê¸°ë³¸ 'MOPJ')
    save_to_csv : bool
        CSV ì €ì¥ ì—¬ë¶€ (ê¸°ë³¸ True)
    
    Returns:
    --------
    dict : ì˜ˆì¸¡ ê²°ê³¼ (attention ë°ì´í„° í¬í•¨)
    """
    try:
        logger.info(f"Starting prediction with attention save for {current_date}")
        
        # ê¸°ì¡´ generate_predictions í•¨ìˆ˜ ì‹¤í–‰
        results = generate_predictions(df, current_date, predict_window, features, target_col, file_path)
        
        # attention í¬í•¨ ì €ì¥ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
        if save_to_csv:
            logger.info("Saving prediction with attention data...")
            save_result = save_prediction_simple(results, current_date)
            results['save_info'] = save_result
            
            if save_result['success']:
                logger.info(f"SUCCESS: Prediction with attention saved successfully")
                logger.info(f"  - CSV: {save_result['csv_file']}")
                logger.info(f"  - Metadata: {save_result['meta_file']}")
                logger.info(f"  - Attention: {save_result['attention_file'] if save_result.get('attention_file') else 'Not saved'}")
            else:
                logger.warning(f"âŒ Failed to save prediction with attention: {save_result.get('error')}")
        else:
            logger.info("Skipping CSV save (save_to_csv=False)")
            results['save_info'] = {'success': False, 'reason': 'save_to_csv=False'}
        
        return results
        
    except Exception as e:
        logger.error(f"Error in generate_predictions_with_attention_save: {str(e)}")
        logger.error(traceback.format_exc())
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ë°˜í™˜í•˜ë˜, ì €ì¥ ì‹¤íŒ¨ ì •ë³´ í¬í•¨
        if 'results' in locals():
            results['save_info'] = {'success': False, 'error': str(e)}
            return results
        else:
            # ì˜ˆì¸¡ ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
            raise e

#######################################################################
# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì²˜ë¦¬
#######################################################################
# ğŸ”§ SyntaxError ìˆ˜ì • - check_existing_prediction í•¨ìˆ˜ (3987ë¼ì¸ ê·¼ì²˜)

def check_existing_prediction(current_date):
    """
    íŒŒì¼ë³„ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì—ì„œ ì €ì¥ëœ ì˜ˆì¸¡ì„ í™•ì¸í•˜ê³  ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    ğŸ¯ í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ì—ì„œ ìš°ì„  ê²€ìƒ‰
    """
    try:
        # í˜„ì¬ ë‚ ì§œ(ë°ì´í„° ê¸°ì¤€ì¼)ì—ì„œ ì²« ë²ˆì§¸ ì˜ˆì¸¡ ë‚ ì§œ ê³„ì‚°
        if isinstance(current_date, str):
            current_date = pd.to_datetime(current_date)
        
        # ë‹¤ìŒ ì˜ì—…ì¼ ì°¾ê¸° (í˜„ì¬ ë‚ ì§œì˜ ë‹¤ìŒ ì˜ì—…ì¼ì´ ì²« ë²ˆì§¸ ì˜ˆì¸¡ ë‚ ì§œ)
        next_date = current_date + pd.Timedelta(days=1)
        while next_date.weekday() >= 5 or is_holiday(next_date):
            next_date += pd.Timedelta(days=1)
        
        first_prediction_date = next_date
        date_str = first_prediction_date.strftime('%Y%m%d')
        
        logger.info(f"ğŸ” Checking cache for prediction starting: {first_prediction_date.strftime('%Y-%m-%d')}")
        logger.info(f"  ğŸ“… Data end date: {current_date.strftime('%Y-%m-%d')}")
        logger.info(f"  ğŸ“… Expected prediction start: {first_prediction_date.strftime('%Y-%m-%d')}")
        logger.info(f"  ğŸ“„ Expected filename pattern: prediction_start_{date_str}.*")
        
        # ğŸ¯ 1ë‹¨ê³„: í˜„ì¬ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ì •í™•í•œ ë‚ ì§œ ë§¤ì¹˜ë¡œ ìºì‹œ ì°¾ê¸°
        try:
            cache_dirs = get_file_cache_dirs()
            file_predictions_dir = cache_dirs['predictions']
            
            logger.info(f"  ğŸ“ Cache directory: {cache_dirs['root']}")
            logger.info(f"  ğŸ“ Predictions directory: {file_predictions_dir}")
            logger.info(f"  ğŸ“ Directory exists: {file_predictions_dir.exists()}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to get cache directories: {str(e)}")
            return None
        
        if file_predictions_dir.exists():
            exact_csv = file_predictions_dir / f"prediction_start_{date_str}.csv"
            exact_meta = file_predictions_dir / f"prediction_start_{date_str}_meta.json"
            
            logger.info(f"  ğŸ” Looking for: {exact_csv}")
            logger.info(f"  ğŸ” CSV exists: {exact_csv.exists()}")
            logger.info(f"  ğŸ” Meta exists: {exact_meta.exists()}")
            
            if exact_csv.exists() and exact_meta.exists():
                logger.info(f"âœ… Found exact prediction cache in file directory: {exact_csv.name}")
                return load_prediction_with_attention_from_csv_in_dir(first_prediction_date, file_predictions_dir)
            
            # í•´ë‹¹ íŒŒì¼ ë””ë ‰í† ë¦¬ì—ì„œ ë‹¤ë¥¸ ë‚ ì§œì˜ ì˜ˆì¸¡ ì°¾ê¸°
            logger.info("ğŸ” Searching for other predictions in file directory...")
            prediction_files = list(file_predictions_dir.glob("prediction_start_*_meta.json"))
            
            logger.info(f"  ğŸ“‹ Found {len(prediction_files)} prediction files:")
            for i, pf in enumerate(prediction_files):
                logger.info(f"    {i+1}. {pf.name}")
            
            if prediction_files:
                # ê°€ì¥ ìµœê·¼ ì˜ˆì¸¡ ì‚¬ìš© (ì„ì‹œ ë°©í¸)
                latest_file = max(prediction_files, key=lambda x: x.stem)
                cached_date_str = latest_file.stem.replace('prediction_start_', '').replace('_meta', '')
                cached_prediction_date = pd.to_datetime(cached_date_str, format='%Y%m%d')
                
                logger.info(f"ğŸ¯ Found compatible prediction in file directory!")
                logger.info(f"  ğŸ“… Cached prediction date: {cached_prediction_date.strftime('%Y-%m-%d')}")
                logger.info(f"  ğŸ“„ Using file: {latest_file.name}")
                
                return load_prediction_with_attention_from_csv_in_dir(cached_prediction_date, file_predictions_dir)
        else:
            logger.warning(f"âŒ Predictions directory does not exist: {file_predictions_dir}")
        
        # ğŸ¯ 2ë‹¨ê³„: ë‹¤ë¥¸ íŒŒì¼ ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ í˜¸í™˜ ìºì‹œ ì°¾ê¸°
        logger.info("ğŸ” Searching in other file cache directories...")
        
        cache_root = Path(CACHE_ROOT_DIR)
        if not cache_root.exists():
            logger.info("âŒ Cache root directory does not exist")
            return None
        
        current_file_path = prediction_state.get('current_file', None)
        logger.info(f"  ğŸ“‚ Current file: {current_file_path}")
        
        # ëª¨ë“  íŒŒì¼ ìºì‹œ ë””ë ‰í† ë¦¬ ìŠ¤ìº”
        other_dirs_checked = 0
        for file_dir in cache_root.iterdir():
            if not file_dir.is_dir() or file_dir.name == "default":
                continue
            
            # í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ëŠ” ì´ë¯¸ í™•ì¸í–ˆìœ¼ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
            if file_dir == cache_dirs['root']:
                continue
            
            other_dirs_checked += 1
            logger.info(f"  ğŸ” Checking other directory: {file_dir.name}")
            
            # ê° íŒŒì¼ ìºì‹œ ë””ë ‰í† ë¦¬ì˜ predictions í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ê²€ìƒ‰
            other_predictions_dir = file_dir / 'predictions'
            if not other_predictions_dir.exists():
                logger.info(f"    âŒ No predictions subdirectory in {file_dir.name}")
                continue
                
            prediction_files = list(other_predictions_dir.glob("prediction_start_*_meta.json"))
            logger.info(f"    ğŸ“‹ Found {len(prediction_files)} prediction files in {file_dir.name}")
            for meta_file in prediction_files:
                try:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        meta_data = json.load(f)
                    
                    # íŒŒì¼ í•´ì‹œ ë¹„êµ
                    current_file_hash = get_data_content_hash(current_file_path) if current_file_path else None
                    cached_file_hash = meta_data.get('file_content_hash')
                    
                    logger.info(f"    ğŸ” Checking {meta_file.name}:")
                    logger.info(f"      ğŸ“ Current file hash: {current_file_hash[:12] if current_file_hash else 'None'}...")
                    logger.info(f"      ğŸ“ Cached file hash:  {cached_file_hash[:12] if cached_file_hash else 'None'}...")
                    
                    if cached_file_hash and cached_file_hash == current_file_hash:
                        # ë™ì¼í•œ íŒŒì¼ ë‚´ìš©ì—ì„œ ìƒì„±ëœ ì˜ˆì¸¡ ë°œê²¬
                        cached_date_str = meta_file.stem.replace('prediction_start_', '').replace('_meta', '')
                        cached_prediction_date = pd.to_datetime(cached_date_str, format='%Y%m%d')
                        
                        logger.info(f"ğŸ¯ Found compatible prediction cache in other directory!")
                        logger.info(f"  ğŸ“ Directory: {file_dir.name}")
                        logger.info(f"  ğŸ“… Cached prediction date: {cached_prediction_date.strftime('%Y-%m-%d')}")
                        logger.info(f"  ğŸ“ File hash match: {cached_file_hash[:12]}...")
                        
                        return load_prediction_with_attention_from_csv_in_dir(cached_prediction_date, other_predictions_dir)
                        
                except Exception as e:
                    logger.debug(f"  âš ï¸  Error reading meta file {meta_file}: {str(e)}")
                    continue
        
        logger.info(f"ğŸ” Summary: Checked {other_dirs_checked} other cache directories")
        logger.info("âŒ No compatible prediction cache found")
        return None
        
    except Exception as e:
        logger.error(f"âŒ Error checking existing prediction: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def generate_visualizations_realtime(predictions, df, current_date, metadata):
    """ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹œê°í™” ìƒì„± (ì €ì¥í•˜ì§€ ì•ŠìŒ)"""
    try:
        # DataFrameìœ¼ë¡œ ë³€í™˜
        sequence_df = pd.DataFrame(predictions)
        if 'Date' in sequence_df.columns:
            sequence_df['Date'] = pd.to_datetime(sequence_df['Date'])
        
        # ì‹œì‘ê°’ ê³„ì‚°
        if isinstance(current_date, str):
            current_date = pd.to_datetime(current_date)
        
        start_day_value = df.loc[current_date, 'MOPJ'] if current_date in df.index else None
        
        if start_day_value is not None:
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = compute_performance_metrics_improved(sequence_df, start_day_value)
            
            # ê¸°ë³¸ ê·¸ë˜í”„ ìƒì„± (ë©”ëª¨ë¦¬ì—ë§Œ)
            _, basic_plot_img = plot_prediction_basic(
                sequence_df, 
                metadata.get('prediction_start_date', current_date),
                start_day_value,
                metrics['f1'],
                metrics['accuracy'], 
                metrics['mape'],
                metrics['weighted_score'],
                save_prefix=None  # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ìë™ ì‚¬ìš©
            )
            
            # ì´ë™í‰ê·  ê³„ì‚° ë° ì‹œê°í™”
            historical_data = df[df.index <= current_date].copy()
            ma_results = calculate_moving_averages_with_history(predictions, historical_data, target_col='MOPJ')
            
            _, ma_plot_img = plot_moving_average_analysis(
                ma_results,
                metadata.get('prediction_start_date', current_date),
                save_prefix=None  # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ìë™ ì‚¬ìš©
            )
            
            # ìƒíƒœì— ì €ì¥
            prediction_state['latest_plots'] = {
                'basic_plot': {'file': None, 'image': basic_plot_img},
                'ma_plot': {'file': None, 'image': ma_plot_img}
            }
            prediction_state['latest_ma_results'] = ma_results
            prediction_state['latest_metrics'] = metrics
            
        else:
            logger.warning("Cannot generate visualizations: start day value not available")
            prediction_state['latest_plots'] = {
                'basic_plot': {'file': None, 'image': None},
                'ma_plot': {'file': None, 'image': None}
            }
            prediction_state['latest_ma_results'] = {}
            prediction_state['latest_metrics'] = {}
            
    except Exception as e:
        logger.error(f"Error generating realtime visualizations: {str(e)}")
        prediction_state['latest_plots'] = {
            'basic_plot': {'file': None, 'image': None},
            'ma_plot': {'file': None, 'image': None}
        }
        prediction_state['latest_ma_results'] = {}
        prediction_state['latest_metrics'] = {}

def regenerate_visualizations_from_cache(predictions, df, current_date, metadata):
    """
    ìºì‹œëœ ë°ì´í„°ë¡œë¶€í„° ì‹œê°í™”ë¥¼ ì¬ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    ğŸ”‘ current_dateë¥¼ ì „ë‹¬í•˜ì—¬ ê³¼ê±°/ë¯¸ë˜ êµ¬ë¶„ ì‹œê°í™” ìƒì„±
    """
    try:
        logger.info("ğŸ¨ Regenerating visualizations from cached data...")
        
        # DataFrameìœ¼ë¡œ ë³€í™˜ (ì•ˆì „í•œ ë°©ì‹)
        temp_df_for_plot = pd.DataFrame([
            {
                'Date': pd.to_datetime(item.get('Date') or item.get('date')),
                'Prediction': safe_serialize_value(item.get('Prediction') or item.get('prediction')),
                'Actual': safe_serialize_value(item.get('Actual') or item.get('actual'))
            } for item in predictions if item.get('Date') or item.get('date')
        ])
        
        logger.info(f"  ğŸ“Š Plot data prepared: {len(temp_df_for_plot)} predictions")
        
        # current_date ì²˜ë¦¬
        if isinstance(current_date, str):
            current_date = pd.to_datetime(current_date)
        
        # ì‹œì‘ê°’ ê³„ì‚°
        start_day_value = None
        if current_date in df.index and not pd.isna(df.loc[current_date, 'MOPJ']):
            start_day_value = df.loc[current_date, 'MOPJ']
            logger.info(f"  ğŸ“ˆ Start day value: {start_day_value:.2f}")
        else:
            logger.warning(f"  âš ï¸  Start day value not available for {current_date}")
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸° (ì•ˆì „í•œ ë°©ì‹)
        metrics = metadata.get('metrics')
        if metrics:
            f1_score = safe_serialize_value(metrics.get('f1', 0.0))
            accuracy = safe_serialize_value(metrics.get('accuracy', 0.0))
            mape = safe_serialize_value(metrics.get('mape', 0.0))
            weighted_score = safe_serialize_value(metrics.get('weighted_score', 0.0))
            logger.info(f"  ğŸ“Š Metrics loaded - F1: {f1_score:.3f}, Acc: {accuracy:.1f}%, MAPE: {mape:.1f}%")
        else:
            f1_score = accuracy = mape = weighted_score = 0.0
            logger.info("  â„¹ï¸  No metrics available - using default values")
        
        plots = {
            'basic_plot': {'file': None, 'image': None},
            'ma_plot': {'file': None, 'image': None}
        }
        
        # ì‹œê°í™” ìƒì„± (ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°ë§Œ)
        if start_day_value is not None and not temp_df_for_plot.empty:
            logger.info("  ğŸ¨ Generating basic prediction plot...")
            
            # ì˜ˆì¸¡ ì‹œì‘ì¼ ê³„ì‚°
            prediction_start_date = metadata.get('prediction_start_date')
            if isinstance(prediction_start_date, str):
                prediction_start_date = pd.to_datetime(prediction_start_date)
            elif prediction_start_date is None:
                # ë©”íƒ€ë°ì´í„°ì— ì—†ìœ¼ë©´ current_date ë‹¤ìŒ ì˜ì—…ì¼ë¡œ ê³„ì‚°
                prediction_start_date = current_date + pd.Timedelta(days=1)
                while prediction_start_date.weekday() >= 5 or is_holiday(prediction_start_date):
                    prediction_start_date += pd.Timedelta(days=1)
                logger.info(f"  ğŸ“… Calculated prediction start date: {prediction_start_date}")
            
            # âœ… í•µì‹¬ ìˆ˜ì •: current_date ì „ë‹¬í•˜ì—¬ ê³¼ê±°/ë¯¸ë˜ êµ¬ë¶„ ì‹œê°í™”
            basic_plot_file, basic_plot_img = plot_prediction_basic(
                temp_df_for_plot,
                prediction_start_date,
                start_day_value,
                f1_score,
                accuracy,
                mape,
                weighted_score,
                current_date=current_date,  # ğŸ”‘ í•µì‹¬ ìˆ˜ì •: current_date ì „ë‹¬
                save_prefix=None,  # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ìë™ ì‚¬ìš©
                title_prefix="Cached Prediction Analysis"
            )
            
            if basic_plot_file:
                logger.info(f"  âœ… Basic plot generated: {basic_plot_file}")
            else:
                logger.warning("  âŒ Basic plot generation failed")
            
            # ì´ë™í‰ê·  ê³„ì‚° ë° ì‹œê°í™”
            logger.info("  ğŸ“ˆ Calculating moving averages...")
            historical_data = df[df.index <= current_date].copy()
            
            # ìºì‹œëœ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ì´ë™í‰ê·  ê³„ì‚°ìš©ìœ¼ë¡œ ë³€í™˜
            ma_input_data = []
            for pred in predictions:
                try:
                    ma_item = {
                        'Date': pd.to_datetime(pred.get('Date') or pred.get('date')),
                        'Prediction': safe_serialize_value(pred.get('Prediction') or pred.get('prediction')),
                        'Actual': safe_serialize_value(pred.get('Actual') or pred.get('actual'))
                    }
                    ma_input_data.append(ma_item)
                except Exception as e:
                    logger.warning(f"  âš ï¸  Error processing MA data item: {str(e)}")
                    continue
            
            ma_results = calculate_moving_averages_with_history(
                ma_input_data, historical_data, target_col='MOPJ'
            )
            
            if ma_results:
                logger.info(f"  ğŸ“Š MA calculated for {len(ma_results)} windows")
                
                # ì´ë™í‰ê·  ì‹œê°í™”
                ma_plot_file, ma_plot_img = plot_moving_average_analysis(
                    ma_results,
                    prediction_start_date,
                    save_prefix=None,  # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ìë™ ì‚¬ìš©
                    title_prefix="Cached Moving Average Analysis"
                )
                
                if ma_plot_file:
                    logger.info(f"  âœ… MA plot generated: {ma_plot_file}")
                else:
                    logger.warning("  âŒ MA plot generation failed")
            else:
                logger.warning("  âš ï¸  Moving average calculation failed")
                ma_plot_file, ma_plot_img = None, None
            
            plots = {
                'basic_plot': {'file': basic_plot_file, 'image': basic_plot_img},
                'ma_plot': {'file': ma_plot_file, 'image': ma_plot_img}
            }
            
            logger.info("  âœ… Visualizations regenerated from cache successfully")
        else:
            if start_day_value is None:
                logger.warning("  âŒ Cannot regenerate visualizations: start day value not available")
            if temp_df_for_plot.empty:
                logger.warning("  âŒ Cannot regenerate visualizations: no prediction data")
        
        return plots
        
    except Exception as e:
        logger.error(f"âŒ Error regenerating visualizations from cache: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'basic_plot': {'file': None, 'image': None},
            'ma_plot': {'file': None, 'image': None}
        }

def background_prediction_simple_compatible(file_path, current_date, save_to_csv=True, use_cache=True):
    """í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì˜ˆì¸¡ í•¨ìˆ˜ - ìºì‹œ ìš°ì„  ì‚¬ìš©, JSON ì•ˆì „ì„± ë³´ì¥"""
    global prediction_state
    
    try:
        prediction_state['is_predicting'] = True
        prediction_state['prediction_progress'] = 10
        prediction_state['error'] = None
        prediction_state['latest_file_path'] = file_path  # íŒŒì¼ ê²½ë¡œ ì €ì¥
        prediction_state['current_file'] = file_path  # ìºì‹œ ì—°ë™ìš© íŒŒì¼ ê²½ë¡œ
        
        logger.info(f"ğŸ¯ Starting compatible prediction for {current_date}")
        logger.info(f"  ğŸ”„ Cache enabled: {use_cache}")
        
        # ë°ì´í„° ë¡œë“œ
        df = load_data(file_path)
        prediction_state['current_data'] = df
        prediction_state['prediction_progress'] = 20
        
        # í˜„ì¬ ë‚ ì§œ ì²˜ë¦¬ ë° ì˜ì—…ì¼ ì¡°ì •
        if current_date is None:
            current_date = df.index.max()
        else:
            current_date = pd.to_datetime(current_date)
        
        # ğŸ¯ íœ´ì¼ì´ë©´ ë‹¤ìŒ ì˜ì—…ì¼ë¡œ ì¡°ì •
        original_date = current_date
        adjusted_date = current_date
        
        # ì£¼ë§ì´ë‚˜ íœ´ì¼ì´ë©´ ë‹¤ìŒ ì˜ì—…ì¼ë¡œ ì´ë™
        while adjusted_date.weekday() >= 5 or is_holiday(adjusted_date):
            adjusted_date += pd.Timedelta(days=1)
        
        if adjusted_date != original_date:
            logger.info(f"ğŸ“… Date adjusted for business day: {original_date.strftime('%Y-%m-%d')} -> {adjusted_date.strftime('%Y-%m-%d')}")
            logger.info(f"  ğŸ“‹ Reason: {'Weekend' if original_date.weekday() >= 5 else 'Holiday'}")
        
        current_date = adjusted_date
        
        # ìºì‹œ í™•ì¸ (ìš°ì„  ì‚¬ìš©)
        if use_cache:
            logger.info("ğŸ” Checking for existing prediction cache...")
            prediction_state['prediction_progress'] = 30
            
            try:
                cached_result = check_existing_prediction(current_date)
                logger.info(f"  ğŸ“‹ Cache check result: {cached_result is not None}")
                if cached_result:
                    logger.info(f"  ğŸ“‹ Cache success status: {cached_result.get('success', False)}")
                else:
                    logger.info("  âŒ No cache result returned")
            except Exception as cache_check_error:
                logger.error(f"  âŒ Cache check failed with error: {str(cache_check_error)}")
                logger.error(f"  ğŸ“ Error traceback: {traceback.format_exc()}")
                cached_result = None
            
            if cached_result and cached_result.get('success'):
                logger.info("ğŸ‰ Found existing prediction! Loading from cache...")
                prediction_state['prediction_progress'] = 50
                
                try:
                    # ìºì‹œëœ ë°ì´í„° ë¡œë“œ ë° ì •ë¦¬
                    predictions = cached_result['predictions']
                    metadata = cached_result['metadata']
                    attention_data = cached_result.get('attention_data')
                    
                    # ë°ì´í„° ì •ë¦¬ (JSON ì•ˆì „ì„± ë³´ì¥)
                    cleaned_predictions = clean_cached_predictions(predictions)
                    
                    # í˜¸í™˜ì„± ìœ ì§€ëœ í˜•íƒœë¡œ ë³€í™˜
                    compatible_predictions = convert_to_legacy_format(cleaned_predictions)
                    
                    # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
                    try:
                        test_json = json.dumps(compatible_predictions[:1] if compatible_predictions else [])
                        logger.info("âœ… JSON serialization test passed for cached data")
                    except Exception as json_error:
                        logger.error(f"âŒ JSON serialization failed for cached data: {str(json_error)}")
                        raise Exception("Cached data serialization failed")
                    
                    # êµ¬ê°„ ì ìˆ˜ ì²˜ë¦¬ (JSON ì•ˆì „)
                    interval_scores = metadata.get('interval_scores', {})
                    cleaned_interval_scores = {}
                    for key, value in interval_scores.items():
                        if isinstance(value, dict):
                            cleaned_score = {}
                            for k, v in value.items():
                                cleaned_score[k] = safe_serialize_value(v)
                            cleaned_interval_scores[key] = cleaned_score
                        else:
                            cleaned_interval_scores[key] = safe_serialize_value(value)
                    
                    # ì´ë™í‰ê·  ì¬ê³„ì‚°
                    prediction_state['prediction_progress'] = 60
                    logger.info("Recalculating moving averages from cached data...")
                    historical_data = df[df.index <= current_date].copy()
                    ma_results = calculate_moving_averages_with_history(
                        cleaned_predictions, historical_data, target_col='MOPJ'
                    )
                    
                    # ì‹œê°í™” ì¬ìƒì„±
                    prediction_state['prediction_progress'] = 70
                    logger.info("Regenerating visualizations from cached data...")
                    plots = regenerate_visualizations_from_cache(
                        cleaned_predictions, df, current_date, metadata
                    )
                    
                    # ë©”íŠ¸ë¦­ ì •ë¦¬
                    metrics = metadata.get('metrics')
                    cleaned_metrics = {}
                    if metrics:
                        for key, value in metrics.items():
                            cleaned_metrics[key] = safe_serialize_value(value)
                    
                    # ì–´í…ì…˜ ë°ì´í„° ì •ë¦¬
                    cleaned_attention = None
                    if attention_data:
                        cleaned_attention = {}
                        for key, value in attention_data.items():
                            if key == 'image' and value:
                                cleaned_attention[key] = value  # base64 ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ
                            elif isinstance(value, dict):
                                cleaned_attention[key] = {}
                                for k, v in value.items():
                                    cleaned_attention[key][k] = safe_serialize_value(v)
                            else:
                                cleaned_attention[key] = safe_serialize_value(value)
                    
                    # ìƒíƒœ ì„¤ì •
                    prediction_state['latest_predictions'] = compatible_predictions
                    prediction_state['latest_attention_data'] = cleaned_attention
                    prediction_state['current_date'] = safe_serialize_value(metadata.get('prediction_start_date'))
                    prediction_state['selected_features'] = metadata.get('selected_features', [])
                    prediction_state['semimonthly_period'] = safe_serialize_value(metadata.get('semimonthly_period'))
                    prediction_state['next_semimonthly_period'] = safe_serialize_value(metadata.get('next_semimonthly_period'))
                    prediction_state['latest_interval_scores'] = cleaned_interval_scores
                    prediction_state['latest_metrics'] = cleaned_metrics
                    prediction_state['latest_plots'] = plots
                    prediction_state['latest_ma_results'] = ma_results
                    
                    # feature_importance ì„¤ì •
                    if cleaned_attention and 'feature_importance' in cleaned_attention:
                        prediction_state['feature_importance'] = cleaned_attention['feature_importance']
                    else:
                        prediction_state['feature_importance'] = None
                    
                    prediction_state['prediction_progress'] = 100
                    prediction_state['is_predicting'] = False
                    logger.info("âœ… Cache prediction completed successfully!")
                    return
                    
                except Exception as cache_error:
                    logger.warning(f"âš ï¸  Cache processing failed: {str(cache_error)}")
                    logger.info("ğŸ”„ Falling back to new prediction...")
            else:
                logger.info("  ğŸ“‹ No usable cache found - proceeding with new prediction")
        else:
            logger.info("ğŸ†• Cache disabled - running new prediction...")
        
        # ìƒˆë¡œìš´ ì˜ˆì¸¡ ìˆ˜í–‰
        logger.info("ğŸ¤– Running new prediction...")
        prediction_state['prediction_progress'] = 40
        
        results = generate_predictions_compatible(df, current_date)
        prediction_state['prediction_progress'] = 80
        
        # ìƒˆë¡œìš´ ì˜ˆì¸¡ ê²°ê³¼ ì •ë¦¬ (JSON ì•ˆì „ì„± ë³´ì¥)
        if isinstance(results.get('predictions'), list):
            raw_predictions = results['predictions']
        else:
            raw_predictions = results.get('predictions_flat', [])
        
        # í˜¸í™˜ì„± ìœ ì§€ëœ í˜•íƒœë¡œ ë³€í™˜
        compatible_predictions = convert_to_legacy_format(raw_predictions)
        
        # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
        try:
            test_json = json.dumps(compatible_predictions[:1] if compatible_predictions else [])
            logger.info("âœ… JSON serialization test passed for new prediction")
        except Exception as json_error:
            logger.error(f"âŒ JSON serialization failed for new prediction: {str(json_error)}")
            # ë°ì´í„° ì¶”ê°€ ì •ë¦¬ ì‹œë„
            for pred in compatible_predictions:
                for key, value in pred.items():
                    pred[key] = safe_serialize_value(value)
        
        # ìƒíƒœ ì„¤ì •
        prediction_state['latest_predictions'] = compatible_predictions
        prediction_state['latest_attention_data'] = results.get('attention_data')
        prediction_state['current_date'] = safe_serialize_value(results.get('current_date'))
        prediction_state['selected_features'] = results.get('selected_features', [])
        prediction_state['semimonthly_period'] = safe_serialize_value(results.get('semimonthly_period'))
        prediction_state['next_semimonthly_period'] = safe_serialize_value(results.get('next_semimonthly_period'))
        prediction_state['latest_interval_scores'] = results.get('interval_scores', {})
        prediction_state['latest_metrics'] = results.get('metrics')
        prediction_state['latest_plots'] = results.get('plots', {})
        prediction_state['latest_ma_results'] = results.get('ma_results', {})
        
        # feature_importance ì„¤ì •
        if results.get('attention_data') and 'feature_importance' in results['attention_data']:
            prediction_state['feature_importance'] = results['attention_data']['feature_importance']
        else:
            prediction_state['feature_importance'] = None
        
        # ì €ì¥
        if save_to_csv:
            logger.info("ğŸ’¾ Saving prediction to cache...")
            save_result = save_prediction_simple(results, current_date)
            if save_result['success']:
                logger.info(f"âœ… Cache saved successfully: {save_result.get('prediction_start_date')}")
            else:
                logger.warning(f"âš ï¸  Cache save failed: {save_result.get('error')}")
        
        prediction_state['prediction_progress'] = 100
        prediction_state['is_predicting'] = False
        logger.info("âœ… New prediction completed successfully")
        
    except Exception as e:
        logger.error(f"âŒ Error in compatible prediction: {str(e)}")
        logger.error(traceback.format_exc())
        prediction_state['error'] = str(e)
        prediction_state['is_predicting'] = False
        prediction_state['prediction_progress'] = 0


def safe_serialize_value(value):
    """ê°’ì„ JSON ì•ˆì „í•˜ê²Œ ì§ë ¬í™” (ë°°ì—´ íƒ€ì… ì²˜ë¦¬ ê°œì„ )"""
    if value is None:
        return None
    
    # numpy/pandas ë°°ì—´ íƒ€ì… ë¨¼ì € ì²´í¬
    if isinstance(value, (np.ndarray, pd.Series, list)):
        if len(value) == 0:
            return []
        elif len(value) == 1:
            # ë‹¨ì¼ ì›ì†Œ ë°°ì—´ì¸ ê²½ìš° ìŠ¤ì¹¼ë¼ë¡œ ì²˜ë¦¬
            return safe_serialize_value(value[0])
        else:
            # ë‹¤ì¤‘ ì›ì†Œ ë°°ì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            try:
                return [safe_serialize_value(item) for item in value]
            except:
                return [str(item) for item in value]
    
    # ìŠ¤ì¹¼ë¼ ê°’ì— ëŒ€í•´ì„œë§Œ pd.isna ì²´í¬
    try:
        if pd.isna(value):  # ìŠ¤ì¹¼ë¼ ê°’ì— ëŒ€í•´ì„œë§Œ ì‚¬ìš©
            return None
    except (TypeError, ValueError):
        # pd.isnaê°€ ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” íƒ€ì…ì¸ ê²½ìš° ë„˜ì–´ê°
        pass
    
    if isinstance(value, (int, float)):
        if np.isnan(value) or np.isinf(value):
            return None
        return float(value)
    elif isinstance(value, np.floating):  # numpy float íƒ€ì… ì²˜ë¦¬
        if np.isnan(value) or np.isinf(value):
            return None
        return float(value)
    elif isinstance(value, np.integer):  # numpy int íƒ€ì… ì²˜ë¦¬
        return int(value)
    elif isinstance(value, str):
        return value
    elif hasattr(value, 'isoformat'):  # datetime/Timestamp
        return value.strftime('%Y-%m-%d')
    elif hasattr(value, 'strftime'):  # ê¸°íƒ€ ë‚ ì§œ ê°ì²´
        return value.strftime('%Y-%m-%d')
    else:
        try:
            # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
            json.dumps(value)
            return value
        except (TypeError, ValueError):
            return str(value)

def clean_predictions_data(predictions):
    """ì˜ˆì¸¡ ë°ì´í„°ë¥¼ JSON ì•ˆì „í•˜ê²Œ ì •ë¦¬"""
    if not predictions:
        return []
    
    cleaned = []
    for pred in predictions:
        cleaned_pred = {}
        for key, value in pred.items():
            if key in ['date', 'prediction_from']:
                # ë‚ ì§œ í•„ë“œ
                if hasattr(value, 'strftime'):
                    cleaned_pred[key] = value.strftime('%Y-%m-%d')
                else:
                    cleaned_pred[key] = str(value)
            elif key in ['prediction', 'actual', 'error', 'error_pct']:
                # ìˆ«ì í•„ë“œ
                cleaned_pred[key] = safe_serialize_value(value)
            else:
                # ê¸°íƒ€ í•„ë“œ
                cleaned_pred[key] = safe_serialize_value(value)
        cleaned.append(cleaned_pred)
    
    return cleaned

def clean_cached_predictions(predictions):
    """ìºì‹œì—ì„œ ë¡œë“œëœ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    cleaned_predictions = []
    
    for pred in predictions:
        try:
            # ëª¨ë“  í•„ë“œë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            cleaned_pred = {}
            for key, value in pred.items():
                if key in ['Date', 'date']:
                    # ë‚ ì§œ í•„ë“œ íŠ¹ë³„ ì²˜ë¦¬
                    if pd.notna(value):
                        if hasattr(value, 'strftime'):
                            cleaned_pred[key] = value.strftime('%Y-%m-%d')
                        else:
                            cleaned_pred[key] = str(value)[:10]
                    else:
                        cleaned_pred[key] = None
                elif key in ['Prediction', 'prediction', 'Actual', 'actual']:
                    # ìˆ«ì í•„ë“œ ì²˜ë¦¬
                    cleaned_pred[key] = safe_serialize_value(value)
                else:
                    # ê¸°íƒ€ í•„ë“œ
                    cleaned_pred[key] = safe_serialize_value(value)
            
            cleaned_predictions.append(cleaned_pred)
            
        except Exception as e:
            logger.warning(f"Error cleaning prediction item: {str(e)}")
            continue
    
    return cleaned_predictions

def clean_interval_scores_safe(interval_scores):
    """êµ¬ê°„ ì ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    cleaned_interval_scores = []
    
    try:
        if isinstance(interval_scores, dict):
            for key, value in interval_scores.items():
                if isinstance(value, dict):
                    cleaned_score = {}
                    for k, v in value.items():
                        # ë°°ì—´ì´ë‚˜ ë³µì¡í•œ íƒ€ì…ì€ íŠ¹ë³„ ì²˜ë¦¬
                        if isinstance(v, (np.ndarray, pd.Series, list)):
                            if len(v) == 1:
                                cleaned_score[k] = safe_serialize_value(v[0])
                            elif len(v) == 0:
                                cleaned_score[k] = None
                            else:
                                # ë‹¤ì¤‘ ì›ì†Œ ë°°ì—´ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
                                cleaned_score[k] = str(v)
                        else:
                            cleaned_score[k] = safe_serialize_value(v)
                    cleaned_interval_scores.append(cleaned_score)
                else:
                    # dictê°€ ì•„ë‹Œ ê²½ìš° ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    cleaned_interval_scores.append(safe_serialize_value(value))
        elif isinstance(interval_scores, list):
            for score in interval_scores:
                if isinstance(score, dict):
                    cleaned_score = {}
                    for k, v in score.items():
                        # ë°°ì—´ì´ë‚˜ ë³µì¡í•œ íƒ€ì…ì€ íŠ¹ë³„ ì²˜ë¦¬
                        if isinstance(v, (np.ndarray, pd.Series, list)):
                            if len(v) == 1:
                                cleaned_score[k] = safe_serialize_value(v[0])
                            elif len(v) == 0:
                                cleaned_score[k] = None
                            else:
                                cleaned_score[k] = str(v)
                        else:
                            cleaned_score[k] = safe_serialize_value(v)
                    cleaned_interval_scores.append(cleaned_score)
                else:
                    cleaned_interval_scores.append(safe_serialize_value(score))
        
        return cleaned_interval_scores
        
    except Exception as e:
        logger.error(f"Error cleaning interval scores: {str(e)}")
        return []

def convert_to_legacy_format(predictions_data):
    """
    ìƒˆÂ·ì˜› êµ¬ì¡°ë¥¼ ëª¨ë‘ ë°›ì•„ í”„ë¡ íŠ¸ì—”ë“œ(ëŒ€ë¬¸ì) + ë°±ì—”ë“œ(ì†Œë¬¸ì) í‚¤ë¥¼ ë™ì‹œ ë³´ì¡´.
    JSON ì§ë ¬í™” ì•ˆì „ì„± ë³´ì¥
    """
    if not predictions_data:
        return []
    
    legacy_out = []
    for pred in predictions_data:
        try:
            # ë‚ ì§œ í•„ë“œ ì•ˆì „ ì²˜ë¦¬
            date_value = pred.get("date") or pred.get("Date")
            if hasattr(date_value, 'strftime'):
                date_str = date_value.strftime('%Y-%m-%d')
            elif isinstance(date_value, str):
                date_str = date_value[:10] if len(date_value) > 10 else date_value
            else:
                date_str = str(date_value) if date_value is not None else None
            
            # ì˜ˆì¸¡ê°’ ì•ˆì „ ì²˜ë¦¬
            prediction_value = pred.get("prediction") or pred.get("Prediction")
            prediction_safe = safe_serialize_value(prediction_value)
            
            # ì‹¤ì œê°’ ì•ˆì „ ì²˜ë¦¬
            actual_value = pred.get("actual") or pred.get("Actual")
            actual_safe = safe_serialize_value(actual_value)
            
            # ê¸°íƒ€ í•„ë“œë“¤ ì•ˆì „ ì²˜ë¦¬
            prediction_from = pred.get("prediction_from")
            if hasattr(prediction_from, 'strftime'):
                prediction_from = prediction_from.strftime('%Y-%m-%d')
            elif prediction_from:
                prediction_from = str(prediction_from)
            
            legacy_item = {
                # â”€â”€ í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ ëŒ€ë¬¸ì í‚¤ (JSON ì•ˆì „) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                "Date": date_str,
                "Prediction": prediction_safe,
                "Actual": actual_safe,

                # â”€â”€ ë°±ì—”ë“œ í›„ì† í•¨ìˆ˜(ì†Œë¬¸ì 'date' ì°¸ì¡°)ìš© â”€â”€
                "date": date_str,
                "prediction": prediction_safe,
                "actual": actual_safe,

                # ê¸°íƒ€ í•„ë“œ ì•ˆì „ ì²˜ë¦¬
                "Prediction_From": prediction_from,
                "SemimonthlyPeriod": safe_serialize_value(pred.get("semimonthly_period")),
                "NextSemimonthlyPeriod": safe_serialize_value(pred.get("next_semimonthly_period")),
                "is_synthetic": bool(pred.get("is_synthetic", False)),
                
                # ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ìˆëŠ” ê²½ìš°)
                "day_offset": safe_serialize_value(pred.get("day_offset")),
                "is_business_day": bool(pred.get("is_business_day", True)),
                "error": safe_serialize_value(pred.get("error")),
                "error_pct": safe_serialize_value(pred.get("error_pct"))
            }
            
            legacy_out.append(legacy_item)
            
        except Exception as e:
            logger.warning(f"Error converting prediction item: {str(e)}")
            continue
    
    return legacy_out

#######################################################################
# API ì—”ë“œí¬ì¸íŠ¸
#######################################################################

@app.route('/api/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸ API"""
    return jsonify({
        'status': 'ok',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/test', methods=['GET'])
def test_api():
    """API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    return jsonify({
        'status': 'ok',
        'message': 'API is working!',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/test/cache-dirs', methods=['GET'])
def test_cache_dirs():
    """ìºì‹œ ë””ë ‰í† ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    try:
        # í˜„ì¬ ìƒíƒœ í™•ì¸
        current_file = prediction_state.get('current_file', None)
        
        # íŒŒì¼ ê²½ë¡œê°€ ìˆìœ¼ë©´ í•´ë‹¹ íŒŒì¼ë¡œ, ì—†ìœ¼ë©´ ê¸°ë³¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        test_file = request.args.get('file_path', current_file)
        
        if test_file and not os.path.exists(test_file):
            return jsonify({
                'error': f'File does not exist: {test_file}',
                'current_file': current_file
            }), 400
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸
        cache_dirs = get_file_cache_dirs(test_file)
        
        # ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        dir_status = {}
        for name, path in cache_dirs.items():
            dir_status[name] = {
                'path': str(path),
                'exists': path.exists(),
                'is_dir': path.is_dir() if path.exists() else False
            }
        
        return jsonify({
            'success': True,
            'test_file': test_file,
            'current_file': current_file,
            'cache_dirs': dir_status,
            'cache_root_exists': Path(CACHE_ROOT_DIR).exists()
        })
        
    except Exception as e:
        logger.error(f"Cache directory test failed: {str(e)}")
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/api/upload', methods=['POST'])
def upload_file():
    """ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê¸°ëŠ¥ì´ ìˆëŠ” CSV íŒŒì¼ ì—…ë¡œë“œ API"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
        
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400
        
    if file and file.filename.endswith('.csv'):
        try:
            # ì„ì‹œ íŒŒì¼ëª… ìƒì„±
            original_filename = secure_filename(file.filename)
            temp_filename = secure_filename(f"temp_{int(time.time())}.csv")
            temp_filepath = os.path.join(app.config['UPLOAD_FOLDER'], temp_filename)
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            file.save(temp_filepath)
            logger.info(f"ğŸ“¤ [UPLOAD] File saved temporarily: {temp_filename}")
            
            # ğŸ” ìºì‹œ í˜¸í™˜ì„± í™•ì¸
            cache_result = find_compatible_cache_file(temp_filepath)
            
            response_data = {
                'success': True,
                'filepath': temp_filepath,
                'filename': temp_filename,
                'original_filename': original_filename,
                'cache_info': {
                    'found': cache_result['found'],
                    'cache_type': cache_result.get('cache_type'),
                    'message': None
                }
            }
            
            if cache_result['found']:
                cache_type = cache_result['cache_type']
                cache_file = cache_result['cache_file']
                
                if cache_type == 'exact':
                    response_data['cache_info']['message'] = f"ë™ì¼í•œ ë°ì´í„° ë°œê²¬! ê¸°ì¡´ ìºì‹œë¥¼ í™œìš©í•©ë‹ˆë‹¤. ({os.path.basename(cache_file)})"
                    response_data['cache_info']['compatible_file'] = cache_file
                    logger.info(f"âœ… [CACHE] Exact match found: {cache_file}")
                    
                elif cache_type == 'extension':
                    ext_info = cache_result['extension_info']
                    response_data['cache_info']['message'] = f"ë°ì´í„° í™•ì¥ ê°ì§€! {ext_info['new_rows_count']}ê°œ ìƒˆ í–‰ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ìºì‹œë¥¼ í™œìš©í•©ë‹ˆë‹¤."
                    response_data['cache_info']['compatible_file'] = cache_file
                    response_data['cache_info']['extension_info'] = ext_info
                    logger.info(f"ğŸ“ˆ [CACHE] Extension detected: +{ext_info['new_rows_count']} rows from {cache_file}")
                    
                # í˜¸í™˜ íŒŒì¼ì„ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ì„¤ì • (ìºì‹œ í™œìš©ì„ ìœ„í•´)
                response_data['filepath'] = cache_file
                response_data['filename'] = os.path.basename(cache_file)
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ (í•„ìš”ì‹œ)
                if temp_filepath != cache_file:
                    try:
                        os.remove(temp_filepath)
                        logger.info(f"ğŸ—‘ï¸ [CLEANUP] Temporary file removed: {temp_filename}")
                    except:
                        pass
            else:
                # ìƒˆ íŒŒì¼ì¸ ê²½ìš° ì •ì‹ íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½
                content_hash = get_data_content_hash(temp_filepath)
                final_filename = f"data_{content_hash}.csv" if content_hash else temp_filename
                final_filepath = os.path.join(app.config['UPLOAD_FOLDER'], final_filename)
                
                if temp_filepath != final_filepath:
                    shutil.move(temp_filepath, final_filepath)
                    logger.info(f"ğŸ“ [UPLOAD] File renamed: {final_filename}")
                    
                response_data['filepath'] = final_filepath
                response_data['filename'] = final_filename
                response_data['cache_info']['message'] = "ìƒˆë¡œìš´ ë°ì´í„°ì…ë‹ˆë‹¤. ì˜ˆì¸¡ í›„ ìºì‹œë¡œ ì €ì¥ë©ë‹ˆë‹¤."
            
            # ğŸ”‘ ì—…ë¡œë“œëœ íŒŒì¼ ê²½ë¡œë¥¼ ì „ì—­ ìƒíƒœì— ì €ì¥
            prediction_state['current_file'] = response_data['filepath']
            logger.info(f"ğŸ“ Set current_file in prediction_state: {response_data['filepath']}")
            
            return jsonify(response_data)
            
        except Exception as e:
            logger.error(f"Error during file upload: {str(e)}")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                if 'temp_filepath' in locals() and os.path.exists(temp_filepath):
                    os.remove(temp_filepath)
            except:
                pass
            return jsonify({'error': f'Error processing file: {str(e)}'}), 500
    
    return jsonify({'error': 'Invalid file type. Only CSV files are allowed'}), 400

@app.route('/api/holidays', methods=['GET'])
def get_holidays():
    """íœ´ì¼ ëª©ë¡ ì¡°íšŒ API"""
    try:
        # íœ´ì¼ì„ ë‚ ì§œì™€ ì„¤ëª…ì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        holidays_list = []
        file_holidays = load_holidays_from_file()  # íŒŒì¼ì—ì„œ ë¡œë“œ
        
        # í˜„ì¬ ì „ì—­ íœ´ì¼ì—ì„œ íŒŒì¼ íœ´ì¼ê³¼ ìë™ ê°ì§€ íœ´ì¼ êµ¬ë¶„
        auto_detected = holidays - file_holidays
        
        for holiday_date in file_holidays:
            holidays_list.append({
                'date': holiday_date,
                'description': 'Holiday (from file)',
                'source': 'file'
            })
        
        for holiday_date in auto_detected:
            holidays_list.append({
                'date': holiday_date,
                'description': 'Holiday (detected from missing data)',
                'source': 'auto_detected'
            })
        
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬
        holidays_list.sort(key=lambda x: x['date'])
        
        return jsonify({
            'success': True,
            'holidays': holidays_list,
            'count': len(holidays_list),
            'file_holidays': len(file_holidays),
            'auto_detected_holidays': len(auto_detected)
        })
    except Exception as e:
        logger.error(f"Error getting holidays: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'holidays': [],
            'count': 0
        }), 500

@app.route('/api/holidays/upload', methods=['POST'])
def upload_holidays():
    """íœ´ì¼ ëª©ë¡ íŒŒì¼ ì—…ë¡œë“œ API"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
        
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400
        
    if file and (file.filename.endswith('.csv') or file.filename.endswith('.xlsx')):
        try:
            # ì„ì‹œ íŒŒì¼ëª… ìƒì„±
            filename = secure_filename(f"holidays_{int(time.time())}{os.path.splitext(file.filename)[1]}")
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            
            # íŒŒì¼ ì €ì¥
            file.save(filepath)
            
            # íœ´ì¼ ì •ë³´ ì—…ë°ì´íŠ¸
            new_holidays = update_holidays(filepath)
            
            # ì›ë³¸ íŒŒì¼ì„ ëª¨ë¸ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬ (standard location)
            permanent_path = os.path.join('models', 'holidays' + os.path.splitext(file.filename)[1])
            shutil.copy2(filepath, permanent_path)
            
            return jsonify({
                'success': True,
                'message': f'Successfully uploaded and loaded {len(new_holidays)} holidays',
                'filepath': permanent_path,
                'filename': os.path.basename(permanent_path),
                'holidays': list(new_holidays)
            })
        except Exception as e:
            logger.error(f"Error during holiday file upload: {str(e)}")
            logger.error(traceback.format_exc())
            return jsonify({'error': f'Error processing file: {str(e)}'}), 500
    
    return jsonify({'error': 'Invalid file type. Only CSV and Excel files are allowed'}), 400

@app.route('/api/holidays/reload', methods=['POST'])
def reload_holidays():
    """íœ´ì¼ ëª©ë¡ ì¬ë¡œë“œ API"""
    filepath = request.json.get('filepath')
    
    # ê¸°ë³¸ íŒŒì¼ ë˜ëŠ” ì§€ì •ëœ íŒŒì¼ë¡œë¶€í„° ì¬ë¡œë“œ
    new_holidays = update_holidays(filepath)
    
    return jsonify({
        'success': True,
        'message': f'Successfully reloaded {len(new_holidays)} holidays',
        'holidays': list(new_holidays)
    })

@app.route('/api/file/metadata', methods=['GET'])
def get_file_metadata():
    """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¡°íšŒ API"""
    filepath = request.args.get('filepath')
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'File not found'}), 404
    
    try:
        # ê¸°ë³¸ ì •ë³´ë§Œ ì½ì–´ì„œ ë°˜í™˜
        df = pd.read_csv(filepath, nrows=5)  # ì²˜ìŒ 5í–‰ë§Œ ì½ê¸°
        columns = df.columns.tolist()
        latest_date = None
        
        if 'Date' in df.columns:
            # ë‚ ì§œ ì •ë³´ë¥¼ ë³„ë„ë¡œ ì½ì–´ì„œ ìµœì‹  ë‚ ì§œ í™•ì¸
            dates_df = pd.read_csv(filepath, usecols=['Date'])
            dates_df['Date'] = pd.to_datetime(dates_df['Date'])
            latest_date = dates_df['Date'].max().strftime('%Y-%m-%d')
        
        return jsonify({
            'success': True,
            'rows': len(df),
            'columns': columns,
            'latest_date': latest_date,
            'sample': df.head().to_dict(orient='records')
        })
    except Exception as e:
        logger.error(f"Error reading file metadata: {str(e)}")
        return jsonify({'error': f'Error reading file: {str(e)}'}), 500
    
@app.route('/api/data/dates', methods=['GET'])
def get_available_dates():
    filepath = request.args.get('filepath')
    days_limit = int(request.args.get('limit', 999999))  # ê¸°ë³¸ê°’ì„ ë§¤ìš° í° ìˆ˜ë¡œ ì„¤ì • (ëª¨ë“  ë‚ ì§œ)
    
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'File not found'}), 404
    
    try:
        df = pd.read_csv(filepath)
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date')
        
        # ğŸ–ï¸ ë°ì´í„°ë¥¼ ë¡œë“œí•œ í›„ íœ´ì¼ ì •ë³´ ìë™ ì—…ë°ì´íŠ¸ (ë¹ˆ í‰ì¼ ê°ì§€) - ì„ì‹œ ë¹„í™œì„±í™”
        logger.info(f"ğŸ–ï¸ [HOLIDAYS] Auto-detection temporarily disabled to show more dates...")
        # updated_holidays = update_holidays(df=df)
        updated_holidays = load_holidays_from_file()  # íŒŒì¼ íœ´ì¼ë§Œ ì‚¬ìš©
        logger.info(f"ğŸ–ï¸ [HOLIDAYS] Total holidays (file only): {len(updated_holidays)}")
        
        # ì „ì²´ ë°ì´í„°ì˜ 50% ì§€ì  ê³„ì‚° (ì°¸ê³ ìš©, ì‹¤ì œ í•„í„°ë§ì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        total_rows = len(df)
        halfway_index = total_rows // 2
        halfway_date = df.iloc[halfway_index]['Date']
        
        logger.info(f"ğŸ“Š Total data rows: {total_rows}")
        logger.info(f"ğŸ“ 50% point: row {halfway_index}, date: {halfway_date.strftime('%Y-%m-%d')}")
        
        # 50% ì§€ì ì—ì„œ ë‹¤ìŒ ë°˜ì›” ì‹œì‘ì¼ ê³„ì‚° (ì°¸ê³ ìš©)
        halfway_semimonthly = get_semimonthly_period(halfway_date)
        next_semimonthly = get_next_semimonthly_period(halfway_date)
        prediction_start_threshold, _ = get_semimonthly_date_range(next_semimonthly)
        
        logger.info(f"ğŸ“… 50% point semimonthly period: {halfway_semimonthly}")
        logger.info(f"ğŸ¯ Next semimonthly period: {next_semimonthly}")
        logger.info(f"ğŸš€ Prediction start threshold: {prediction_start_threshold.strftime('%Y-%m-%d')}")
        
        # ğŸ”§ 50% ì§€ì  ì´í›„ë§Œ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë‚ ì§œë¡œ ì„¤ì •
        predictable_dates = df.iloc[halfway_index:]['Date']
        
        # ì˜ˆì¸¡ ê°€ëŠ¥í•œ ëª¨ë“  ë‚ ì§œë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ë°˜í™˜ (ìµœì‹  ë‚ ì§œë¶€í„°)
        # days_limitë³´ë‹¤ ì‘ì€ ê²½ìš°ì—ë§Œ ì œí•œ ì ìš©
        if len(predictable_dates) <= days_limit:
            dates = predictable_dates.sort_values(ascending=False).dt.strftime('%Y-%m-%d').tolist()
        else:
            dates = predictable_dates.sort_values(ascending=False).head(days_limit).dt.strftime('%Y-%m-%d').tolist()
        
        logger.info(f"ğŸ”¢ Predictable dates count: {len(predictable_dates)} â†’ ë°˜í™˜: {len(dates)}ê°œ")
        
        response_data = {
            'success': True,
            'dates': dates,
            'latest_date': dates[0] if dates else None,  # ì²« ë²ˆì§¸ ìš”ì†Œê°€ ìµœì‹  ë‚ ì§œ (ë‚´ë¦¼ì°¨ìˆœ)
            'prediction_threshold': prediction_start_threshold.strftime('%Y-%m-%d'),
            'halfway_point': halfway_date.strftime('%Y-%m-%d'),
            'halfway_semimonthly': halfway_semimonthly,
            'target_semimonthly': next_semimonthly
        }
        
        logger.info(f"ğŸ“¡ [API RESPONSE] Sending dates response:")
        logger.info(f"  ğŸ“… Total predictable dates: {len(predictable_dates)}")
        logger.info(f"  ğŸ“… Returned dates: {len(dates)}")
        logger.info(f"  ğŸ“ 50% threshold: {response_data['prediction_threshold']}")
        logger.info(f"  ğŸ¯ Target period: {response_data['target_semimonthly']}")
        logger.info(f"  ğŸ“… Date range: {dates[-1]} ~ {dates[0]} (ìµœì‹ ë¶€í„°)")  # ì²«ë²ˆì§¸ê°€ ìµœì‹ , ë§ˆì§€ë§‰ì´ ê°€ì¥ ì˜¤ë˜ëœ
        
        return jsonify(response_data)
    except Exception as e:
        logger.error(f"Error reading dates: {str(e)}")
        return jsonify({'error': f'Error reading dates: {str(e)}'}), 500

@app.route('/api/predictions/saved', methods=['GET'])
def get_saved_predictions():
    """ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ API"""
    try:
        limit = int(request.args.get('limit', 100))
        predictions_list = get_saved_predictions_list(limit)
        
        return jsonify({
            'success': True,
            'predictions': predictions_list,
            'count': len(predictions_list)
        })
    except Exception as e:
        logger.error(f"Error getting saved predictions: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/saved/<date>', methods=['GET'])
def get_saved_prediction_by_date(date):
    """íŠ¹ì • ë‚ ì§œì˜ ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ API"""
    try:
        result = load_prediction_from_csv(date)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify({'error': result['error']}), 404
    except Exception as e:
        logger.error(f"Error loading saved prediction: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/saved/<date>', methods=['DELETE'])
def delete_saved_prediction_api(date):
    """ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ì‚­ì œ API"""
    try:
        result = delete_saved_prediction(date)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify({'error': result['error']}), 500
    except Exception as e:
        logger.error(f"Error deleting saved prediction: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/export', methods=['GET'])
def export_predictions():
    """ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ë“¤ì„ í•˜ë‚˜ì˜ CSV íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° API"""
    try:
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        
        # ë‚ ì§œ ë²”ìœ„ì— ë”°ë¥¸ ì˜ˆì¸¡ ë¡œë“œ
        if start_date:
            predictions = load_accumulated_predictions_from_csv(start_date, end_date)
        else:
            # ëª¨ë“  ì €ì¥ëœ ì˜ˆì¸¡ ë¡œë“œ
            predictions_list = get_saved_predictions_list(limit=1000)
            predictions = []
            for pred_info in predictions_list:
                loaded = load_prediction_from_csv(pred_info['prediction_date'])
                if loaded['success']:
                    predictions.extend(loaded['predictions'])
        
        if not predictions:
            return jsonify({'error': 'No predictions found for export'}), 404
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        if isinstance(predictions[0], dict) and 'predictions' in predictions[0]:
            # ëˆ„ì  ì˜ˆì¸¡ í˜•ì‹ì¸ ê²½ìš°
            all_predictions = []
            for pred_group in predictions:
                all_predictions.extend(pred_group['predictions'])
            export_df = pd.DataFrame(all_predictions)
        else:
            # ë‹¨ìˆœ ì˜ˆì¸¡ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            export_df = pd.DataFrame(predictions)
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)
        export_df.to_csv(temp_file.name, index=False)
        temp_file.close()
        
        # íŒŒì¼ ì „ì†¡
        return send_file(
            temp_file.name,
            as_attachment=True,
            download_name=f'predictions_export_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
            mimetype='text/csv'
        )
        
    except Exception as e:
        logger.error(f"Error exporting predictions: {str(e)}")
        return jsonify({'error': str(e)}), 500

# 7. API ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì • - ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‚¬ìš©
@app.route('/api/predict', methods=['POST'])
def start_prediction_compatible():
    """í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ëŠ” ì˜ˆì¸¡ ì‹œì‘ API - ìºì‹œ ìš°ì„  ì‚¬ìš© (ë¡œê·¸ ê°•í™”)"""
    global prediction_state
    
    if prediction_state['is_predicting']:
        return jsonify({
            'success': False,
            'error': 'Prediction already in progress',
            'progress': prediction_state['prediction_progress']
        }), 409
    
    data = request.json
    filepath = data.get('filepath')
    current_date = data.get('date')
    save_to_csv = data.get('save_to_csv', True)
    use_cache = data.get('use_cache', True)  # ê¸°ë³¸ê°’ True
    
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'Invalid file path'}), 400
    
    # ğŸ”‘ íŒŒì¼ ê²½ë¡œë¥¼ ì „ì—­ ìƒíƒœì— ì €ì¥ (ìºì‹œ ì—°ë™ìš©)
    prediction_state['current_file'] = filepath
    
    # âœ… ë¡œê·¸ ê°•í™”
    logger.info(f"ğŸš€ Prediction API called:")
    logger.info(f"  ğŸ“… Target date: {current_date}")
    logger.info(f"  ğŸ“ Data file: {filepath}")
    logger.info(f"  ğŸ’¾ Save to CSV: {save_to_csv}")
    logger.info(f"  ğŸ”„ Use cache: {use_cache}")
    
    # í˜¸í™˜ì„± ìœ ì§€ ë°±ê·¸ë¼ìš´ë“œ í•¨ìˆ˜ ì‹¤í–‰ (ìºì‹œ ìš°ì„  ì‚¬ìš©)
    thread = Thread(target=background_prediction_simple_compatible, 
                   args=(filepath, current_date, save_to_csv, use_cache))
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'success': True,
        'message': 'Compatible prediction started (cache-first)',
        'use_cache': use_cache,
        'cache_priority': 'high',
        'features': ['Cache-first loading', 'Unified file naming', 'Enhanced logging', 'Past/Future visualization split']
    })

@app.route('/api/predict/status', methods=['GET'])
def prediction_status():
    """ì˜ˆì¸¡ ìƒíƒœ í™•ì¸ API"""
    global prediction_state
    
    status = {
        'is_predicting': prediction_state['is_predicting'],
        'progress': prediction_state['prediction_progress'],
        'error': prediction_state['error']
    }
    
    # ì˜ˆì¸¡ì´ ì™„ë£Œëœ ê²½ìš° ë‚ ì§œ ì •ë³´ë„ ë°˜í™˜
    if not prediction_state['is_predicting'] and prediction_state['current_date']:
        status['current_date'] = prediction_state['current_date']
    
    return jsonify(status)

@app.route('/api/results', methods=['GET'])
def get_prediction_results_compatible():
    """í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ëŠ” ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ API (ì˜¤ë¥˜ ìˆ˜ì •)"""
    global prediction_state
    
    logger.info(f"=== API /results called (compatible version) ===")
    
    if prediction_state['is_predicting']:
        return jsonify({
            'success': False,
            'error': 'Prediction in progress',
            'progress': prediction_state['prediction_progress']
        }), 409
    
    if prediction_state['latest_predictions'] is None:
        return jsonify({'error': 'No prediction results available'}), 404

    try:
        # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
        if isinstance(prediction_state['latest_predictions'], list):
            raw_predictions = prediction_state['latest_predictions']
        else:
            raw_predictions = prediction_state['latest_predictions']
        
        # ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
        compatible_predictions = convert_to_legacy_format(raw_predictions)
        
        logger.info(f"Converted {len(raw_predictions)} predictions to legacy format")
        logger.info(f"Sample converted prediction: {compatible_predictions[0] if compatible_predictions else 'None'}")
        
        # ë©”íŠ¸ë¦­ ì •ë¦¬
        metrics = prediction_state['latest_metrics']
        cleaned_metrics = {}
        if metrics:
            for key, value in metrics.items():
                cleaned_metrics[key] = safe_serialize_value(value)
        
        # êµ¬ê°„ ì ìˆ˜ ì•ˆì „ ì •ë¦¬
        interval_scores = prediction_state['latest_interval_scores'] or []
        cleaned_interval_scores = clean_interval_scores_safe(interval_scores)
        
        # MA ê²°ê³¼ ì •ë¦¬ ë° í•„ìš”ì‹œ ì¬ê³„ì‚°
        ma_results = prediction_state['latest_ma_results'] or {}
        cleaned_ma_results = {}
        
        # ì´ë™í‰ê·  ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆë‹¤ë©´ ì¬ê³„ì‚° ì‹œë„
        if not ma_results or len(ma_results) == 0:
            logger.info("ğŸ”„ MA results missing, attempting to recalculate...")
            try:
                # í˜„ì¬ ë°ì´í„°ì™€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë™í‰ê·  ì¬ê³„ì‚°
                current_date = prediction_state.get('current_date')
                if current_date and prediction_state.get('latest_file_path'):
                    # ì›ë³¸ ë°ì´í„° ë¡œë“œ
                    df = load_data(prediction_state['latest_file_path'])
                    if df is not None and not df.empty:
                        # í˜„ì¬ ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                        if isinstance(current_date, str):
                            current_date_dt = pd.to_datetime(current_date)
                        else:
                            current_date_dt = current_date
                        
                        # ê³¼ê±° ë°ì´í„° ì¶”ì¶œ
                        historical_data = df[df.index <= current_date_dt].copy()
                        
                        # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ì´ë™í‰ê·  ê³„ì‚°ìš©ìœ¼ë¡œ ë³€í™˜
                        ma_input_data = []
                        for pred in raw_predictions:
                            try:
                                ma_item = {
                                    'Date': pd.to_datetime(pred.get('Date') or pred.get('date')),
                                    'Prediction': safe_serialize_value(pred.get('Prediction') or pred.get('prediction')),
                                    'Actual': safe_serialize_value(pred.get('Actual') or pred.get('actual'))
                                }
                                ma_input_data.append(ma_item)
                            except Exception as e:
                                logger.warning(f"âš ï¸ Error processing MA data item: {str(e)}")
                                continue
                        
                        # ì´ë™í‰ê·  ê³„ì‚°
                        if ma_input_data:
                            ma_results = calculate_moving_averages_with_history(
                                ma_input_data, historical_data, target_col='MOPJ'
                            )
                            if ma_results:
                                logger.info(f"âœ… MA recalculated successfully with {len(ma_results)} windows")
                                prediction_state['latest_ma_results'] = ma_results
                            else:
                                logger.warning("âš ï¸ MA recalculation returned empty results")
                        else:
                            logger.warning("âš ï¸ No valid input data for MA calculation")
                    else:
                        logger.warning("âš ï¸ Unable to load original data for MA calculation")
                else:
                    logger.warning("âš ï¸ Missing current_date or file_path for MA calculation")
            except Exception as e:
                logger.error(f"âŒ Error recalculating MA: {str(e)}")
        
        # MA ê²°ê³¼ ì •ë¦¬
        for key, value in ma_results.items():
            if isinstance(value, list):
                cleaned_ma_results[key] = []
                for item in value:
                    if isinstance(item, dict):
                        cleaned_item = {}
                        for k, v in item.items():
                            cleaned_item[k] = safe_serialize_value(v)
                        cleaned_ma_results[key].append(cleaned_item)
                    else:
                        cleaned_ma_results[key].append(safe_serialize_value(item))
            else:
                cleaned_ma_results[key] = safe_serialize_value(value)
        
        # ì–´í…ì…˜ ë°ì´í„° ì •ë¦¬
        attention_data = prediction_state['latest_attention_data']
        cleaned_attention = None
        if attention_data:
            cleaned_attention = {}
            for key, value in attention_data.items():
                if key == 'image' and value:
                    cleaned_attention[key] = value  # base64 ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ
                elif isinstance(value, dict):
                    cleaned_attention[key] = {}
                    for k, v in value.items():
                        cleaned_attention[key][k] = safe_serialize_value(v)
                else:
                    cleaned_attention[key] = safe_serialize_value(value)
        
        # í”Œë¡¯ ë°ì´í„° ì •ë¦¬
        plots = prediction_state['latest_plots'] or {}
        cleaned_plots = {}
        for key, value in plots.items():
            if isinstance(value, dict):
                cleaned_plots[key] = {}
                for k, v in value.items():
                    if k == 'image' and v:
                        cleaned_plots[key][k] = v  # base64 ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ
                    else:
                        cleaned_plots[key][k] = safe_serialize_value(v)
            else:
                cleaned_plots[key] = safe_serialize_value(value)
        
        response_data = {
            'success': True,
            'current_date': safe_serialize_value(prediction_state['current_date']),
            'predictions': compatible_predictions,  # í˜¸í™˜ì„± ìœ ì§€ëœ í˜•íƒœ
            'interval_scores': cleaned_interval_scores,
            'ma_results': cleaned_ma_results,
            'attention_data': cleaned_attention,
            'plots': cleaned_plots,
            'metrics': cleaned_metrics if cleaned_metrics else None,
            'selected_features': prediction_state['selected_features'] or [],
            'feature_importance': safe_serialize_value(prediction_state.get('feature_importance')),
            'semimonthly_period': safe_serialize_value(prediction_state['semimonthly_period']),
            'next_semimonthly_period': safe_serialize_value(prediction_state['next_semimonthly_period'])
        }
        
        # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
        try:
            test_json = json.dumps(response_data)
            logger.info(f"JSON serialization test: SUCCESS (length: {len(test_json)})")
        except Exception as json_error:
            logger.error(f"JSON serialization test: FAILED - {str(json_error)}")
            return jsonify({
                'success': False,
                'error': f'Data serialization error: {str(json_error)}'
            }), 500
        
        logger.info(f"=== Compatible Response Summary ===")
        logger.info(f"Total predictions: {len(compatible_predictions)}")
        logger.info(f"Has metrics: {cleaned_metrics is not None}")
        logger.info(f"Sample prediction fields: {list(compatible_predictions[0].keys()) if compatible_predictions else 'None'}")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error creating compatible response: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': f'Error creating response: {str(e)}'}), 500
    
@app.route('/api/clear-cache', methods=['POST'])
def clear_prediction_cache():
    """ì˜ˆì¸¡ ìºì‹œ í´ë¦¬ì–´ (í…ŒìŠ¤íŠ¸ìš©)"""
    global prediction_state
    
    try:
        # ìƒíƒœ ì´ˆê¸°í™”
        prediction_state['latest_predictions'] = None
        prediction_state['latest_interval_scores'] = None
        prediction_state['latest_ma_results'] = None
        prediction_state['latest_attention_data'] = None
        prediction_state['latest_plots'] = None
        prediction_state['latest_metrics'] = None
        prediction_state['current_date'] = None
        prediction_state['selected_features'] = None
        prediction_state['feature_importance'] = None
        
        # ìºì‹œ íŒŒì¼ë“¤ë„ ì‚­ì œ (ì„ íƒì )
        cache_dir = Path(PREDICTIONS_DIR)
        if cache_dir.exists():
            for file in cache_dir.glob("prediction_*.csv"):
                file.unlink()
            for file in cache_dir.glob("prediction_*.json"):
                file.unlink()
            logger.info("Cache files cleared")
        
        logger.info("Prediction cache cleared successfully")
        
        return jsonify({
            'success': True,
            'message': 'Cache cleared successfully'
        })
        
    except Exception as e:
        logger.error(f"Error clearing cache: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/results/predictions', methods=['GET'])
def get_predictions_only():
    """ì˜ˆì¸¡ ê°’ë§Œ ì¡°íšŒ API"""
    global prediction_state
    
    if prediction_state['latest_predictions'] is None:
        return jsonify({'error': 'No prediction results available'}), 404
    
    return jsonify({
        'success': True,
        'current_date': prediction_state['current_date'],
        'predictions': prediction_state['latest_predictions']
    })

@app.route('/api/results/plots', methods=['GET'])
def get_plots():
    """ì‹œê°í™” ê²°ê³¼ë§Œ ì¡°íšŒ API"""
    global prediction_state
    
    if prediction_state['latest_plots'] is None:
        return jsonify({'error': 'No plot data available'}), 404
    
    return jsonify({
        'success': True,
        'current_date': prediction_state['current_date'],
        'plots': prediction_state['latest_plots']
    })

@app.route('/api/results/interval-scores', methods=['GET'])
def get_interval_scores():
    """êµ¬ê°„ ì ìˆ˜ ì¡°íšŒ API"""
    global prediction_state
    
    if prediction_state['latest_interval_scores'] is None:
        return jsonify({'error': 'No interval scores available'}), 404
    
    # prediction_state['latest_interval_scores']ê°€ dictì¸ ê²½ìš° ê°’ì„ ë°°ì—´ë¡œ ë³€í™˜,
    # ì´ë¯¸ ë°°ì—´ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if isinstance(prediction_state['latest_interval_scores'], dict):
        interval_scores = list(prediction_state['latest_interval_scores'].values())
    else:
        interval_scores = prediction_state['latest_interval_scores']
    
    return jsonify({
        'success': True,
        'current_date': prediction_state['current_date'],
        'interval_scores': interval_scores
    })

@app.route('/api/results/moving-averages', methods=['GET'])
def get_moving_averages():
    """ì´ë™í‰ê·  ì¡°íšŒ API"""
    global prediction_state
    
    if prediction_state['latest_ma_results'] is None:
        logger.warning("No moving average results available")
        return jsonify({'error': 'No moving average results available'}), 404
    
    logger.info(f"Returning MA results with {len(prediction_state['latest_ma_results'])} windows")
    
    return jsonify({
        'success': True,
        'current_date': prediction_state['current_date'],
        'ma_results': prediction_state['latest_ma_results']
    })

@app.route('/api/results/attention-map', methods=['GET'])
def get_attention_map():
    """ì–´í…ì…˜ ë§µ ì¡°íšŒ API"""
    global prediction_state
    
    if prediction_state['latest_attention_data'] is None:
        return jsonify({'error': 'No attention map available'}), 404
    
    return jsonify({
        'success': True,
        'current_date': prediction_state['current_date'],
        'attention_data': prediction_state['latest_attention_data']
    })

@app.route('/api/features', methods=['GET'])
def get_features():
    """ì„ íƒëœ íŠ¹ì„± ì¡°íšŒ API"""
    global prediction_state
    
    if prediction_state['selected_features'] is None:
        return jsonify({'error': 'No feature information available'}), 404
    
    return jsonify({
        'success': True,
        'current_date': prediction_state['current_date'],
        'selected_features': prediction_state['selected_features'],
        'feature_importance': prediction_state['feature_importance']
    })

# ì •ì  íŒŒì¼ ì œê³µ
@app.route('/static/<path:path>')
def serve_static(path):
    return send_file(os.path.join('static', path))

# ê¸°ë³¸ ë¼ìš°íŠ¸
@app.route('/')
def index():
    return jsonify({
        'app': 'MOPJ Prediction API',
        'version': '1.0.0',
        'status': 'running',
        'endpoints': [
            '/api/health',
            '/api/upload',
            '/api/holidays',
            '/api/holidays/upload',
            '/api/holidays/reload',
            '/api/file/metadata',
            '/api/data/dates',
            '/api/predict',
            '/api/predict/accumulated',
            '/api/predict/status',
            '/api/results',
            '/api/results/predictions',
            '/api/results/interval-scores',
            '/api/results/moving-averages',
            '/api/results/attention-map',
            '/api/results/accumulated',
            '/api/results/accumulated/interval-scores',
            '/api/results/accumulated/<date>',
            '/api/results/accumulated/report',
            '/api/results/accumulated/visualization',
            '/api/results/reliability',  # ìƒˆë¡œ ì¶”ê°€ëœ ì‹ ë¢°ë„ API
            '/api/features'
        ],
        'new_features': [
            'Prediction consistency scoring (ì˜ˆì¸¡ ì‹ ë¢°ë„)',
            'Purchase reliability percentage (êµ¬ë§¤ ì‹ ë¢°ë„)',
            'Holiday management system',
            'Accumulated predictions analysis'
        ]
    })

# 4. API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ - ëˆ„ì  ì˜ˆì¸¡ ì‹œì‘
@app.route('/api/predict/accumulated', methods=['POST'])
def start_accumulated_prediction():
    """ì—¬ëŸ¬ ë‚ ì§œì— ëŒ€í•œ ëˆ„ì  ì˜ˆì¸¡ ì‹œì‘ API (ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ í¬í•¨)"""
    global prediction_state
    
    if prediction_state['is_predicting']:
        return jsonify({
            'success': False,
            'error': 'Prediction already in progress',
            'progress': prediction_state['prediction_progress']
        }), 409
    
    data = request.json
    filepath = data.get('filepath')
    start_date = data.get('start_date')
    end_date = data.get('end_date')
    save_to_csv = data.get('save_to_csv', True)
    use_saved_data = data.get('use_saved_data', True)  # ì €ì¥ëœ ë°ì´í„° í™œìš© ì—¬ë¶€
    
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'Invalid file path'}), 400
    
    if not start_date:
        return jsonify({'error': 'Start date is required'}), 400
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëˆ„ì  ì˜ˆì¸¡ ì‹¤í–‰
    thread = Thread(target=run_accumulated_predictions_with_save, 
                   args=(filepath, start_date, end_date, save_to_csv, use_saved_data))
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'success': True,
        'message': 'Accumulated prediction started',
        'save_to_csv': save_to_csv,
        'use_saved_data': use_saved_data,
        'status_url': '/api/predict/status'
    })

# 5. API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ - ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ
@app.route('/api/results/accumulated', methods=['GET'])
def get_accumulated_results():
    global prediction_state
    
    logger.info("ğŸ” [ACCUMULATED] API call received")
    
    if prediction_state['is_predicting']:
        logger.warning("âš ï¸ [ACCUMULATED] Prediction still in progress")
        return jsonify({
            'success': False,
            'error': 'Prediction in progress',
            'progress': prediction_state['prediction_progress']
        }), 409

    if not prediction_state['accumulated_predictions']:
        logger.error("âŒ [ACCUMULATED] No accumulated prediction results available")
        return jsonify({'error': 'No accumulated prediction results available'}), 404

    logger.info("âœ… [ACCUMULATED] Processing accumulated predictions...")
    
    # ëˆ„ì  êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚° - ì˜¬ë°”ë¥¸ ë°©ì‹ ì‚¬ìš©
    accumulated_purchase_reliability = calculate_accumulated_purchase_reliability(
        prediction_state['accumulated_predictions']
    )
    
    logger.info(f"ğŸ’° [ACCUMULATED] Purchase reliability calculated: {accumulated_purchase_reliability}")
    
    # ë°ì´í„° ì•ˆì „ì„± ê²€ì‚¬
    safe_interval_scores = []
    if prediction_state.get('accumulated_interval_scores'):
        safe_interval_scores = [
            item for item in prediction_state['accumulated_interval_scores'] 
            if item is not None and isinstance(item, dict) and 'days' in item and item['days'] is not None
        ]
        logger.info(f"ğŸ“Š [ACCUMULATED] Safe interval scores count: {len(safe_interval_scores)}")
    else:
        logger.warning("âš ï¸ [ACCUMULATED] No accumulated_interval_scores found")
    
    consistency_scores = prediction_state.get('accumulated_consistency_scores', {})
    logger.info(f"ğŸ¯ [ACCUMULATED] Consistency scores keys: {list(consistency_scores.keys())}")
    
    # âœ… ìºì‹œ í†µê³„ ì •ë³´ ì¶”ê°€
    cache_stats = prediction_state.get('cache_statistics', {
        'total_dates': 0,
        'cached_dates': 0,
        'new_predictions': 0,
        'cache_hit_rate': 0.0
    })
    
    response_data = {
        'success': True,
        'prediction_dates': prediction_state.get('prediction_dates', []),
        'accumulated_metrics': prediction_state.get('accumulated_metrics', {}),
        'predictions': prediction_state['accumulated_predictions'],
        'accumulated_interval_scores': safe_interval_scores,
        'accumulated_consistency_scores': consistency_scores,
        'accumulated_purchase_reliability': accumulated_purchase_reliability,
        'cache_statistics': cache_stats  # âœ… ìºì‹œ í†µê³„ ì¶”ê°€
    }
    
    logger.info(f"ğŸ“¤ [ACCUMULATED] Response summary: predictions={len(response_data['predictions'])}, metrics_keys={list(response_data['accumulated_metrics'].keys())}, reliability={response_data['accumulated_purchase_reliability']}")
    
    return jsonify(response_data)

@app.route('/api/results/accumulated/interval-scores', methods=['GET'])
def get_accumulated_interval_scores():
    global prediction_state
    scores = prediction_state.get('accumulated_interval_scores', [])
    
    # 'days' ì†ì„±ì´ ì—†ëŠ” í•­ëª© í•„í„°ë§
    safe_scores = [
        item for item in scores 
        if item is not None and isinstance(item, dict) and 'days' in item and item['days'] is not None
    ]
    
    return jsonify(safe_scores)

# 7. ëˆ„ì  ë³´ê³ ì„œ API ì—”ë“œí¬ì¸íŠ¸
@app.route('/api/results/accumulated/report', methods=['GET'])
def get_accumulated_report():
    """ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ API"""
    global prediction_state
    
    # ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    if not prediction_state['accumulated_predictions']:
        return jsonify({'error': 'No accumulated prediction results available'}), 404
    
    report_file = generate_accumulated_report()
    if not report_file:
        return jsonify({'error': 'Failed to generate report'}), 500
    
    return send_file(report_file, as_attachment=True)

def return_prediction_result(pred, date, match_type):
    """
    ì˜ˆì¸¡ ê²°ê³¼ë¥¼ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    
    Parameters:
    -----------
    pred : dict
        ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    date : str
        ìš”ì²­ëœ ë‚ ì§œ
    match_type : str
        ë§¤ì¹­ ë°©ì‹ ì„¤ëª…
    
    Returns:
    --------
    JSON response
    """
    try:
        logger.info(f"ğŸ”„ [API] Returning prediction result for date={date}, match_type={match_type}")
        
        # ì˜ˆì¸¡ ë°ì´í„° ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        predictions = pred.get('predictions', [])
        if not isinstance(predictions, list):
            logger.warning(f"âš ï¸ [API] predictions is not a list: {type(predictions)}")
            predictions = []
        
        # êµ¬ê°„ ì ìˆ˜ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ ë° ë³€í™˜
        interval_scores = pred.get('interval_scores', {})
        if isinstance(interval_scores, dict):
            # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            interval_scores_list = []
            for key, interval in interval_scores.items():
                if interval and isinstance(interval, dict) and 'days' in interval:
                    interval_scores_list.append(interval)
            interval_scores = interval_scores_list
        elif not isinstance(interval_scores, list):
            logger.warning(f"âš ï¸ [API] interval_scores is neither dict nor list: {type(interval_scores)}")
            interval_scores = []
        
        # ë©”íŠ¸ë¦­ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        metrics = pred.get('metrics', {})
        if not isinstance(metrics, dict):
            logger.warning(f"âš ï¸ [API] metrics is not a dict: {type(metrics)}")
            metrics = {}
        
        # ğŸ”„ ì´ë™í‰ê·  ë°ì´í„° ì¶”ì¶œ (ìºì‹œëœ ë°ì´í„° ë˜ëŠ” íŒŒì¼ì—ì„œ ë¡œë“œ)
        ma_results = pred.get('ma_results', {})
        if not ma_results:
            # íŒŒì¼ë³„ ìºì‹œì—ì„œ MA íŒŒì¼ ë¡œë“œ ì‹œë„
            try:
                current_file = prediction_state.get('current_file')
                if current_file:
                    cache_dirs = get_file_cache_dirs(current_file)
                    predictions_dir = cache_dirs['predictions']
                    
                    pred_start_date = pred.get('prediction_start_date') or date
                    ma_file_path = predictions_dir / f"prediction_start_{pd.to_datetime(pred_start_date).strftime('%Y%m%d')}_ma.json"
                else:
                    # ë°±ì—…: ê¸€ë¡œë²Œ ìºì‹œ ì‚¬ìš©
                    pred_start_date = pred.get('prediction_start_date') or date
                    ma_file_path = Path(PREDICTIONS_DIR) / f"prediction_start_{pd.to_datetime(pred_start_date).strftime('%Y%m%d')}_ma.json"
                
                if ma_file_path.exists():
                    with open(ma_file_path, 'r', encoding='utf-8') as f:
                        ma_results = json.load(f)
                    logger.info(f"ğŸ“Š [API] MA results loaded from file for {date}: {len(ma_results)} windows")
                else:
                    logger.info(f"âš ï¸ [API] No MA file found for {date}: {ma_file_path}")
                    
                    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì˜ˆì¸¡ ë°ì´í„°ì—ì„œ ì¬ê³„ì‚° (íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì—†ì´ ì œí•œì ìœ¼ë¡œ)
                    if predictions:
                        ma_results = calculate_moving_averages_with_history(
                            predictions, None, target_col='MOPJ', windows=[5, 10, 23]
                        )
                        logger.info(f"ğŸ“Š [API] MA results recalculated for {date}: {len(ma_results)} windows")
            except Exception as e:
                logger.warning(f"âš ï¸ [API] Error loading/calculating MA for {date}: {str(e)}")
                ma_results = {}
        
        # ğŸ¯ Attention ë°ì´í„° ì¶”ì¶œ
        attention_data = pred.get('attention_data', {})
        if not attention_data:
            # íŒŒì¼ë³„ ìºì‹œì—ì„œ Attention íŒŒì¼ ë¡œë“œ ì‹œë„
            try:
                current_file = prediction_state.get('current_file')
                if current_file:
                    cache_dirs = get_file_cache_dirs(current_file)
                    predictions_dir = cache_dirs['predictions']
                    
                    pred_start_date = pred.get('prediction_start_date') or date
                    attention_file_path = predictions_dir / f"prediction_start_{pd.to_datetime(pred_start_date).strftime('%Y%m%d')}_attention.json"
                else:
                    # ë°±ì—…: ê¸€ë¡œë²Œ ìºì‹œ ì‚¬ìš©
                    pred_start_date = pred.get('prediction_start_date') or date
                    attention_file_path = Path(PREDICTIONS_DIR) / f"prediction_start_{pd.to_datetime(pred_start_date).strftime('%Y%m%d')}_attention.json"
                
                if attention_file_path.exists():
                    with open(attention_file_path, 'r', encoding='utf-8') as f:
                        attention_data = json.load(f)
                    logger.info(f"ğŸ“Š [API] Attention data loaded from file for {date}")
                else:
                    logger.info(f"âš ï¸ [API] No attention file found for {date}: {attention_file_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ [API] Error loading attention data for {date}: {str(e)}")
        
        response_data = {
            'success': True,
            'date': date,
            'predictions': predictions,
            'interval_scores': interval_scores,
            'metrics': metrics,
            'ma_results': ma_results,  # ğŸ”‘ ì´ë™í‰ê·  ë°ì´í„° ì¶”ê°€
            'attention_data': attention_data,  # ğŸ”‘ Attention ë°ì´í„° ì¶”ê°€
            'next_semimonthly_period': pred.get('next_semimonthly_period'),
            'actual_business_days': pred.get('actual_business_days'),
            'match_type': match_type,
            'data_end_date': pred.get('date'),  # ë°ì´í„° ê¸°ì¤€ì¼ ì¶”ê°€
            'prediction_start_date': pred.get('prediction_start_date')  # ì˜ˆì¸¡ ì‹œì‘ì¼ ì¶”ê°€
        }
        
        logger.info(f"âœ… [API] Successfully prepared response for {date}: predictions={len(predictions)}, interval_scores={len(interval_scores)}, ma_windows={len(ma_results)}, attention_data={bool(attention_data)}")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ [API] Error in return_prediction_result for {date}: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'Error processing prediction result: {str(e)}',
            'date': date
        }), 500

# 8. API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ - íŠ¹ì • ë‚ ì§œ ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ

@app.route('/api/results/accumulated/<date>', methods=['GET'])
def get_accumulated_result_by_date(date):
    """íŠ¹ì • ë‚ ì§œì˜ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ API"""
    global prediction_state
    
    logger.info(f"ğŸ” [API] Searching for accumulated result by date: {date}")
    
    if not prediction_state['accumulated_predictions']:
        logger.warning("âŒ [API] No accumulated prediction results available")
        return jsonify({'error': 'No accumulated prediction results available'}), 404
    
    logger.info(f"ğŸ“Š [API] Available prediction dates (data_end_date): {[p['date'] for p in prediction_state['accumulated_predictions']]}")
    
    # âœ… 1ë‹¨ê³„: ì •í™•í•œ ë°ì´í„° ê¸°ì¤€ì¼ ë§¤ì¹­ ìš°ì„  í™•ì¸
    logger.info(f"ğŸ” [API] Step 1: Looking for EXACT data_end_date match for {date}")
    for i, pred in enumerate(prediction_state['accumulated_predictions']):
        data_end_date = pred.get('date')  # ë°ì´í„° ê¸°ì¤€ì¼
        
        logger.info(f"ğŸ” [API] Checking prediction {i+1}: data_end_date={data_end_date}")
        
        if data_end_date == date:
            logger.info(f"âœ… [API] Found prediction by EXACT DATA END DATE match: {date}")
            logger.info(f"ğŸ“Š [API] Prediction data preview: predictions={len(pred.get('predictions', []))}, interval_scores={len(pred.get('interval_scores', {}))}")
            return return_prediction_result(pred, date, "exact data end date")
    
    # âœ… 2ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ì´ ì—†ìœ¼ë©´ ê³„ì‚°ëœ ì˜ˆì¸¡ ì‹œì‘ì¼ë¡œ ë§¤ì¹­
    logger.info(f"ğŸ” [API] Step 2: No exact match found. Looking for calculated prediction start date match for {date}")
    for i, pred in enumerate(prediction_state['accumulated_predictions']):
        data_end_date = pred.get('date')  # ë°ì´í„° ê¸°ì¤€ì¼
        prediction_start_date = pred.get('prediction_start_date')  # ì˜ˆì¸¡ ì‹œì‘ì¼
        
        logger.info(f"ğŸ” [API] Checking prediction {i+1}: data_end_date={data_end_date}, prediction_start_date={prediction_start_date}")
        
        if data_end_date:
            try:
                data_end_dt = pd.to_datetime(data_end_date)
                calculated_start_date = data_end_dt + pd.Timedelta(days=1)
                
                # ì£¼ë§ê³¼ íœ´ì¼ ê±´ë„ˆë›°ê¸°
                while calculated_start_date.weekday() >= 5 or is_holiday(calculated_start_date):
                    calculated_start_date += pd.Timedelta(days=1)
                
                calculated_start_str = calculated_start_date.strftime('%Y-%m-%d')
                
                if calculated_start_str == date:
                    logger.info(f"âœ… [API] Found prediction by CALCULATED PREDICTION START DATE: {date} (from data end date: {data_end_date})")
                    return return_prediction_result(pred, date, "calculated prediction start date from data end date")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ [API] Error calculating prediction start date for {data_end_date}: {str(e)}")
                continue
    
    logger.error(f"âŒ [API] No prediction results found for date {date}")
    return jsonify({'error': f'No prediction results for date {date}'}), 404

# 10. ëˆ„ì  ì§€í‘œ ì‹œê°í™” API ì—”ë“œí¬ì¸íŠ¸
@app.route('/api/results/accumulated/visualization', methods=['GET'])
def get_accumulated_visualization():
    """ëˆ„ì  ì˜ˆì¸¡ ì§€í‘œ ì‹œê°í™” API"""
    global prediction_state
    
    if not prediction_state['accumulated_predictions']:
        return jsonify({'error': 'No accumulated prediction results available'}), 404
    
    filename, img_str = visualize_accumulated_metrics()
    if not filename:
        return jsonify({'error': 'Failed to generate visualization'}), 500
    
    return jsonify({
        'success': True,
        'file_path': filename,
        'image': img_str
    })

# ìƒˆë¡œìš´ API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@app.route('/api/results/reliability', methods=['GET'])
def get_reliability_scores():
    """ì‹ ë¢°ë„ ì ìˆ˜ ì¡°íšŒ API"""
    global prediction_state
    
    # ë‹¨ì¼ ì˜ˆì¸¡ ì‹ ë¢°ë„
    single_reliability = {}
    if prediction_state.get('latest_interval_scores') and prediction_state.get('latest_predictions'):
        try:
            # ì‹¤ì œ ì˜ì—…ì¼ ìˆ˜ ê³„ì‚°
            actual_business_days = len([p for p in prediction_state['latest_predictions'] 
                                       if p.get('Date') and not p.get('is_synthetic', False)])
            
            single_reliability = {
                'period': prediction_state['next_semimonthly_period']
            }
        except Exception as e:
            logger.error(f"Error calculating single prediction reliability: {str(e)}")
            single_reliability = {'error': 'Unable to calculate single prediction reliability'}
    
    # ëˆ„ì  ì˜ˆì¸¡ ì‹ ë¢°ë„ (ì•ˆì „í•œ ì ‘ê·¼)
    accumulated_reliability = prediction_state.get('accumulated_consistency_scores', {})
    
    return jsonify({
        'success': True,
        'single_prediction_reliability': single_reliability,
        'accumulated_prediction_reliability': accumulated_reliability
    })

@app.route('/api/cache/clear/accumulated', methods=['POST'])
def clear_accumulated_cache():
    """ëˆ„ì  ì˜ˆì¸¡ ìºì‹œ í´ë¦¬ì–´"""
    global prediction_state
    
    try:
        # ëˆ„ì  ì˜ˆì¸¡ ê´€ë ¨ ìƒíƒœ í´ë¦¬ì–´
        prediction_state['accumulated_predictions'] = []
        prediction_state['accumulated_metrics'] = {}
        prediction_state['accumulated_interval_scores'] = []
        prediction_state['accumulated_consistency_scores'] = {}
        prediction_state['accumulated_purchase_reliability'] = 0
        prediction_state['prediction_dates'] = []
        
        logger.info("ğŸ§¹ [CACHE] Accumulated prediction cache cleared")
        
        return jsonify({
            'success': True,
            'message': 'Accumulated prediction cache cleared successfully'
        })
        
    except Exception as e:
        logger.error(f"âŒ [CACHE] Error clearing accumulated cache: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/debug/reliability', methods=['GET'])
def debug_reliability_calculation():
    """êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚° ë””ë²„ê¹… API"""
    global prediction_state
    
    if not prediction_state['accumulated_predictions']:
        return jsonify({'error': 'No accumulated prediction results available'}), 404
    
    predictions = prediction_state['accumulated_predictions']
    print(f"ğŸ” [DEBUG] Total predictions: {len(predictions)}")
    
    debug_data = {
        'prediction_count': len(predictions),
        'predictions_details': []
    }
    
    total_score = 0
    
    for i, pred in enumerate(predictions):
        pred_date = pred.get('date')
        interval_scores = pred.get('interval_scores', {})
        
        print(f"ğŸ“Š [DEBUG] Prediction {i+1} ({pred_date}):")
        print(f"   - interval_scores type: {type(interval_scores)}")
        print(f"   - interval_scores keys: {list(interval_scores.keys()) if isinstance(interval_scores, dict) else 'N/A'}")
        
        pred_detail = {
            'date': pred_date,
            'interval_scores_type': str(type(interval_scores)),
            'interval_scores_keys': list(interval_scores.keys()) if isinstance(interval_scores, dict) else [],
            'individual_scores': [],
            'best_score': 0
        }
        
        if isinstance(interval_scores, dict):
            for key, score_data in interval_scores.items():
                print(f"   - {key}: {score_data}")
                if isinstance(score_data, dict) and 'score' in score_data:
                    score_value = score_data.get('score', 0)
                    pred_detail['individual_scores'].append({
                        'key': key,
                        'score': score_value,
                        'full_data': score_data
                    })
                    print(f"     -> score: {score_value}")
        
        if pred_detail['individual_scores']:
            best_score = max([s['score'] for s in pred_detail['individual_scores']])
            pred_detail['best_score'] = best_score
            total_score += min(best_score, 3.0)  # 3ì  ì œí•œ
            print(f"   - Best score: {best_score}")
        
        debug_data['predictions_details'].append(pred_detail)
    
    max_possible_score = len(predictions) * 3
    reliability = (total_score / max_possible_score) * 100 if max_possible_score > 0 else 0
    
    debug_data.update({
        'total_score': total_score,
        'max_possible_score': max_possible_score,
        'reliability_percentage': reliability
    })
    
    print(f"ğŸ¯ [DEBUG] CALCULATION SUMMARY:")
    print(f"   - Total predictions: {len(predictions)}")
    print(f"   - Total score: {total_score}")
    print(f"   - Max possible score: {max_possible_score}")
    print(f"   - Reliability: {reliability:.1f}%")
    
    return jsonify(debug_data)

@app.route('/api/cache/check', methods=['POST'])
def check_cached_predictions():
    """ëˆ„ì  ì˜ˆì¸¡ ë²”ìœ„ì—ì„œ ìºì‹œëœ ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ìˆëŠ”ì§€ í™•ì¸"""
    data = request.get_json()
    start_date = data.get('start_date')
    end_date = data.get('end_date')
    
    if not start_date or not end_date:
        return jsonify({'error': 'start_date and end_date are required'}), 400
    
    try:
        logger.info(f"ğŸ” [CACHE_CHECK] Checking cache availability for {start_date} to {end_date}")
        
        # ì €ì¥ëœ ì˜ˆì¸¡ í™•ì¸
        cached_predictions = load_accumulated_predictions_from_csv(start_date, end_date)
        
        # ì „ì²´ ë²”ìœ„ ê³„ì‚° (ë°ì´í„° ê¸°ì¤€ì¼ ê¸°ì¤€)
        start_dt = pd.to_datetime(start_date)
        end_dt = pd.to_datetime(end_date)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ê³„ì‚° (ë°ì´í„° ê¸°ì¤€ì¼)
        available_dates = []
        current_dt = start_dt
        while current_dt <= end_dt:
            # ì˜ì—…ì¼ë§Œ í¬í•¨ (ì£¼ë§ê³¼ íœ´ì¼ ì œì™¸)
            if current_dt.weekday() < 5 and not is_holiday(current_dt):
                available_dates.append(current_dt.strftime('%Y-%m-%d'))
            current_dt += pd.Timedelta(days=1)
        
        # ìºì‹œëœ ë‚ ì§œ ëª©ë¡
        cached_dates = [pred['date'] for pred in cached_predictions]
        missing_dates = [date for date in available_dates if date not in cached_dates]
        
        cache_percentage = round(len(cached_predictions) / max(len(available_dates), 1) * 100, 1)
        
        logger.info(f"ğŸ“Š [CACHE_CHECK] Cache status: {len(cached_predictions)}/{len(available_dates)} ({cache_percentage}%)")
        
        return jsonify({
            'success': True,
            'total_dates_in_range': len(available_dates),
            'cached_predictions': len(cached_predictions),
            'cached_dates': cached_dates,
            'missing_dates': missing_dates,
            'cache_percentage': cache_percentage,
            'will_use_cache': len(cached_predictions) > 0,
            'estimated_time_savings': f"ì•½ {len(cached_predictions) * 3}ë¶„ ì ˆì•½ ì˜ˆìƒ" if len(cached_predictions) > 0 else "ì—†ìŒ"
        })
        
    except Exception as e:
        logger.error(f"âŒ [CACHE_CHECK] Error checking cached predictions: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/results/accumulated/recent', methods=['GET'])
def get_recent_accumulated_results():
    """
    í˜ì´ì§€ ë¡œë“œ ì‹œ ìµœê·¼ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ë³µì›í•˜ëŠ” API
    """
    try:
        # ì €ì¥ëœ ì˜ˆì¸¡ ëª©ë¡ ì¡°íšŒ (ìµœê·¼ ê²ƒë¶€í„°)
        predictions_list = get_saved_predictions_list(limit=50)
        
        if not predictions_list:
            return jsonify({
                'success': False, 
                'message': 'No saved predictions found',
                'has_recent_results': False
            })
        
        # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì—°ì†ëœ ë²”ìœ„ ì°¾ê¸°
        dates_by_groups = {}
        for pred in predictions_list:
            data_end_date = pred.get('data_end_date', pred.get('prediction_date'))
            if data_end_date:
                date_obj = pd.to_datetime(data_end_date)
                # ì£¼ì°¨ë³„ë¡œ ê·¸ë£¹í™” (ê°™ì€ ì£¼ì˜ ì˜ˆì¸¡ë“¤ì„ í•˜ë‚˜ì˜ ë²”ìœ„ë¡œ ê°„ì£¼)
                week_key = date_obj.strftime('%Y-W%U')
                if week_key not in dates_by_groups:
                    dates_by_groups[week_key] = []
                dates_by_groups[week_key].append({
                    'date': data_end_date,
                    'date_obj': date_obj,
                    'pred_info': pred
                })
        
        # ê°€ì¥ ìµœê·¼ ê·¸ë£¹ ì„ íƒ
        if not dates_by_groups:
            return jsonify({
                'success': False, 
                'message': 'No valid date groups found',
                'has_recent_results': False
            })
        
        # ìµœê·¼ ì£¼ì˜ ì˜ˆì¸¡ë“¤ ê°€ì ¸ì˜¤ê¸°
        latest_week = max(dates_by_groups.keys())
        latest_group = dates_by_groups[latest_week]
        latest_group.sort(key=lambda x: x['date_obj'])
        
        # ì—°ì†ëœ ë‚ ì§œ ë²”ìœ„ ì°¾ê¸°
        start_date = latest_group[0]['date_obj']
        end_date = latest_group[-1]['date_obj']
        
        logger.info(f"ğŸ”„ [AUTO_RESTORE] Found recent accumulated predictions: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        
        # ê¸°ì¡´ ìºì‹œì—ì„œ ëˆ„ì  ê²°ê³¼ ë¡œë“œ
        loaded_predictions = load_accumulated_predictions_from_csv(start_date, end_date)
        
        if not loaded_predictions:
            return jsonify({
                'success': False, 
                'message': 'Failed to load cached predictions',
                'has_recent_results': False
            })
        
        # ëˆ„ì  ë©”íŠ¸ë¦­ ê³„ì‚°
        accumulated_metrics = {
            'f1': 0.0,
            'accuracy': 0.0,
            'mape': 0.0,
            'weighted_score': 0.0,
            'total_predictions': 0
        }
        
        for pred in loaded_predictions:
            metrics = pred.get('metrics', {})
            if isinstance(metrics, dict):
                accumulated_metrics['f1'] += metrics.get('f1', 0.0)
                accumulated_metrics['accuracy'] += metrics.get('accuracy', 0.0)
                accumulated_metrics['mape'] += metrics.get('mape', 0.0)
                accumulated_metrics['weighted_score'] += metrics.get('weighted_score', 0.0)
                accumulated_metrics['total_predictions'] += 1
        
        if accumulated_metrics['total_predictions'] > 0:
            count = accumulated_metrics['total_predictions']
            accumulated_metrics['f1'] /= count
            accumulated_metrics['accuracy'] /= count
            accumulated_metrics['mape'] /= count
            accumulated_metrics['weighted_score'] /= count
        
        # êµ¬ê°„ ì ìˆ˜ ê³„ì‚°
        accumulated_interval_scores = {}
        for pred in loaded_predictions:
            interval_scores = pred.get('interval_scores', {})
            if isinstance(interval_scores, dict):
                for interval in interval_scores.values():
                    if not interval or not isinstance(interval, dict) or 'days' not in interval or interval['days'] is None:
                        continue
                    interval_key = f"{format_date(interval['start_date'])}-{format_date(interval['end_date'])}"
                    if interval_key in accumulated_interval_scores:
                        accumulated_interval_scores[interval_key]['score'] += interval['score']
                        accumulated_interval_scores[interval_key]['count'] += 1
                    else:
                        accumulated_interval_scores[interval_key] = interval.copy()
                        accumulated_interval_scores[interval_key]['count'] = 1
        
        # ì •ë ¬ëœ êµ¬ê°„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        accumulated_scores_list = [
            v for v in accumulated_interval_scores.values()
            if v is not None and isinstance(v, dict) and 'days' in v and v['days'] is not None
        ]
        accumulated_scores_list.sort(key=lambda x: x['score'], reverse=True)
        
        # êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚°
        accumulated_purchase_reliability = calculate_accumulated_purchase_reliability(loaded_predictions)
        
        # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
        unique_periods = set()
        for pred in loaded_predictions:
            if 'next_semimonthly_period' in pred and pred['next_semimonthly_period']:
                unique_periods.add(pred['next_semimonthly_period'])
        
        accumulated_consistency_scores = {}
        for period in unique_periods:
            try:
                consistency_data = calculate_prediction_consistency(loaded_predictions, period)
                accumulated_consistency_scores[period] = consistency_data
            except Exception as e:
                logger.error(f"Error calculating consistency for period {period}: {str(e)}")
        
        # ìºì‹œ í†µê³„
        cache_statistics = {
            'total_dates': len(loaded_predictions),
            'cached_dates': len(loaded_predictions),
            'new_predictions': 0,
            'cache_hit_rate': 100.0
        }
        
        # ì „ì—­ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì„ íƒì )
        global prediction_state
        prediction_state['accumulated_predictions'] = loaded_predictions
        prediction_state['accumulated_metrics'] = accumulated_metrics
        prediction_state['prediction_dates'] = [p['date'] for p in loaded_predictions]
        prediction_state['accumulated_interval_scores'] = accumulated_scores_list
        prediction_state['accumulated_consistency_scores'] = accumulated_consistency_scores
        prediction_state['accumulated_purchase_reliability'] = accumulated_purchase_reliability
        prediction_state['cache_statistics'] = cache_statistics
        
        logger.info(f"âœ… [AUTO_RESTORE] Successfully restored {len(loaded_predictions)} accumulated predictions")
        
        return jsonify({
            'success': True,
            'has_recent_results': True,
            'restored_range': {
                'start_date': start_date.strftime('%Y-%m-%d'),
                'end_date': end_date.strftime('%Y-%m-%d'),
                'prediction_count': len(loaded_predictions)
            },
            'prediction_dates': [p['date'] for p in loaded_predictions],
            'accumulated_metrics': accumulated_metrics,
            'predictions': loaded_predictions,
            'accumulated_interval_scores': accumulated_scores_list,
            'accumulated_consistency_scores': accumulated_consistency_scores,
            'accumulated_purchase_reliability': accumulated_purchase_reliability,
            'cache_statistics': cache_statistics,
            'message': f"ìµœê·¼ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ë³µì›í–ˆìŠµë‹ˆë‹¤ ({start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')})"
        })
        
    except Exception as e:
        logger.error(f"âŒ [AUTO_RESTORE] Error restoring recent accumulated results: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False, 
            'error': str(e),
            'has_recent_results': False
        }), 500

@app.route('/api/cache/rebuild-index', methods=['POST'])
def rebuild_predictions_index_api():
    """ì˜ˆì¸¡ ì¸ë±ìŠ¤ ì¬ìƒì„± API (rebuild_index.py ê¸°ëŠ¥ì„ í†µí•©)"""
    try:
        # í˜„ì¬ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        current_file = prediction_state.get('current_file')
        if not current_file:
            return jsonify({'success': False, 'error': 'í˜„ì¬ ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'})
        
        # ğŸ”§ ìƒˆë¡œìš´ rebuild í•¨ìˆ˜ ì‚¬ìš©
        success = rebuild_predictions_index_from_existing_files()
        
        if success:
            cache_dirs = get_file_cache_dirs(current_file)
            index_file = cache_dirs['predictions'] / 'predictions_index.csv'
            
            # ê²°ê³¼ ë°ì´í„° ì½ê¸°
            index_data = []
            if index_file.exists():
                with open(index_file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    index_data = list(reader)
            
            return jsonify({
                'success': True,
                'message': f'ì¸ë±ìŠ¤ íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ì¬ìƒì„±í–ˆìŠµë‹ˆë‹¤. ({len(index_data)}ê°œ í•­ëª©)',
                'file_location': str(index_file),
                'entries_count': len(index_data),
                'rebuilt_entries': [{'date': row.get('prediction_start_date', ''), 'data_end': row.get('data_end_date', '')} for row in index_data]
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ì¸ë±ìŠ¤ ì¬ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.'
            })
        
    except Exception as e:
        logger.error(f"âŒ Error rebuilding predictions index: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': f'ì¸ë±ìŠ¤ ì¬ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'})

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ì—…ë°ì´íŠ¸
if __name__ == '__main__':
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
    try:
        import optuna
    except ImportError:
        logger.warning("Optuna íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•´ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        logger.warning("pip install optuna ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ğŸ¯ íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œ - ë ˆê±°ì‹œ ë””ë ‰í† ë¦¬ ë° ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„± ì œê±°
    # ëª¨ë“  ë°ì´í„°ëŠ” ì´ì œ íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤
    logger.info("ğŸš€ Starting with file-based cache system - no legacy directories needed")
    
    app.run(debug=True, host='0.0.0.0', port=5000, threaded=True)
